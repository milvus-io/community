---
id: >-
  we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md
title: 'قمنا بمقارنة أكثر من 20 واجهة برمجة تطبيقات مدمجة مع Milvus: 7 رؤى ستفاجئك'
author: Jeremy Zhu
date: 2025-05-23T00:00:00.000Z
desc: >-
  واجهات برمجة التطبيقات الأكثر شيوعاً للتضمين ليست الأسرع. الجغرافيا مهمة أكثر
  من بنية النموذج. وأحيانًا تتفوق وحدة المعالجة المركزية التي تبلغ قيمتها 20
  دولارًا شهريًا على واجهة برمجة التطبيقات التي تبلغ قيمتها 200 دولار شهريًا.
cover: >-
  assets.zilliz.com/We_Benchmarked_20_Embedding_AP_Is_with_Milvus_7_Insights_That_Will_Surprise_You_12268622f0.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database, vector search'
meta_keywords: 'Milvus, Embedding API, RAG, latency, vector search'
meta_title: >
  We Benchmarked 20+ Embedding APIs with Milvus: 7 Insights That Will Surprise
  You
origin: >-
  https://milvus.io/blog/we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md
---
<p><strong>ربما قام كل مطور ذكاء اصطناعي على الأرجح ببناء نظام RAG يعمل بشكل مثالي في بيئته المحلية.</strong></p>
<p>لقد أتقنت دقة الاسترجاع، وحسّنت قاعدة بيانات المتجهات، ويعمل العرض التوضيحي الخاص بك مثل الزبدة. ثم تقوم بالنشر إلى الإنتاج وفجأة:</p>
<ul>
<li><p>تستغرق استفساراتك المحلية التي تستغرق 200 مللي ثانية للمستخدمين الفعليين 3 ثوانٍ</p></li>
<li><p>أبلغ زملاؤك في مناطق مختلفة عن أداء مختلف تمامًا</p></li>
<li><p>يصبح موفر التضمين الذي اخترته للحصول على "أفضل دقة" أكبر عائق أمامك</p></li>
</ul>
<p>ماذا حدث؟ ها هو قاتل الأداء الذي لا يقيسه أحد: <strong>زمن انتقال واجهة برمجة التطبيقات المضمنة</strong>.</p>
<p>في حين أن تصنيفات MTEB تستحوذ على درجات الاستدعاء وأحجام النماذج، فإنها تتجاهل المقياس الذي يشعر به المستخدمون بالفعل - كم من الوقت ينتظرون قبل رؤية أي استجابة. لقد اختبرنا كل مزوّد رئيسي لخدمات التضمين في ظروف واقعية واكتشفنا اختلافات كبيرة في زمن الاستجابة لدرجة تجعلك تشكك في استراتيجية اختيار المزود بالكامل.</p>
<p><strong><em>إفساد: أكثر واجهات برمجة تطبيقات التضمين شيوعاً ليست الأسرع. الجغرافيا مهمة أكثر من بنية النموذج. وأحيانًا <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">تتفوق وحدة المعالجة المركزية</annotation><mrow><mi>20/شهرًا</mi><mn></mn></mrow><annotation encoding="application/x-tex">على وحدة المعالجة المركزية</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord">20/شهرًا على</span></span></span><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">وحدة المعالجة المركزية</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord">20/شهرًا على</span></span></span></span>واجهة برمجة التطبيقات</em></strong><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><strong><em> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathnormal">20/شهرًا</span></span></span></span>.</em></strong></p>
<h2 id="Why-Embedding-API-Latency-Is-the-Hidden-Bottleneck-in-RAG" class="common-anchor-header">لماذا يعد تضمين كمون واجهة برمجة التطبيقات هو عنق الزجاجة الخفي في RAG<button data-href="#Why-Embedding-API-Latency-Is-the-Hidden-Bottleneck-in-RAG" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>عند إنشاء أنظمة RAG، أو البحث في التجارة الإلكترونية، أو محركات التوصية، تعمل نماذج التضمين كمكون أساسي يحول النص إلى متجهات، مما يمكّن الآلات من فهم الدلالات وإجراء عمليات بحث فعالة عن التشابه. بينما نقوم عادةً بحساب التضمينات مسبقًا لمكتبات المستندات، لا تزال استعلامات المستخدم تتطلب مكالمات واجهة برمجة تطبيقات التضمين في الوقت الفعلي لتحويل الأسئلة إلى متجهات قبل الاسترجاع، وغالبًا ما يصبح هذا التأخير في الوقت الفعلي عنق الزجاجة في الأداء في سلسلة التطبيقات بأكملها.</p>
<p>وتركز معايير التضمين الشائعة مثل MTEB على دقة الاستدعاء أو حجم النموذج، وغالبًا ما تتجاهل مقياس الأداء الحاسم - زمن انتقال واجهة برمجة التطبيقات. باستخدام وظيفة ميلفوس <code translate="no">TextEmbedding</code> ، أجرينا اختبارات شاملة في العالم الحقيقي على مزودي خدمات التضمين الرئيسيين في أمريكا الشمالية وآسيا.</p>
<p>يظهر زمن انتقال التضمين في مرحلتين حاسمتين:</p>
<h3 id="Query-Time-Impact" class="common-anchor-header">تأثير وقت الاستعلام</h3><p>في سير عمل RAG النموذجي، عندما يطرح المستخدم سؤالاً، يجب على النظام</p>
<ul>
<li><p>تحويل الاستعلام إلى متجه عبر استدعاء واجهة برمجة تطبيقات التضمين</p></li>
<li><p>البحث عن متجهات مشابهة في ميلفوس</p></li>
<li><p>تغذية النتائج والسؤال الأصلي إلى LLM</p></li>
<li><p>توليد الإجابة وإرجاعها</p></li>
</ul>
<p>يفترض العديد من المطورين أن توليد إجابة LLM هو الجزء الأبطأ. ومع ذلك، تخلق العديد من إمكانيات الإخراج المتدفق لدى العديد من LLMs وهمًا بالسرعة - فأنت ترى الرمز الأول بسرعة. في الواقع، إذا كان استدعاء واجهة برمجة التطبيقات المضمنة يستغرق مئات المللي ثانية أو حتى ثواني، يصبح أول - وأكثرها وضوحاً - عنق الزجاجة في سلسلة الاستجابة.</p>
<h3 id="Data-Ingestion-Impact" class="common-anchor-header">تأثير تضمين البيانات</h3><p>سواءً كان إنشاء فهرس من الصفر أو إجراء تحديثات روتينية، يتطلب الاستيعاب الجماعي تضمين آلاف أو ملايين القطع النصية. إذا واجهت كل مكالمة تضمين وقت استجابة مرتفع، فإن خط أنابيب البيانات بأكمله يتباطأ بشكل كبير، مما يؤخر إصدارات المنتجات وتحديثات قاعدة المعرفة.</p>
<p>كلا السيناريوهين يجعل كلا السيناريوهين من زمن انتقال واجهة برمجة التطبيقات المضمنة مقياس أداء غير قابل للتفاوض لأنظمة RAG للإنتاج.</p>
<h2 id="Measuring-Real-World-Embedding-API-Latency-with-Milvus" class="common-anchor-header">قياس زمن انتقال واجهة برمجة التطبيقات المدمجة في العالم الحقيقي باستخدام Milvus<button data-href="#Measuring-Real-World-Embedding-API-Latency-with-Milvus" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Milvus عبارة عن قاعدة بيانات متجهة مفتوحة المصدر وعالية الأداء توفر واجهة جديدة <code translate="no">TextEmbedding</code> Function. تعمل هذه الميزة على دمج نماذج التضمين الشائعة من OpenAI وCohere وAWS Bedrock وGoogle Vertex AI وVoyage AI والعديد من المزودين الآخرين مباشرةً في خط أنابيب البيانات، مما يؤدي إلى تبسيط خط أنابيب البحث المتجه الخاص بك من خلال مكالمة واحدة.</p>
<p>باستخدام واجهة الدالة الجديدة هذه، اختبرنا وقمنا بقياس العديد من واجهات برمجة التطبيقات الشائعة المدمجة من مزودي النماذج الأمريكيين مثل OpenAI وCohere، بالإضافة إلى مزودي النماذج الآسيويين مثل AliCloud وSiliconFlow، وقياس زمن الاستجابة من طرف إلى طرف في سيناريوهات نشر واقعية.</p>
<p>غطت مجموعة الاختبارات الشاملة التي أجريناها مختلف تكوينات النماذج:</p>
<table>
<thead>
<tr><th><strong>المزود</strong></th><th><strong>النموذج</strong></th><th><strong>الأبعاد</strong></th></tr>
</thead>
<tbody>
<tr><td>OpenAI</td><td>تضمين النص-تضمين النص-ADA-002</td><td>1536</td></tr>
<tr><td>OpenAI</td><td>تضمين النص-تضمين النص 3-صغير</td><td>1536</td></tr>
<tr><td>OpenAI</td><td>تضمين النص-تضمين النص-3-كبير</td><td>3072</td></tr>
<tr><td>AWS Bedrock</td><td>amazon.amazon.titan-embed-text-v2:0</td><td>1024</td></tr>
<tr><td>جوجل فيرتكس AI</td><td>تضمين النص-005</td><td>768</td></tr>
<tr><td>جوجل فيرتكس AI</td><td>نص-تضمين نص متعدد اللغات-002</td><td>768</td></tr>
<tr><td>VoyageAI</td><td>Voyage-3-large</td><td>1024</td></tr>
<tr><td>VoyageAI</td><td>رحلة 3</td><td>1024</td></tr>
<tr><td>VoyageAI</td><td>فوياج-3-لايت</td><td>512</td></tr>
<tr><td>VoyageAI</td><td>رمز الرحلة-3</td><td>1024</td></tr>
<tr><td>كوهير</td><td>تضمين-الإنجليزية-v3.0</td><td>1024</td></tr>
<tr><td>كوهير</td><td>تضمين-متعدد اللغات-v3.0</td><td>1024</td></tr>
<tr><td>كوهير</td><td>تضمين-إنجليزي-إنجليزي-خفيف-ف3.0</td><td>384</td></tr>
<tr><td>كوهير</td><td>تضمين-متعدد اللغات-لايت-v3.0</td><td>384</td></tr>
<tr><td>عليون داش سكوب</td><td>تضمين-تضمين-نص-ف1</td><td>1536</td></tr>
<tr><td>عليون داش سكوب</td><td>تضمين النص-تضمين النص-ف2</td><td>1536</td></tr>
<tr><td>عليون داش سكوب</td><td>تضمين النص-تضمين النص-ف3</td><td>1024</td></tr>
<tr><td>سيليكون فلو</td><td>BAAI/bge-large-zh-v1.5</td><td>1024</td></tr>
<tr><td>سيليكون فلو</td><td>BAAI/bge-large-en-v1.5</td><td>1024</td></tr>
<tr><td>سيليكون فلو</td><td>netease-youdao/bce- تضمين-قاعدة-تضمين-إصدار 1</td><td>768</td></tr>
<tr><td>سيليكون فلو</td><td>BAAI/bge-m3</td><td>1024</td></tr>
<tr><td>سيليكون فلو</td><td>Pro/BAAI/Bge-m3</td><td>1024</td></tr>
<tr><td>TEI</td><td>BAAI/Bge-base-ar-v1.5</td><td>768</td></tr>
</tbody>
</table>
<h2 id="7-Key-Findings-from-Our-Benchmarking-Results" class="common-anchor-header">7 النتائج الرئيسية من نتائج المقارنة المعيارية لدينا<button data-href="#7-Key-Findings-from-Our-Benchmarking-Results" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>لقد اختبرنا نماذج التضمين الشهيرة من أمريكا الشمالية وآسيا في ظل أحجام دفعات مختلفة وأطوال الرموز وظروف الشبكة، وقياس متوسط زمن الاستجابة في جميع السيناريوهات. تكشف النتائج التي توصلنا إليها عن رؤى مهمة من شأنها أن تعيد تشكيل طريقة تفكيرك في اختيار واجهة برمجة التطبيقات المضمنة وتحسينها. دعنا نلقي نظرة.</p>
<h3 id="1-Global-Network-Effects-Are-More-Significant-Than-You-Think" class="common-anchor-header">1. تأثيرات الشبكة العالمية أكثر أهمية مما تعتقد</h3><p>ربما تكون بيئة الشبكة هي العامل الأكثر أهمية الذي يؤثر على أداء واجهة برمجة التطبيقات المدمجة. يمكن أن يختلف أداء نفس مزود خدمة تضمين واجهة برمجة التطبيقات بشكل كبير عبر بيئات الشبكة.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/latency_in_Asia_vs_in_US_cb4b5a425a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>عندما يتم نشر تطبيقك في آسيا ويصل إلى خدمات مثل OpenAI أو Cohere أو VoyageAI المنتشرة في أمريكا الشمالية، يزداد زمن وصول الشبكة بشكل كبير. تُظهر اختباراتنا في العالم الواقعي أن زمن انتقال مكالمات واجهة برمجة التطبيقات قد زاد عالمياً من <strong>3 إلى 4 مرات</strong>!</p>
<p>على العكس من ذلك، عندما يتم نشر تطبيقك في أمريكا الشمالية ويصل إلى خدمات آسيوية مثل AliCloud Dashscope أو SiliconFlow، يكون تدهور الأداء أكثر حدة. أظهر تطبيق SiliconFlow، على وجه الخصوص، زيادات في زمن الوصول إلى الكمون <strong>بحوالي 100 مرة</strong> في السيناريوهات عبر المناطق!</p>
<p>هذا يعني أنه يجب عليك دائمًا اختيار موفري التضمين استنادًا إلى موقع النشر وجغرافية المستخدم - فمطالبات الأداء بدون سياق الشبكة لا معنى لها.</p>
<h3 id="2-Model-Performance-Rankings-Reveal-Surprising-Results" class="common-anchor-header">2. تصنيفات أداء النموذج تكشف عن نتائج مفاجئة</h3><p>كشف اختبار الكمون الشامل الذي أجريناه عن تسلسل هرمي واضح للأداء:</p>
<ul>
<li><p><strong>نماذج أمريكا الشمالية (متوسط زمن الاستجابة)</strong>: Cohere &gt; Google Vertex AI &gt; VoyageAI &gt; OpenAI &gt; AWS Bedrock</p></li>
<li><p>النماذج<strong>الآسيوية (متوسط زمن الاستجابة)</strong>: SiliconFlow &gt; AliCloud Dashscope</p></li>
</ul>
<p>تتحدى هذه التصنيفات الحكمة التقليدية حول اختيار المزود.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/median_latency_with_batch_size_1_ef83bec9c8.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/median_latency_with_batch_size_10_0d4e52566f.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/all_model_latency_vs_token_length_when_batch_size_is_10_537516cc1c.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/all_model_latency_vstoken_lengthwhen_batch_size_is_10_4dcf0d549a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>ملحوظة: نظراً للتأثير الكبير لبيئة الشبكة والمناطق الجغرافية للخوادم على زمن انتقال واجهة برمجة التطبيقات المدمجة في الوقت الحقيقي، قمنا بمقارنة زمن انتقال نماذج أمريكا الشمالية وآسيا بشكل منفصل.</p>
<h3 id="3-Model-Size-Impact-Varies-Dramatically-by-Provider" class="common-anchor-header">3. يختلف تأثير حجم النموذج بشكل كبير حسب المزود</h3><p>لاحظنا اتجاهاً عاماً حيث تتميز النماذج الأكبر حجماً بزمن كمون أعلى من النماذج القياسية، والتي تتميز بزمن كمون أعلى من النماذج الأصغر/الأقل حجماً. ومع ذلك، لم يكن هذا النمط عامًا وكشف عن رؤى مهمة حول بنية الواجهة الخلفية. على سبيل المثال:</p>
<ul>
<li><p>أظهر<strong>Cohere و OpenAI</strong> فجوات أداء ضئيلة بين أحجام النماذج</p></li>
<li><p>أظهر<strong>VoyageAI</strong> اختلافات واضحة في الأداء بناءً على حجم النموذج</p></li>
</ul>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_1_f9eaf2be26.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_2_cf4d72d1ad.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_3_5e0c8d890b.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>يشير هذا إلى أن وقت استجابة واجهة برمجة التطبيقات يعتمد على عوامل متعددة تتجاوز بنية النموذج، بما في ذلك استراتيجيات تجميع الواجهة الخلفية، وتحسين معالجة الطلبات، والبنية التحتية الخاصة بالمزود. الدرس المستفاد واضح: <em>لا تثق بحجم النموذج أو تاريخ الإصدار كمؤشرات أداء موثوقة - اختبر دائمًا في بيئة النشر الخاصة بك.</em></p>
<h3 id="4-Token-Length-and-Batch-Size-Create-Complex-Trade-offs" class="common-anchor-header">4. طول الرمز وحجم الدفعة يخلقان مقايضات معقدة</h3><p>اعتمادًا على تنفيذ الواجهة الخلفية، وخاصةً استراتيجيات التجميع، قد لا يؤثر طول الرمز المميز بشكل كبير على زمن الاستجابة حتى تزداد أحجام الدُفعات. كشفت اختباراتنا عن أنماط مميزة:</p>
<ul>
<li><p>ظل<strong>زمن انتقال OpenAI</strong> ثابتًا إلى حد ما بين الدفعات الصغيرة والكبيرة، مما يشير إلى قدرات تجميع خلفية سخية</p></li>
<li><p>أظهر<strong>VoyageAI</strong> تأثيرات واضحة على طول الرمز المميز، مما يشير إلى الحد الأدنى من تحسين الدُفعات الخلفية</p></li>
</ul>
<p>تزيد أحجام الدفعات الأكبر من زمن الاستجابة المطلق، لكنها تحسن الإنتاجية الإجمالية. في اختباراتنا، أدى الانتقال من الدفعة = 1 إلى الدفعة = 10 إلى زيادة زمن الاستجابة بمقدار 2×- 5×، مع زيادة الإنتاجية الإجمالية بشكل كبير. يمثل هذا فرصة تحسين حاسمة لسير عمل المعالجة المجمعة حيث يمكنك مقايضة زمن انتقال الطلب الفردي مقابل تحسين الإنتاجية الإجمالية للنظام بشكل كبير.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Going_from_batch_1_to_10_latency_increased_2_5_9811536a3c.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>بالانتقال من الدفعة=1 إلى 10، زاد زمن الاستجابة بمقدار 2×-5×</p>
<h3 id="5-API-Reliability-Introduces-Production-Risk" class="common-anchor-header">5. موثوقية واجهة برمجة التطبيقات تقدم مخاطر الإنتاج</h3><p>لاحظنا تباينًا كبيرًا في زمن الاستجابة، خاصةً مع OpenAI و VoyageAI، مما يؤدي إلى عدم القدرة على التنبؤ في أنظمة الإنتاج.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Latency_variance_when_batch_1_d9cd88fb73.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>تباين الكمون عندما تكون الدفعة = 1</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Latency_variance_when_batch_10_5efc33bf4e.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>تباين الكمون عندما تكون الدفعة = 10</p>
<p>بينما ركز اختبارنا في المقام الأول على زمن الاستجابة، فإن الاعتماد على أي واجهة برمجة تطبيقات خارجية ينطوي على مخاطر فشل متأصلة، بما في ذلك تقلبات الشبكة وتحديد معدل المزودين وانقطاع الخدمة. في غياب اتفاقيات مستوى الخدمة الواضحة من مقدمي الخدمة، يجب على المطورين تنفيذ استراتيجيات قوية للتعامل مع الأخطاء، بما في ذلك إعادة المحاولة والمهلات وقواطع الدائرة للحفاظ على موثوقية النظام في بيئات الإنتاج.</p>
<h3 id="6-Local-Inference-Can-Be-Surprisingly-Competitive" class="common-anchor-header">6. الاستدلال المحلي يمكن أن يكون تنافسيًا بشكل مدهش</h3><p>كشفت اختباراتنا أيضاً أن نشر نماذج التضمين متوسطة الحجم محلياً يمكن أن يوفر أداءً يضاهي أداء واجهات برمجة التطبيقات السحابية - وهي نتيجة مهمة للتطبيقات التي تراعي الميزانية أو التطبيقات الحساسة لزمن الاستجابة.</p>
<p>على سبيل المثال، فإن نشر المصدر المفتوح <code translate="no">bge-base-en-v1.5</code> عبر TEI (استدلال تضمين النص) على وحدة معالجة مركزية متواضعة 4c8g يطابق أداء زمن الوصول إلى سيليكون فلو (SiliconFlow)، مما يوفر بديلاً محلياً للاستدلال بأسعار معقولة. وتعد هذه النتيجة مهمة بشكل خاص للمطورين الأفراد والفرق الصغيرة الذين قد يفتقرون إلى موارد وحدة معالجة الرسومات على مستوى المؤسسات ولكنهم لا يزالون بحاجة إلى قدرات تضمين عالية الأداء.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/TEI_Latency_2f09be1ef0.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>كمون TEI</p>
<h3 id="7-Milvus-Overhead-Is-Negligible" class="common-anchor-header">7. نفقات Milvus الزائدة لا تُذكر</h3><p>نظرًا لأننا استخدمنا Milvus لاختبار زمن انتقال واجهة برمجة تطبيقات التضمين، فقد تحققنا من أن النفقات الإضافية التي قدمتها وظيفة تضمين النص في Milvus صغيرة للغاية ولا تكاد تذكر. تُظهر قياساتنا أن عمليات Milvus لا تضيف سوى 20-40 مللي ثانية إجمالاً، بينما تستغرق مكالمات تضمين واجهة برمجة التطبيقات مئات المللي ثانية إلى عدة ثوانٍ - مما يعني <strong>أن Milvus يضيف أقل من 5% من النفقات العامة</strong> إلى إجمالي وقت العملية. يكمن عنق الزجاجة في الأداء في المقام الأول في نقل الشبكة وقدرات المعالجة الخاصة بمزودي خدمة تضمين واجهة برمجة التطبيقات، وليس في طبقة خادم Milvus.</p>
<h2 id="Tips-How-to-Optimize-Your-RAG-Embedding-Performance" class="common-anchor-header">نصائح: كيفية تحسين أداء التضمين RAG الخاص بك<button data-href="#Tips-How-to-Optimize-Your-RAG-Embedding-Performance" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>استنادًا إلى معاييرنا الشاملة، نوصي بالاستراتيجيات التالية لتحسين أداء تضمين نظام RAG الخاص بك:</p>
<h3 id="1-Always-Localize-Your-Testing" class="common-anchor-header">1. توطين الاختبار الخاص بك دائمًا</h3><p>لا تثق بشكل أعمى في أي تقارير معيارية عامة (بما في ذلك هذا التقرير!). يجب عليك دائمًا اختبار النماذج داخل بيئة النشر الفعلية الخاصة بك بدلاً من الاعتماد فقط على المعايير المنشورة. يمكن أن تؤثر ظروف الشبكة والقرب الجغرافي واختلافات البنية التحتية بشكل كبير على الأداء في العالم الحقيقي.</p>
<h3 id="2-Geo-Match-Your-Providers-Strategically" class="common-anchor-header">2. طابق مزوديك جغرافيًا بشكل استراتيجي</h3><ul>
<li><p><strong>بالنسبة لعمليات النشر في أمريكا الشمالية</strong>: ضع في اعتبارك Cohere أو VoyageAI أو OpenAI/Azure أو GCP Vertex AI - وقم دائمًا بإجراء التحقق من صحة أدائك</p></li>
<li><p><strong>لعمليات النشر الآسيوية</strong>: فكر بجدية في موفري النماذج الآسيوية مثل AliCloud Dashscope أو SiliconFlow، والتي تقدم أداءً إقليمياً أفضل</p></li>
<li><p><strong>للجماهير العالمية</strong>: تنفيذ التوجيه متعدد المناطق أو اختيار مزودين ذوي بنية تحتية موزعة عالميًا لتقليل عقوبات الكمون عبر المناطق</p></li>
</ul>
<h3 id="3-Question-Default-Provider-Choices" class="common-anchor-header">3. سؤال اختيارات الموفر الافتراضي</h3><p>تحظى نماذج تضمين OpenAI بشعبية كبيرة لدرجة أن العديد من الشركات والمطورين يختارونها كخيارات افتراضية. ومع ذلك، فقد كشفت اختباراتنا أن زمن انتقال OpenAI وثباته كانا متوسطين في أحسن الأحوال، على الرغم من شعبيته في السوق. تحدى الافتراضات حول "أفضل" مقدمي الخدمات من خلال معاييرك الصارمة الخاصة بك - لا ترتبط الشعبية دائمًا بالأداء الأمثل لحالة استخدامك المحددة.</p>
<h3 id="4-Optimize-Batch-and-Chunk-Configurations" class="common-anchor-header">4. تحسين تكوينات الدفعات والقطع</h3><p>تكوين واحد لا يناسب جميع النماذج أو حالات الاستخدام. يختلف الحجم الأمثل للدُفعات وطول القطع بشكل كبير بين مقدمي الخدمة بسبب اختلاف بنيات الواجهة الخلفية واستراتيجيات التجميع. قم بالتجربة بشكل منهجي مع تكوينات مختلفة للعثور على نقطة الأداء الأمثل، مع الأخذ في الاعتبار مقايضات الإنتاجية مقابل الكمون لمتطلبات تطبيقك المحددة.</p>
<h3 id="5-Implement-Strategic-Caching" class="common-anchor-header">5. تنفيذ التخزين المؤقت الاستراتيجي</h3><p>بالنسبة للاستعلامات عالية التردد، قم بتخزين نص الاستعلام والتضمينات التي تم إنشاؤها مؤقتًا (باستخدام حلول مثل Redis). يمكن أن تصل الاستعلامات المتطابقة اللاحقة مباشرةً إلى ذاكرة التخزين المؤقت، مما يقلل من زمن الاستجابة إلى أجزاء من الثانية. يمثل هذا أحد أكثر التقنيات المتاحة لتحسين زمن انتقال الاستعلام فعالية من حيث التكلفة والتأثير.</p>
<h3 id="6-Consider-Local-Inference-Deployment" class="common-anchor-header">6. النظر في نشر الاستدلال المحلي</h3><p>إذا كانت لديك متطلبات عالية للغاية فيما يتعلق بزمن انتقال البيانات وزمن انتقال الاستعلام وخصوصية البيانات، أو إذا كانت تكاليف مكالمات واجهة برمجة التطبيقات باهظة، ففكر في نشر نماذج التضمين محليًا للاستدلال. غالبًا ما تأتي خطط واجهة برمجة التطبيقات القياسية مصحوبة بقيود QPS، وزمن انتقال غير مستقر، ونقص في ضمانات اتفاقية مستوى الخدمة - وهي قيود قد تكون إشكالية لبيئات الإنتاج.</p>
<p>بالنسبة للعديد من المطورين الأفراد أو الفرق الصغيرة، قد يبدو نقص وحدات معالجة الرسومات على مستوى المؤسسات عائقاً أمام النشر المحلي لنماذج التضمين عالية الأداء. ومع ذلك، هذا لا يعني التخلي عن الاستدلال المحلي بالكامل. فبالاقتران مع محركات الاستدلال عالية الأداء مثل محرك الاستدلال <a href="https://github.com/huggingface/text-embeddings-inference">النصي Hugging Face،</a> يمكن حتى لتشغيل نماذج التضمين الصغيرة والمتوسطة الحجم على وحدة المعالجة المركزية أن يحقق أداءً لائقًا قد يتفوق على مكالمات واجهة برمجة التطبيقات ذات الكمون العالي، خاصةً بالنسبة لتوليد التضمين على نطاق واسع دون اتصال بالإنترنت.</p>
<p>يتطلب هذا النهج دراسة متأنية للمفاضلة بين التكلفة والأداء وتعقيد الصيانة.</p>
<h2 id="How-Milvus-Simplifies-Your-Embedding-Workflow" class="common-anchor-header">كيف يبسّط ميلفوس سير عمل التضمين الخاص بك<button data-href="#How-Milvus-Simplifies-Your-Embedding-Workflow" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>كما ذكرنا، فإن Milvus ليس مجرد قاعدة بيانات متجهة عالية الأداء - فهو يوفر أيضًا واجهة دالة تضمين مريحة تتكامل بسلاسة مع نماذج التضمين الشائعة من مختلف مقدمي الخدمات مثل OpenAI وCohere وAWS Bedrock وGoogle Vertex AI وVoyage AI وغيرها من جميع أنحاء العالم في خط أنابيب البحث المتجه الخاص بك.</p>
<p>يتجاوز Milvus حدود تخزين المتجهات واسترجاعها بميزات تعمل على تبسيط تكامل التضمين:</p>
<ul>
<li><p><strong>الإدارة الفعالة للمتجهات</strong>: بصفتها قاعدة بيانات عالية الأداء مصممة لمجموعات المتجهات الضخمة، توفر Milvus تخزينًا موثوقًا وخيارات فهرسة مرنة (HNSW، و IVF، و RaBitQ، و DiskANN، وغيرها)، وقدرات استرجاع سريعة ودقيقة.</p></li>
<li><p><strong>تبسيط تبديل الموفرين</strong>: يوفر Milvus واجهة <code translate="no">TextEmbedding</code> وظيفة، مما يسمح لك بتكوين الوظيفة باستخدام مفاتيح واجهة برمجة التطبيقات الخاصة بك، وتبديل الموفرين أو النماذج على الفور، وقياس الأداء في العالم الحقيقي دون تكامل SDK معقد.</p></li>
<li><p><strong>خطوط أنابيب البيانات من النهاية إلى النهاية</strong>: اتصل <code translate="no">insert()</code> بالنص الخام، وسيقوم Milvus تلقائيًا بتضمين المتجهات وتخزينها في عملية واحدة، مما يبسّط بشكل كبير كود خط أنابيب البيانات الخاص بك.</p></li>
<li><p><strong>تحويل النص إلى نتائج في مكالمة واحدة</strong>: اتصل على <code translate="no">search()</code> باستخدام استعلامات نصية، وسيتولى Milvus عملية التضمين والبحث وإرجاع النتائج - كل ذلك في مكالمة واحدة لواجهة برمجة التطبيقات.</p></li>
<li><p><strong>تكامل لا يعتمد على المزود</strong>: يُلخص Milvus تفاصيل تنفيذ الموفر؛ فقط قم بتكوين الوظيفة ومفتاح واجهة برمجة التطبيقات مرة واحدة، وستكون جاهزًا للانطلاق.</p></li>
<li><p><strong>توافق النظام البيئي مفتوح المصدر</strong>: سواء كنت تنشئ التضمينات عبر دالتنا المدمجة <code translate="no">TextEmbedding</code> أو الاستدلال المحلي أو أي طريقة أخرى، توفر Milvus قدرات تخزين واسترجاع موحدة.</p></li>
</ul>
<p>وهذا يخلق تجربة "إدخال البيانات واستخراجها" مبسطة حيث تتعامل Milvus مع توليد المتجهات داخليًا، مما يجعل كود التطبيق الخاص بك أكثر وضوحًا وقابلية للصيانة.</p>
<h2 id="Conclusion-The-Performance-Truth-Your-RAG-System-Needs" class="common-anchor-header">الخاتمة: حقيقة الأداء التي يحتاجها نظام RAG الخاص بك<button data-href="#Conclusion-The-Performance-Truth-Your-RAG-System-Needs" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>القاتل الصامت لأداء RAG ليس هو المكان الذي يبحث فيه معظم المطورين. بينما تضخ الفرق مواردها في الهندسة السريعة وتحسين LLM، فإن تضمين زمن انتقال واجهة برمجة التطبيقات يخرب بهدوء تجربة المستخدم بتأخيرات قد تكون أسوأ 100 مرة من المتوقع. وتكشف معاييرنا الشاملة الحقيقة القاسية: فالشعبية لا تعني الأداء، والجغرافيا أكثر أهمية من اختيار الخوارزمية في كثير من الحالات، والاستدلال المحلي يتفوق أحياناً على واجهات برمجة التطبيقات السحابية المكلفة.</p>
<p>تسلط هذه النتائج الضوء على نقطة عمياء حاسمة في تحسين RAG. إن عقوبات الكمون عبر المناطق، وتصنيفات أداء المزود غير المتوقعة، والقدرة التنافسية المدهشة للاستدلال المحلي ليست حالات حافة - إنها حقائق إنتاجية تؤثر على التطبيقات الحقيقية. يعد فهم وقياس أداء واجهة برمجة التطبيقات المضمّنة أمرًا ضروريًا لتقديم تجارب مستخدمين سريعة الاستجابة.</p>
<p>يعد اختيارك لموفر التضمين أحد الأجزاء المهمة في لغز أداء RAG الخاص بك. من خلال الاختبار في بيئة النشر الفعلية الخاصة بك، واختيار مزودين مناسبين جغرافيًا، والنظر في بدائل مثل الاستدلال المحلي، يمكنك التخلص من مصدر رئيسي للتأخير في مواجهة المستخدم وبناء تطبيقات ذكاء اصطناعي سريعة الاستجابة حقًا.</p>
