{"codeList":["# The core configuration demonstrates the parallel processing design\nconfig = RAGAnythingConfig(\n    working_dir=\"./rag_storage\",\n    parser=\"mineru\",\n    parse_method=\"auto\",  # Automatically selects the optimal parsing strategy\n    enable_image_processing=True,\n    enable_table_processing=True, \n    enable_equation_processing=True,\n    max_workers=8  # Supports multi-threaded parallel processing\n)\n","- python -m venv .venv && source .venv/bin/activate  # For Windows users:  .venvScriptsactivate\n- pip install -r requirements-min.txt\n- cp .env.example .env #add DASHSCOPE_API_KEY\n","python minimal_[main.py](<http://main.py>)\n",".\n├─ requirements-min.txt\n├─ .env.example\n├─ [config.py](<http://config.py>)\n├─ milvus_[store.py](<http://store.py>)\n├─ [adapters.py](<http://adapters.py>)\n├─ minimal_[main.py](<http://main.py>)\n└─ sample\n   ├─ docs\n   │  └─ faq_milvus.txt\n   └─ images\n      └─ milvus_arch.png\n","raganything\nlightrag\npymilvus[lite]>=2.3.0\naiohttp>=3.8.0\norjson>=3.8.0\npython-dotenv>=1.0.0\nPillow>=9.0.0\nnumpy>=1.21.0,<2.0.0\nrich>=12.0.0\n","# Alibaba Cloud DashScope\nDASHSCOPE_API_KEY=your_api_key_here\n# If the endpoint changes in future releases, please update it accordingly.\nALIYUN_LLM_URL=https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\nALIYUN_VLM_URL=https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions\nALIYUN_EMBED_URL=https://dashscope.aliyuncs.com/api/v1/services/embeddings/text-embedding\n# Model names (configure all models here for consistency)\nLLM_TEXT_MODEL=qwen-max\nLLM_VLM_MODEL=qwen-vl-max\nEMBED_MODEL=tongyi-embedding-vision-plus\n# Milvus Lite\nMILVUS_URI=milvus_lite.db\nMILVUS_COLLECTION=rag_multimodal_collection\nEMBED_DIM=1152\n","import os\nfrom dotenv import load_dotenv\nload_dotenv()\nDASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\", \"\")\nLLM_TEXT_MODEL = os.getenv(\"LLM_TEXT_MODEL\", \"qwen-max\")\nLLM_VLM_MODEL = os.getenv(\"LLM_VLM_MODEL\", \"qwen-vl-max\")\nEMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"tongyi-embedding-vision-plus\")\nALIYUN_LLM_URL = os.getenv(\"ALIYUN_LLM_URL\")\nALIYUN_VLM_URL = os.getenv(\"ALIYUN_VLM_URL\")\nALIYUN_EMBED_URL = os.getenv(\"ALIYUN_EMBED_URL\")\nMILVUS_URI = os.getenv(\"MILVUS_URI\", \"milvus_lite.db\")\nMILVUS_COLLECTION = os.getenv(\"MILVUS_COLLECTION\", \"rag_multimodal_collection\")\nEMBED_DIM = int(os.getenv(\"EMBED_DIM\", \"1152\"))\n# Basic runtime parameters\nTIMEOUT = 60\nMAX_RETRIES = 2\n","import os\nimport base64\nimport aiohttp\nimport asyncio\nfrom typing import List, Dict, Any, Optional\nfrom config import (\n    DASHSCOPE_API_KEY, LLM_TEXT_MODEL, LLM_VLM_MODEL, EMBED_MODEL,\n    ALIYUN_LLM_URL, ALIYUN_VLM_URL, ALIYUN_EMBED_URL, EMBED_DIM, TIMEOUT\n)\nHEADERS = {\n    \"Authorization\": f\"Bearer {DASHSCOPE_API_KEY}\",\n    \"Content-Type\": \"application/json\",\n}\nclass AliyunLLMAdapter:\n    def __init__(self):\n        self.text_url = ALIYUN_LLM_URL\n        self.vlm_url = ALIYUN_VLM_URL\n        self.text_model = LLM_TEXT_MODEL\n        self.vlm_model = LLM_VLM_MODEL\n    async def chat(self, prompt: str) -> str:\n        payload = {\n            \"model\": self.text_model,\n            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": prompt}]},\n            \"parameters\": {\"max_tokens\": 1024, \"temperature\": 0.5},\n        }\n        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=TIMEOUT)) as s:\n            async with [s.post](<http://s.post>)(self.text_url, json=payload, headers=HEADERS) as r:\n                r.raise_for_status()\n                data = await r.json()\n                return data[\"output\"][\"choices\"][0][\"message\"][\"content\"]\n    async def chat_vlm_with_image(self, prompt: str, image_path: str) -> str:\n        with open(image_path, \"rb\") as f:\n            image_b64 = base64.b64encode([f.read](<http://f.read>)()).decode(\"utf-8\")\n        payload = {\n            \"model\": self.vlm_model,\n            \"input\": {\"messages\": [{\"role\": \"user\", \"content\": [\n                {\"text\": prompt},\n                {\"image\": f\"data:image/png;base64,{image_b64}\"}\n            ]}]},\n            \"parameters\": {\"max_tokens\": 1024, \"temperature\": 0.2},\n        }\n        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=TIMEOUT)) as s:\n            async with [s.post](<http://s.post>)(self.vlm_url, json=payload, headers=HEADERS) as r:\n                r.raise_for_status()\n                data = await r.json()\n                return data[\"output\"][\"choices\"][0][\"message\"][\"content\"]\nclass AliyunEmbeddingAdapter:\n    def __init__(self):\n        self.url = ALIYUN_EMBED_URL\n        self.model = EMBED_MODEL\n        self.dim = EMBED_DIM\n    async def embed_text(self, text: str) -> List[float]:\n        payload = {\n            \"model\": self.model,\n            \"input\": {\"texts\": [text]},\n            \"parameters\": {\"text_type\": \"query\", \"dimensions\": self.dim},\n        }\n        async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=TIMEOUT)) as s:\n            async with [s.post](<http://s.post>)(self.url, json=payload, headers=HEADERS) as r:\n                r.raise_for_status()\n                data = await r.json()\n                return data[\"output\"][\"embeddings\"][0][\"embedding\"]\n","import json\nimport time\nfrom typing import List, Dict, Any, Optional\nfrom pymilvus import connections, Collection, CollectionSchema, FieldSchema, DataType, utility\nfrom config import MILVUS_URI, MILVUS_COLLECTION, EMBED_DIM\nclass MilvusVectorStore:\n    def __init__(self, uri: str = MILVUS_URI, collection_name: str = MILVUS_COLLECTION, dim: int = EMBED_DIM):\n        self.uri = uri\n        self.collection_name = collection_name\n        self.dim = dim\n        self.collection: Optional[Collection] = None\n        self._connect_and_prepare()\n    def _connect_and_prepare(self):\n        connections.connect(\"default\", uri=self.uri)\n        if utility.has_collection(self.collection_name):\n            self.collection = Collection(self.collection_name)\n        else:\n            fields = [\n                FieldSchema(name=\"id\", dtype=DataType.VARCHAR, max_length=512, is_primary=True),\n                FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=self.dim),\n                FieldSchema(name=\"content\", dtype=DataType.VARCHAR, max_length=65535),\n                FieldSchema(name=\"content_type\", dtype=DataType.VARCHAR, max_length=32),\n                FieldSchema(name=\"source\", dtype=DataType.VARCHAR, max_length=1024),\n                FieldSchema(name=\"ts\", dtype=[DataType.INT](<http://DataType.INT>)64),\n            ]\n            schema = CollectionSchema(fields, \"Minimal multimodal collection\")\n            self.collection = Collection(self.collection_name, schema)\n            self.collection.create_index(\"vector\", {\n                \"metric_type\": \"COSINE\",\n                \"index_type\": \"IVF_FLAT\",\n                \"params\": {\"nlist\": 1024}\n            })\n        self.collection.load()\n    def upsert(self, ids: List[str], vectors: List[List[float]], contents: List[str],\n               content_types: List[str], sources: List[str]) -> None:\n        data = [\n            ids,\n            vectors,\n            contents,\n            content_types,\n            sources,\n            [int(time.time() * 1000)] * len(ids)\n        ]\n        self.collection.upsert(data)\n        self.collection.flush()\n    def search(self, query_vectors: List[List[float]], top_k: int = 5, content_type: Optional[str] = None):\n        expr = f'content_type == \"{content_type}\"' if content_type else None\n        params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 16}}\n        results = [self.collection.search](<http://self.collection.search>)(\n            data=query_vectors,\n            anns_field=\"vector\",\n            param=params,\n            limit=top_k,\n            expr=expr,\n            output_fields=[\"id\", \"content\", \"content_type\", \"source\", \"ts\"]\n        )\n        out = []\n        for hits in results:\n            out.append([{\n                \"id\": h.entity.get(\"id\"),\n                \"content\": h.entity.get(\"content\"),\n                \"content_type\": h.entity.get(\"content_type\"),\n                \"source\": h.entity.get(\"source\"),\n                \"score\": h.score\n            } for h in hits])\n        return out\n","\"\"\"\nMinimal Working Example:\n- Insert a short text FAQ into LightRAG (text retrieval context)\n- Insert an image description vector into Milvus (image retrieval context)\n- Execute two example queries: one text QA and one image-based QA\n\"\"\"\nimport asyncio\nimport uuid\nfrom pathlib import Path\nfrom rich import print\nfrom lightrag import LightRAG, QueryParam\nfrom lightrag.utils import EmbeddingFunc\nfrom adapters import AliyunLLMAdapter, AliyunEmbeddingAdapter\nfrom milvus_store import MilvusVectorStore\nfrom config import EMBED_DIM\nSAMPLE_DOC = Path(\"sample/docs/faq_milvus.txt\")\nSAMPLE_IMG = Path(\"sample/images/milvus_arch.png\")\nasync def main():\n    # 1) Initialize core components\n    llm = AliyunLLMAdapter()\n    emb = AliyunEmbeddingAdapter()\n    store = MilvusVectorStore()\n    # 2) Initialize LightRAG (for text-only retrieval)\n    async def llm_complete(prompt: str, max_tokens: int = 1024) -> str:\n        return await [llm.chat](<http://llm.chat>)(prompt)\n    async def embed_func(text: str) -> list:\n        return await emb.embed_text(text)\n    rag = LightRAG(\n        working_dir=\"rag_workdir_min\",\n        llm_model_func=llm_complete,\n        embedding_func=EmbeddingFunc(\n            embedding_dim=EMBED_DIM,\n            max_token_size=8192,\n            func=embed_func\n        ),\n    )\n    # 3) Insert text data\n    if SAMPLE_DOC.exists():\n        text = SAMPLE_[DOC.read](<http://DOC.read>)_text(encoding=\"utf-8\")\n        await rag.ainsert(text)\n        print(\"[green]Inserted FAQ text into LightRAG[/green]\")\n    else:\n        print(\"[yellow] sample/docs/faq_milvus.txt not found[/yellow]\")\n    # 4) Insert image data (store description in Milvus)\n    if SAMPLE_IMG.exists():\n        # Use the VLM to generate a description as its semantic content\n        desc = await [llm.chat](<http://llm.chat>)_vlm_with_image(\"Please briefly describe the key components of the Milvus architecture shown in the image.\", str(SAMPLE_IMG))\n        vec = await emb.embed_text(desc)  # Use text embeddings to maintain a consistent vector dimension, simplifying reuse\n        store.upsert(\n            ids=[str(uuid.uuid4())],\n            vectors=[vec],\n            contents=[desc],\n            content_types=[\"image\"],\n            sources=[str(SAMPLE_IMG)]\n        )\n        print(\"[green]Inserted image description into Milvus（content_type=image）[/green]\")\n    else:\n        print(\"[yellow] sample/images/milvus_arch.png not found[/yellow]\")\n    # 5) Query: Text-based QA (from LightRAG)\n    q1 = \"Does Milvus support simultaneous insertion and search? Give a short answer.\"\n    ans1 = await rag.aquery(q1, param=QueryParam(mode=\"hybrid\"))\n    print(\"\\\\n[bold]Text QA[/bold]\")\n    print(ans1)\n    # 6) Query: Image-related QA (from Milvus)\n    q2 = \"What are the key components of the Milvus architecture?\"\n    q2_vec = await emb.embed_text(q2)\n    img_hits = [store.search](<http://store.search>)([q2_vec], top_k=3, content_type=\"image\")\n    print(\"\\\\n[bold]Image Retrieval (returns semantic image descriptions)[/bold]\")\n    print(img_hits[0] if img_hits else [])\nif __name__ == \"__main__\":\n    [asyncio.run](<http://asyncio.run>)(main())\n"],"headingContent":"","anchorList":[{"label":"ما هو RAG-Anything وكيف يعمل؟","href":"What-Is-RAG-Anything-and-How-It-Works","type":2,"isActive":false},{"label":"كيف يتناسب ميلفوس مع RAG-Anything","href":"How-Milvus-Fits-into-RAG-Anything","type":2,"isActive":false},{"label":"كيفية بناء نظام أسئلة وأجوبة متعدد الوسائط باستخدام RAG-Anything و Milvus","href":"How-to-Build-a-Multimodal-QA-System-with-RAG-Anything-and-Milvus","type":2,"isActive":false},{"label":"مستقبل نظام RAG متعدد الوسائط","href":"The-Future-for-Multimodal-RAG","type":2,"isActive":false}]}