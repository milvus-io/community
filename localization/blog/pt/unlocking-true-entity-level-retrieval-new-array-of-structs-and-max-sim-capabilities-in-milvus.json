{"codeList":["{\n    'id': 0,\n    'title': 'Walden',\n    'title_vector': [0.1, 0.2, 0.3, 0.4, 0.5],\n    'author': 'Henry David Thoreau',\n    'year_of_publication': 1845,\n    // highlight-start\n    'chunks': [\n        {\n            'text': 'When I wrote the following pages, or rather the bulk of them...',\n            'text_vector': [0.3, 0.2, 0.3, 0.2, 0.5],\n            'chapter': 'Economy',\n        },\n        {\n            'text': 'I would fain say something, not so much concerning the Chinese and...',\n            'text_vector': [0.7, 0.4, 0.2, 0.7, 0.8],\n            'chapter': 'Economy'\n        }\n    ]\n    // hightlight-end\n}\n","{\n    \"wiki_id\": int,                  # WIKI ID(primary key） \n    \"paragraphs\": ARRAY<STRUCT<      # Array of paragraph structs\n        text:VARCHAR                 # Paragraph text\n        emb: FLOAT_VECTOR(768)       # Embedding for each paragraph\n    >>\n}\n","import pandas as pd\nimport pyarrow as pa\n\n# Load the dataset and group by wiki_id\ndf = pd.read_parquet(\"train-*.parquet\")\ngrouped = df.groupby('wiki_id')\n\n# Build the paragraph array for each article\nwiki_data = []\nfor wiki_id, group in grouped:\n    wiki_data.append({\n        'wiki_id': wiki_id,\n        'paragraphs': [{'text': row['text'], 'emb': row['emb']}\n                       for _, row in group.iterrows()]\n    })\n","from pymilvus import MilvusClient, DataType\n\nclient = MilvusClient(uri=\"http://localhost:19530\")\nschema = client.create_schema()\nschema.add_field(\"wiki_id\", DataType.INT64, is_primary=True)\n\n# Define the Struct schema\nstruct_schema = client.create_struct_field_schema()\nstruct_schema.add_field(\"text\", DataType.VARCHAR, max_length=65535)\nstruct_schema.add_field(\"emb\", DataType.FLOAT_VECTOR, dim=768)\n\nschema.add_field(\"paragraphs\", DataType.ARRAY,\n                 element_type=DataType.STRUCT,\n                 struct_schema=struct_schema, max_capacity=200)\n\nclient.create_collection(\"wiki_docs\", schema=schema)\n","# Batch insert documents\nclient.insert(\"wiki_docs\", wiki_data)\n\n# Create an HNSW index\nindex_params = client.prepare_index_params()\nindex_params.add_index(\n    field_name=\"paragraphs[emb]\",\n    index_type=\"HNSW\",\n    metric_type=\"MAX_SIM_COSINE\",\n    params={\"M\": 16, \"efConstruction\": 200}\n)\nclient.create_index(\"wiki_docs\", index_params)\nclient.load_collection(\"wiki_docs\")\n","# Search query\nimport cohere\nfrom pymilvus.client.embedding_list import EmbeddingList\n\n# The dataset uses Cohere's multilingual-22-12 embedding model, so we must embed the query using the same model.\nco = cohere.Client(f\"<<COHERE_API_KEY>>\")\nquery = 'Who founded Youtube'\nresponse = co.embed(texts=[query], model='multilingual-22-12')\nquery_embedding = response.embeddings\nquery_emb_list = EmbeddingList()\n\nfor vec in query_embedding[0]:\n    query_emb_list.add(vec)\n\nresults = client.search(\n    collection_name=\"wiki_docs\",\n    data=[query_emb_list],\n    anns_field=\"paragraphs[emb]\",\n    search_params={\n        \"metric_type\": \"MAX_SIM_COSINE\",\n        \"params\": {\"ef\": 200, \"retrieval_ann_ratio\": 3}\n    },\n    limit=10,\n    output_fields=[\"wiki_id\"]\n)\n\n# Results: directly return 10 full articles!\nfor hit in results[0]:\n    print(f\"Article {hit['entity']['wiki_id']}: Score {hit['distance']:.4f}\")\n","{\n    \"page_id\": int,                     # Page ID (primary key) \n    \"page_number\": int,                 # Page number within the document \n    \"doc_name\": VARCHAR,                # Document name\n    \"patches\": ARRAY<STRUCT<            # Array of patch objects\n        patch_embedding: FLOAT_VECTOR(128)  # Embedding for each patch\n    >>\n}\n","import torch\nfrom PIL import Image\n\nfrom colpali_engine.models import ColPali, ColPaliProcessor\n\nmodel_name = \"vidore/colpali-v1.3\"\n\nmodel = ColPali.from_pretrained(\n    model_name,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n# Example: 2 documents, 5 pages each, total 10 images\nimages = [\n    Image.open(\"path/to/your/image1.png\"), \n    Image.open(\"path/to/your/image2.png\"), \n    ....\n    Image.open(\"path/to/your/image10.png\")\n]\n# Convert each image into multiple patch embeddings\nbatch_images = processor.process_images(images).to(model.device)\nwith torch.no_grad():\n    image_embeddings = model(**batch_images)\n","from pymilvus import MilvusClient, DataType\n\nclient = MilvusClient(uri=\"http://localhost:19530\")\nschema = client.create_schema()\nschema.add_field(\"page_id\", DataType.INT64, is_primary=True)\nschema.add_field(\"page_number\", DataType.INT64)\nschema.add_field(\"doc_name\", DataType.VARCHAR, max_length=500)\n\n# Struct Array for patches\nstruct_schema = client.create_struct_field_schema()\nstruct_schema.add_field(\"patch_embedding\", DataType.FLOAT_VECTOR, dim=128)\n\nschema.add_field(\"patches\", DataType.ARRAY,\n                 element_type=DataType.STRUCT,\n                 struct_schema=struct_schema, max_capacity=2048)\n\nclient.create_collection(\"doc_pages\", schema=schema)\n","# Prepare data for insertion\npage_data=[\n    {\n        \"page_id\": 0,\n        \"page_number\": 0,\n        \"doc_name\": \"Q1_Financial_Report.pdf\",\n        \"patches\": [\n            {\"patch_embedding\": emb} for emb in image_embeddings[0]\n        ],\n    },\n    ...,\n    {\n        \"page_id\": 9,\n        \"page_number\": 4,\n        \"doc_name\": \"Product_Manual.pdf\",\n        \"patches\": [\n            {\"patch_embedding\": emb} for emb in image_embeddings[9]\n        ],\n    },\n]\n\nclient.insert(\"doc_pages\", page_data)\n\n# Create index\nindex_params = client.prepare_index_params()\nindex_params.add_index(\n    field_name=\"patches[patch_embedding]\",\n    index_type=\"HNSW\",\n    metric_type=\"MAX_SIM_IP\",\n    params={\"M\": 32, \"efConstruction\": 200}\n)\nclient.create_index(\"doc_pages\", index_params)\nclient.load_collection(\"doc_pages\")\n","# Run the search\nfrom pymilvus.client.embedding_list import EmbeddingList\n\nqueries = [\n    \"quarterly revenue growth chart\"    \n]\n# Convert the text query into a multi-vector representation\nbatch_queries = processor.process_queries(queries).to(model.device)\nwith torch.no_grad():\n    query_embeddings = model(**batch_queries)\n\nquery_emb_list = EmbeddingList()\nfor vec in query_embeddings[0]:\n    query_emb_list.add(vec)\nresults = client.search(\n    collection_name=\"doc_pages\",\n    data=[query_emb_list],\n    anns_field=\"patches[patch_embedding]\",\n    search_params={\n        \"metric_type\": \"MAX_SIM_IP\",\n        \"params\": {\"ef\": 100, \"retrieval_ann_ratio\": 3}\n    },\n    limit=3,\n    output_fields=[\"page_id\", \"doc_name\", \"page_number\"]\n)\n\n\nprint(f\"Query: '{queries[0]}'\")\nfor i, hit in enumerate(results, 1):\n    entity = hit['entity']\n    print(f\"{i}. {entity['doc_name']} - Page {entity['page_number']}\")\n    print(f\"   Score: {hit['distance']:.4f}\\n\")\n","Query: 'quarterly revenue growth chart'\n1. Q1_Financial_Report.pdf - Page 2\n   Score: 0.9123\n\n2. Q1_Financial_Report.pdf - Page 1\n   Score: 0.7654\n\n3. Product_Manual.pdf - Page 1\n   Score: 0.5231\n"],"headingContent":"","anchorList":[{"label":"O que é um Array of Structs?","href":"What-is-an-Array-of-Structs","type":2,"isActive":false},{"label":"Como um conjunto de estruturas funciona com o MAX_SIM?","href":"How-Does-an-Array-of-Structs-Work-with-MAXSIM","type":2,"isActive":false},{"label":"Porque é que MAX_SIM + Array of Structs são importantes na Base de Dados Vetorial","href":"Why-MAXSIM-+-Array-of-Structs-Matter-in-Vector-Database","type":2,"isActive":false},{"label":"Quando usar uma Matriz de Estruturas","href":"When-to-Use-an-Array-of-Structs","type":2,"isActive":false},{"label":"Tutoriais práticos: Recuperação em nível de documento com a matriz de estruturas","href":"Hands-on-Tutorials-Document-Level-Retrieval-with-the-Array-of-Structs","type":2,"isActive":false},{"label":"Conclusão","href":"Conclusion","type":2,"isActive":false}]}