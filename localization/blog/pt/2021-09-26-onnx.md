---
id: 2021-09-26-onnx.md
title: Processar modelos com ONNX
date: 2021-09-26T00:00:00.000Z
desc: >-
  como utilizar vários modelos para a pesquisa de imagens com base em ONNX e
  Milvus
cover: assets.zilliz.com/medium_1_cfb009269a.png
tag: Engineering
canonicalUrl: >-
  https://zilliz.com/blog/combine-ai-models-for-image-search-using-onnx-and-milvus
---
<custom-h1>Combinar modelos de IA para pesquisa de imagens utilizando ONNX e Milvus</custom-h1><p>O Open Neural Network Exchange (ONNX) é um formato aberto criado para representar modelos de aprendizagem automática. Desde que foi aberto em 2017, o ONNX tornou-se um padrão para IA, fornecendo blocos de construção para modelos de aprendizagem automática e aprendizagem profunda. O ONNX define um formato de ficheiro comum para permitir que os programadores de IA utilizem modelos com várias estruturas, ferramentas, tempos de execução e compiladores, e ajuda a aumentar a velocidade de inovação na comunidade de inteligência artificial.</p>
<p>Milvus é uma base de dados vetorial de código aberto que é altamente flexível, fiável e extremamente rápida. Suporta a adição, eliminação, atualização e pesquisa de vectores quase em tempo real. Milvus tem um conjunto abrangente de APIs intuitivas e suporte para várias bibliotecas de índices amplamente adoptadas (por exemplo, Faiss, NMSLIB e Annoy), simplificando a seleção de índices para um determinado cenário. O Milvus é simples de usar e tem sido utilizado em centenas de organizações e instituições em todo o mundo, incluindo pesquisa de imagem, áudio e vídeo, recomendação, chatbot, pesquisa de novos medicamentos, etc.</p>
<p>Este artigo irá apresentar-lhe como utilizar vários modelos para a pesquisa de imagens com base no ONNX e no Milvus. Toma os modelos VGG16 e ResNet50 como exemplos, utiliza o ONNX para executar diferentes modelos de IA para gerar vectores de caraterísticas e, por fim, executa a recuperação de vectores de caraterísticas em Milvus para devolver imagens semelhantes.</p>
<h2 id="Process-Models-with-ONNX" class="common-anchor-header">Processar modelos com ONNX<button data-href="#Process-Models-with-ONNX" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>O formato ONNX pode ser facilmente trocado entre modelos de IA. Por exemplo, o modelo TensorFlow pode ser convertido para o formato ONNX e executado no ambiente Caffe. Neste exemplo, convertemos o modelo ResNet50 pré-treinado no âmbito da estrutura Keras para o formato ONNX e, em seguida, chamamos o modelo VGG16 no formato ONNX para analisar diferentes modelos.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> keras.applications.resnet50 <span class="hljs-keyword">import</span> ResNet50
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># load keras-resnet50 model and save as a floder</span>
model_resnet50 = ResNet50(include_top=<span class="hljs-literal">False</span>, pooling=<span class="hljs-string">&#x27;max&#x27;</span>, weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>)
tf.saved_model.save(model_resnet50, <span class="hljs-string">&quot;keras_resnet50_model&quot;</span>)

<span class="hljs-comment"># convert resnet50 model to onnx</span>
! python -m tf2onnx.convert --saved-model <span class="hljs-string">&quot;keras_resnet50_model&quot;</span> --output <span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>
<button class="copy-code-btn"></button></code></pre>
<p>Nota: Quando utilizámos a interface <code translate="no">keras2onnx.convert_keras(model, model.name)</code> para converter o modelo, esta devolverá o erro <code translate="no">AttributeError:'KerasTensor' object has no attribute'graph'</code>. Em seguida, podemos usar o comando Bash do Python para converter de acordo com a solução no Stack Overflow.</p>
<h2 id="Extract-Feature-Vectors-using-Models" class="common-anchor-header">Extrair vectores de caraterísticas utilizando modelos<button data-href="#Extract-Feature-Vectors-using-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Depois de converter o modelo ResNet50 para o formato ONNX, pode extrair o vetor de caraterísticas da imagem diretamente através da inferência. Nota: Os vectores de caraterísticas têm de ser normalizados após a extração.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># get the image vectors with onnx model</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_onnx_vectors</span>(<span class="hljs-params">onnx_model, img_path</span>):
    img = image.load_img(img_path, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=<span class="hljs-number">0</span>)
    x = preprocess_input(x)
    
    sess = onnxruntime.InferenceSession(onnx_model)
    x = x <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> [x]
    feed = <span class="hljs-built_in">dict</span>([(<span class="hljs-built_in">input</span>.name, x[n]) <span class="hljs-keyword">for</span> n, <span class="hljs-built_in">input</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sess.get_inputs())])
    feat = sess.run(<span class="hljs-literal">None</span>, feed)[<span class="hljs-number">0</span>]
    
    norm_feat = feat[<span class="hljs-number">0</span>] / LA.norm(feat[<span class="hljs-number">0</span>])
    norm_feat = [i.item() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> norm_feat]
    <span class="hljs-keyword">return</span> norm_feat
<button class="copy-code-btn"></button></code></pre>
<p>Utilize o modelo VGG16 em formato ONNX para processar dados de imagem:</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># generate vectors with ResNet50 and VGG16 ONNX model</span>
2vec_resnet = get_onnx_vectors(<span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
3vec_vgg = get_onnx_vectors(<span class="hljs-string">&quot;onnx_vgg16.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Store-Vector-Data" class="common-anchor-header">Armazenar dados vectoriais<button data-href="#Store-Vector-Data" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Os dados não estruturados, como as imagens, não podem ser processados diretamente por um computador, mas podem ser convertidos em vectores através de um modelo de IA e depois analisados por um computador. A base de dados vetorial Milvus foi concebida para potenciar a análise maciça de dados não estruturados. Pode armazenar dados vectoriais e efetuar análises quase em tempo real. Em primeiro lugar, crie uma coleção do modelo correspondente em Milvus e, em seguida, insira os vectores de imagem.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> milvus <span class="hljs-keyword">import</span> *

<span class="hljs-comment"># create collections in Milvus</span>
milvus.create_collection(resnet_collection_param)
milvus.create_collection(vgg_collection_param)

<span class="hljs-comment"># insert data to Milvus and return ids</span>
status, resnet_ids = milvus.insert(resnet_collection_name, resnet_vectors)
status, vgg_ids = milvus.insert(vgg_collection_name, vgg_vectors)
<button class="copy-code-btn"></button></code></pre>
<p>Depois de inserir os dados com sucesso, o Milvus devolverá o ID correspondente ao vetor e, em seguida, podemos encontrar a imagem pelo ID. Uma vez que o Milvus 1.1 utilizado neste caso não suporta filtragem escalar (que o Milvus 2.0 suporta agora), o Redis é utilizado para armazenar o ID do vetor e o valor-chave do caminho da imagem.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">import</span> redis
<span class="hljs-keyword">def</span> <span class="hljs-title function_">img_ids_to_redis</span>(<span class="hljs-params">img_directory, res_ids</span>):
  <span class="hljs-keyword">for</span> img, ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images, res_ids):
    redis.<span class="hljs-built_in">set</span>(ids, img)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Search-for-Similar-Images" class="common-anchor-header">Pesquisa de imagens semelhantes<button data-href="#Search-for-Similar-Images" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Depois de armazenar os dados, podemos recuperar o vetor. O Milvus suporta vários métodos de cálculo de distâncias, incluindo a distância Euclidiana, o produto interno e a distância de Hamming. A pesquisa de semelhança de imagens neste artigo adopta o cálculo da distância euclidiana entre os vectores no Milvus, devolve o ID do vetor semelhante e, em seguida, encontra a imagem correspondente ao ID no Redis.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># search in Milvus and return the similarly results with ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">search_in_milvus</span>(<span class="hljs-params">collection_name, search_vector</span>):
    status, results = milvus.search(collection_name, TOP_K, [search_vector])
    <span class="hljs-built_in">print</span>(status)
    re_ids = [x.<span class="hljs-built_in">id</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    re_distance = [x.distance <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    <span class="hljs-keyword">return</span> re_ids, re_distance
    
<span class="hljs-comment"># get the images according the result ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_sim_imgs</span>(<span class="hljs-params">collection_name, search_vector</span>):
    ids, distance = search_in_milvus(collection_name, search_vector)
    img = [red.get(i).decode(<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids]
    <span class="hljs-keyword">return</span> ids, distance, img
<button class="copy-code-btn"></button></code></pre>
<p>Tomando os modelos VGG16 e ResNet50 como exemplos, este artigo mostra o processamento de vários modelos através do ONNX e a combinação de vários modelos com o Milvus para a recuperação de vectores semelhantes, de modo a obter imagens semelhantes. Os dois modelos acima referidos baseiam-se na estrutura Keras, que pode extrair rapidamente vectores de caraterísticas. Pode ver-se no Notebook que, embora os resultados da pesquisa de imagens do Milvus no conjunto de dados COCO com base nestes dois modelos sejam semelhantes, as suas distâncias euclidianas não são as mesmas. Também pode tentar comparar os resultados da pesquisa dos dois modelos utilizando outros conjuntos de dados.</p>
<p>Milvus é uma base de dados vetorial de alto desempenho e altamente disponível que pode ser utilizada para processar vectores de caraterísticas gerados a partir de dados massivos não estruturados. Para obter mais soluções, pode consultar o <a href="https://github.com/milvus-io/bootcamp">bootcamp do Milvus</a>.</p>
<h2 id="References" class="common-anchor-header">Referências<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ol>
<li>https://github.com/onnx/onnx</li>
<li>https://onnx.ai/</li>
<li>https://milvus.io/cn/</li>
<li>https://github.com/milvus-io/bootcamp</li>
</ol>
<h3 id="About-author" class="common-anchor-header">Sobre o autor</h3><p>Shiyu Chen, engenheira de dados na Zilliz, licenciou-se em Ciências Informáticas na Universidade de Xidian. Desde que se juntou à Zilliz, tem vindo a explorar soluções para o Milvus em vários domínios, como a análise de áudio e vídeo, a recuperação de fórmulas de moléculas, etc., o que enriqueceu bastante os cenários de aplicação da comunidade. Atualmente, está a explorar mais soluções interessantes. No seu tempo livre, gosta de desporto e de ler.</p>
