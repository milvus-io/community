---
id: 2021-09-26-onnx.md
title: Modelle mit ONNX verarbeiten
date: 2021-09-26T00:00:00.000Z
desc: >-
  wie man mehrere Modelle für die Bildsuche auf der Grundlage von ONNX und
  Milvus verwendet
cover: assets.zilliz.com/medium_1_cfb009269a.png
tag: Engineering
canonicalUrl: >-
  https://zilliz.com/blog/combine-ai-models-for-image-search-using-onnx-and-milvus
---
<custom-h1>Kombinieren Sie AI-Modelle für die Bildsuche mit ONNX und Milvus</custom-h1><p>Open Neural Network Exchange (ONNX) ist ein offenes Format, das zur Darstellung von Modellen für maschinelles Lernen entwickelt wurde. Seit seiner Freigabe im Jahr 2017 hat sich ONNX zu einem Standard für KI entwickelt und bietet Bausteine für Modelle für maschinelles Lernen und Deep Learning. ONNX definiert ein gemeinsames Dateiformat, das es KI-Entwicklern ermöglicht, Modelle mit verschiedenen Frameworks, Tools, Laufzeiten und Compilern zu verwenden, und trägt dazu bei, die Innovationsgeschwindigkeit in der Gemeinschaft der künstlichen Intelligenz zu erhöhen.</p>
<p>Milvus ist eine quelloffene Vektordatenbank, die äußerst flexibel, zuverlässig und rasend schnell ist. Sie unterstützt das Hinzufügen, Löschen, Aktualisieren und Suchen von Vektoren nahezu in Echtzeit. Milvus verfügt über ein umfassendes Set an intuitiven APIs und unterstützt mehrere weit verbreitete Indexbibliotheken (z.B. Faiss, NMSLIB und Annoy), was die Indexauswahl für ein bestimmtes Szenario vereinfacht. Milvus ist einfach zu bedienen und wurde bereits in Hunderten von Organisationen und Institutionen weltweit eingesetzt, z. B. bei der Bild-, Audio- und Videosuche, bei Empfehlungen, Chatbots, der Suche nach neuen Medikamenten usw.</p>
<p>In diesem Artikel erfahren Sie, wie Sie mehrere Modelle für die Bildsuche auf der Grundlage von ONNX und Milvus verwenden können. Er nimmt VGG16- und ResNet50-Modelle als Beispiele, verwendet ONNX, um verschiedene KI-Modelle laufen zu lassen, um Merkmalsvektoren zu generieren, und führt schließlich ein Merkmalsvektor-Retrieval in Milvus durch, um ähnliche Bilder zurückzugeben.</p>
<h2 id="Process-Models-with-ONNX" class="common-anchor-header">Modelle mit ONNX verarbeiten<button data-href="#Process-Models-with-ONNX" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Das ONNX-Format kann leicht zwischen KI-Modellen ausgetauscht werden. Zum Beispiel kann das TensorFlow-Modell in das ONNX-Format konvertiert und in der Caffe-Umgebung ausgeführt werden. In diesem Beispiel konvertieren wir das vortrainierte ResNet50-Modell unter dem Keras-Framework in das ONNX-Format und rufen dann das VGG16-Modell im ONNX-Format auf, um verschiedene Modelle zu analysieren.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> keras.applications.resnet50 <span class="hljs-keyword">import</span> ResNet50
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># load keras-resnet50 model and save as a floder</span>
model_resnet50 = ResNet50(include_top=<span class="hljs-literal">False</span>, pooling=<span class="hljs-string">&#x27;max&#x27;</span>, weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>)
tf.saved_model.save(model_resnet50, <span class="hljs-string">&quot;keras_resnet50_model&quot;</span>)

<span class="hljs-comment"># convert resnet50 model to onnx</span>
! python -m tf2onnx.convert --saved-model <span class="hljs-string">&quot;keras_resnet50_model&quot;</span> --output <span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>
<button class="copy-code-btn"></button></code></pre>
<p>Hinweis: Wenn wir die Schnittstelle <code translate="no">keras2onnx.convert_keras(model, model.name)</code> verwenden, um das Modell zu konvertieren, gibt sie den Fehler <code translate="no">AttributeError:'KerasTensor' object has no attribute'graph'</code> zurück. Dann können wir den Bash-Befehl von Python zur Konvertierung gemäß der Lösung auf Stack Overflow verwenden.</p>
<h2 id="Extract-Feature-Vectors-using-Models" class="common-anchor-header">Extrahieren von Merkmalsvektoren mit Modellen<button data-href="#Extract-Feature-Vectors-using-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Nach der Konvertierung des ResNet50-Modells in das ONNX-Format können Sie den Merkmalsvektor des Bildes direkt durch die Inferenz extrahieren. Hinweis: Die Feature-Vektoren müssen nach der Extraktion normalisiert werden.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># get the image vectors with onnx model</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_onnx_vectors</span>(<span class="hljs-params">onnx_model, img_path</span>):
    img = image.load_img(img_path, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=<span class="hljs-number">0</span>)
    x = preprocess_input(x)
    
    sess = onnxruntime.InferenceSession(onnx_model)
    x = x <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> [x]
    feed = <span class="hljs-built_in">dict</span>([(<span class="hljs-built_in">input</span>.name, x[n]) <span class="hljs-keyword">for</span> n, <span class="hljs-built_in">input</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sess.get_inputs())])
    feat = sess.run(<span class="hljs-literal">None</span>, feed)[<span class="hljs-number">0</span>]
    
    norm_feat = feat[<span class="hljs-number">0</span>] / LA.norm(feat[<span class="hljs-number">0</span>])
    norm_feat = [i.item() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> norm_feat]
    <span class="hljs-keyword">return</span> norm_feat
<button class="copy-code-btn"></button></code></pre>
<p>Verwenden Sie ein VGG16-Modell im ONNX-Format, um Bilddaten zu verarbeiten:</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># generate vectors with ResNet50 and VGG16 ONNX model</span>
2vec_resnet = get_onnx_vectors(<span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
3vec_vgg = get_onnx_vectors(<span class="hljs-string">&quot;onnx_vgg16.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Store-Vector-Data" class="common-anchor-header">Speichern von Vektordaten<button data-href="#Store-Vector-Data" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Unstrukturierte Daten wie Bilder können nicht direkt von einem Computer verarbeitet werden, aber sie können durch ein KI-Modell in Vektoren umgewandelt und dann von einem Computer analysiert werden. Die Milvus-Vektordatenbank ist für die Analyse umfangreicher unstrukturierter Daten ausgelegt. Sie kann Vektordaten speichern und Analysen nahezu in Echtzeit durchführen. Erstellen Sie zunächst eine Sammlung des entsprechenden Modells in Milvus, und fügen Sie dann die Bildvektoren ein.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> milvus <span class="hljs-keyword">import</span> *

<span class="hljs-comment"># create collections in Milvus</span>
milvus.create_collection(resnet_collection_param)
milvus.create_collection(vgg_collection_param)

<span class="hljs-comment"># insert data to Milvus and return ids</span>
status, resnet_ids = milvus.insert(resnet_collection_name, resnet_vectors)
status, vgg_ids = milvus.insert(vgg_collection_name, vgg_vectors)
<button class="copy-code-btn"></button></code></pre>
<p>Nachdem die Daten erfolgreich eingefügt wurden, gibt Milvus die ID zurück, die dem Vektor entspricht, und dann können wir das Bild anhand der ID finden. Da Milvus 1.1, das in diesem Fall verwendet wird, keine skalare Filterung unterstützt (die Milvus 2.0 jetzt unterstützt), wird Redis verwendet, um die Vektor-ID und den Schlüsselwert des Bildpfads zu speichern.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">import</span> redis
<span class="hljs-keyword">def</span> <span class="hljs-title function_">img_ids_to_redis</span>(<span class="hljs-params">img_directory, res_ids</span>):
  <span class="hljs-keyword">for</span> img, ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images, res_ids):
    redis.<span class="hljs-built_in">set</span>(ids, img)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Search-for-Similar-Images" class="common-anchor-header">Suche nach ähnlichen Bildern<button data-href="#Search-for-Similar-Images" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Nach dem Speichern der Daten können wir den Vektor abrufen. Milvus unterstützt mehrere Methoden zur Abstandsberechnung, darunter Euklidischer Abstand, Inneres Produkt und Hamming-Abstand. Die Bildähnlichkeitssuche in diesem Artikel verwendet die euklidische Abstandsberechnung zwischen den Vektoren in Milvus, gibt die ähnliche Vektor-ID zurück und findet dann das Bild, das der ID in Redis entspricht.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># search in Milvus and return the similarly results with ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">search_in_milvus</span>(<span class="hljs-params">collection_name, search_vector</span>):
    status, results = milvus.search(collection_name, TOP_K, [search_vector])
    <span class="hljs-built_in">print</span>(status)
    re_ids = [x.<span class="hljs-built_in">id</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    re_distance = [x.distance <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    <span class="hljs-keyword">return</span> re_ids, re_distance
    
<span class="hljs-comment"># get the images according the result ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_sim_imgs</span>(<span class="hljs-params">collection_name, search_vector</span>):
    ids, distance = search_in_milvus(collection_name, search_vector)
    img = [red.get(i).decode(<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids]
    <span class="hljs-keyword">return</span> ids, distance, img
<button class="copy-code-btn"></button></code></pre>
<p>Anhand der Modelle VGG16 und ResNet50 zeigt dieser Artikel die Verarbeitung mehrerer Modelle durch ONNX und die Kombination mehrerer Modelle mit Milvus für die Suche nach ähnlichen Vektoren, um ähnliche Bilder zu erhalten. Die beiden oben genannten Modelle basieren auf dem Keras-Framework, das schnell Feature-Vektoren extrahieren kann. Aus dem Notebook geht hervor, dass die Ergebnisse der Milvus-Suche nach Bildern im COCO-Datensatz auf der Grundlage dieser beiden Modelle zwar ähnlich sind, ihre euklidischen Abstände aber nicht gleich sind. Sie können auch versuchen, die Suchergebnisse der beiden Modelle mit anderen Datensätzen zu vergleichen.</p>
<p>Milvus ist eine hochleistungsfähige, hochverfügbare Vektordatenbank, die für die Verarbeitung von Merkmalsvektoren aus umfangreichen unstrukturierten Daten verwendet werden kann. Weitere Lösungen finden Sie im <a href="https://github.com/milvus-io/bootcamp">Milvus Bootcamp</a>.</p>
<h2 id="References" class="common-anchor-header">Referenzen<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ol>
<li>https://github.com/onnx/onnx</li>
<li>https://onnx.ai/</li>
<li>https://milvus.io/cn/</li>
<li>https://github.com/milvus-io/bootcamp</li>
</ol>
<h3 id="About-author" class="common-anchor-header">Über den Autor</h3><p>Shiyu Chen, Dateningenieurin bei Zilliz, hat einen Abschluss in Informatik von der Xidian Universität. Seit sie bei Zilliz arbeitet, hat sie Lösungen für Milvus in verschiedenen Bereichen erforscht, wie z.B. Audio- und Videoanalyse, Abrufen von Molekülformeln usw., was die Anwendungsszenarien der Gemeinschaft sehr bereichert hat. Derzeit erforscht sie weitere interessante Lösungen. In ihrer Freizeit treibt sie gerne Sport und liest.</p>
