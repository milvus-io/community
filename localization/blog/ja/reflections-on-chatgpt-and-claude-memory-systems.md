---
id: reflections-on-chatgpt-and-claude-memory-systems.md
title: ChatGPTとクロードの記憶システムについての考察：オンデマンド会話検索を可能にするために必要なこと
author: Min Yin
date: 2026-01-09T00:00:00.000Z
cover: assets.zilliz.com/Chat_GPT_VS_Claude_cover_555fdac36d.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'ChatGPT, Claude, memory systems, on-demand retrieval, conversational retrieval'
meta_title: |
  Milvus 2.6 Makes Claude-Style On-Demand Retrieval Practical
desc: >-
  ChatGPTとClaudeのメモリ設計の違い、オンデマンドの会話検索が難しい理由、Milvus
  2.6がプロダクションスケールでそれを可能にする方法について説明します。
origin: 'https://milvus.io/blog/reflections-on-chatgpt-and-claude-memory-systems.md'
---
<p>高品質のAIエージェントシステムにおいて、メモリ設計は見た目よりもはるかに複雑である。その核となるのは、3つの基本的な質問に答えることである：会話の履歴はどのように保存すべきか？いつ過去の文脈を取り出すべきか？そして、具体的に何を取り出すべきか？</p>
<p>これらの選択は、エージェントの応答待ち時間、リソースの使用量、そして最終的には能力の上限を直接形成します。</p>
<p>ChatGPTやClaudeのようなモデルは、使えば使うほど "メモリを意識している "ように感じます。嗜好を記憶し、長期的な目標に適応し、セッション間の連続性を維持する。その意味では、すでにミニAIエージェントとして機能している。しかしその表面下では、彼らの記憶システムはまったく異なるアーキテクチャを前提として構築されている。</p>
<p><a href="https://manthanguptaa.in/posts/chatgpt_memory/">ChatGPTと</a> <a href="https://manthanguptaa.in/posts/claude_memory/">クロードの記憶メカニズムに関する</a>最近のリバースエンジニアリング解析によって、明確なコントラストが明らかになった。<strong>ChatGPTは</strong>、軽量で予測可能な継続性を実現するために、事前に計算されたコンテキスト・インジェクションとレイヤード・キャッシングに依存している。対照的に<strong>Claudeは</strong>、メモリの深さと効率のバランスをとるために、動的なメモリ更新を伴うRAGスタイルのオンデマンド検索を採用している。</p>
<p>これら2つのアプローチは、単なる設計上の好みではなく、インフラストラクチャーの能力によって形作られている。<a href="https://milvus.io/docs/release_notes.md#v268"><strong>Milvus2.</strong></a>6は、オンデマンドの会話型メモリが必要とする、密と疎のハイブリッド検索、効率的なスカラーフィルタリング、階層型ストレージの組み合わせを導入し、選択的検索を実世界のシステムで展開するのに十分な速さと経済性を実現している。</p>
<p>この投稿では、ChatGPTとClaudeのメモリシステムが実際にどのように動作するのか、なぜアーキテクチャ的に分岐したのか、そしてMilvusのようなシステムの最近の進歩がどのようにオンデマンド会話検索を大規模で実用的なものにしているのかを説明します。</p>
<h2 id="ChatGPT’s-Memory-System" class="common-anchor-header">ChatGPTの記憶システム<button data-href="#ChatGPT’s-Memory-System" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>ChatGPTはベクターデータベースを照会したり、推論時に過去の会話を動的に取得する代わりに、コンテキストコンポーネントの固定セットを組み立て、すべてのプロンプトに直接注入することで「メモリ」を構築します。各コンポーネントは前もって用意され、プロンプト内の既知の位置を占めます。</p>
<p>この設計により、パーソナライゼーションと会話の連続性はそのままに、待ち時間、トークンの使用量、システムの動作をより予測しやすくしている。言い換えれば、メモリはモデルがその場で検索するものではなく、システムがパッケージ化し、応答を生成するたびにモデルに渡すものなのです。</p>
<p>高いレベルでは、ChatGPTプロンプトは以下のレイヤーで構成されています：</p>
<p>[0] システム指示</p>
<p>[1] 開発者の指示</p>
<p>[2] セッションメタデータ (エフェメラル)</p>
<p>[3] ユーザーメモリ (長期的な事実)</p>
<p>[4] Recent Conversations Summary (最近の会話の要約) (過去のチャット、タイトル、スニペット)</p>
<p>[5] 現在のセッションメッセージ（このチャット）</p>
<p>[6] 最新のメッセージ</p>
<p>これらのうち、[2]から[5]までのコンポーネントがシステムの有効な記憶を形成し、それぞれが明確な役割を果たします。</p>
<h3 id="Session-Metadata" class="common-anchor-header">セッションメタデータ</h3><p>セッションメタデータは、会話の最初に一度注入され、セッションが終了すると破棄される、短命で永続的でない情報を表す。その役割は、長期的に行動をパーソナライズすることよりも、モデルが現在の使用状況に適応するのを助けることである。</p>
<p>このレイヤーは、ユーザーの直近の環境と最近の使用パターンに関するシグナルを取得する。典型的なシグナルには以下が含まれる：</p>
<ul>
<li><p><strong>デバイス情報</strong>- 例えば、ユーザーがモバイルかデスクトップか。</p></li>
<li><p><strong>アカウント属性</strong>- サブスクリプション・ティア（例：ChatGPT Go）、アカウント年齢、全体的な利用頻度など。</p></li>
<li><p><strong>行動メトリクス</strong>- 過去1日、7日、30日間のアクティブ日数、平均会話時間、モデル使用分布（たとえば、リクエストの49%をGPT-5で処理）など。</p></li>
</ul>
<h3 id="User-Memory" class="common-anchor-header">ユーザー・メモリー</h3><p>ユーザーメモリは、会話全体のパーソナライゼーションを可能にする、永続的で編集可能なメモリ層です。ユーザーの名前、役割やキャリア目標、進行中のプロジェクト、過去の成果、学習嗜好など、比較的安定した情報が保存され、新しい会話ごとに注入されることで、時間の経過に伴う連続性が維持されます。</p>
<p>このメモリは2つの方法で更新できる：</p>
<ul>
<li><p><strong>明示的な更新は</strong>、ユーザーが "これを覚えておけ "とか "これを記憶から削除しろ "といった指示で直接記憶を管理するときに行われる。</p></li>
<li><p><strong>暗黙的な更新は</strong>、システムがOpenAIの保存基準を満たす情報（確認された名前や役職など）を識別し、ユーザーのデフォルトの同意とメモリ設定に従って自動的に保存するときに発生します。</p></li>
</ul>
<h3 id="Recent-Conversation-Summary" class="common-anchor-header">最近の会話の要約</h3><p>最近の会話の要約は、完全なチャット履歴を再生または取得することなく、連続性を維持する軽量で、クロスセッションコンテキストレイヤーです。従来のRAGベースのアプローチのように動的な検索に頼る代わりに、この要約は事前に計算され、すべての新しい会話に直接注入される。</p>
<p>このレイヤーはアシスタントの返信を除いて、ユーザーメッセージのみを要約する。それは意図的にサイズが制限されており、通常15エントリ程度であり、詳細な内容ではなく、最近の関心についての高レベルの信号のみを保持する。埋め込みや類似検索に依存しないため、待ち時間とトークンの消費を低く抑えることができる。</p>
<h3 id="Current-Session-Messages" class="common-anchor-header">現在のセッションメッセージ</h3><p>現在のセッションメッセージは、進行中の会話の完全なメッセージ履歴を含み、首尾一貫した、ターンバイターンの応答に必要な短期的なコンテキストを提供する。このレイヤーは、ユーザーの入力とアシスタントの返答の両方を含みますが、セッションがアクティブな間だけです。</p>
<p>このモデルは固定トークンの制限内で動作するため、この履歴は無制限に増やすことはできません。制限に達すると、システムは古いメッセージを削除し、新しいメッセージのためのスペースを確保します。この切り捨ては現在のセッションだけに影響し、長期的なユーザーの記憶と最近の会話の要約はそのまま残ります。</p>
<h2 id="Claude’s-Memory-System" class="common-anchor-header">クロードのメモリシステム<button data-href="#Claude’s-Memory-System" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Claudeはメモリ管理に異なるアプローチをとっている。ChatGPTのようにすべてのプロンプトに固定された大きなメモリコンポーネントを注入するのではなく、クロードは永続的なユーザメモリとオンデマンドツールと選択的な検索を組み合わせます。過去の文脈は、モデルが関連性があると判断したときにのみ取得され、システムは文脈の深さと計算コストをトレードオフすることができる。</p>
<p>クロードのプロンプトコンテキストは以下のような構造になっている：</p>
<p>[0] システムプロンプト（静的命令）</p>
<p>[1] ユーザーの記憶</p>
<p>[2] 会話履歴</p>
<p>[3] 現在のメッセージ</p>
<p>ClaudeとChatGPTの主な違いは、<strong>会話履歴の取得方法と</strong> <strong>ユーザーメモリの更新・維持</strong>方法にあります。</p>
<h3 id="User-Memories" class="common-anchor-header">ユーザメモリ</h3><p>Claudeでは、ユーザ・メモリはChatGPTのユーザ・メモリと同様の目的で長期的なコンテキスト・レイヤーを形成しますが、自動的なバックグラウンド主導の更新に重点を置いています。これらのメモリは（XMLスタイルのタグでラップされた）構造化された形式で保存され、最小限のユーザの介入で時間の経過とともに徐々に進化するように設計されています。</p>
<p>クロードは2つの更新経路をサポートしている：</p>
<ul>
<li><p><strong>暗黙的な更新</strong>- システムは定期的に会話の内容を分析し、バックグラウンドでメモリを更新する。これらのアップデートはリアルタイムでは適用されず、削除された会話に関連するメモリは継続的な最適化の一環として徐々に削除されます。</p></li>
<li><p><strong>明示的な更新</strong>- ユーザーは、専用の<code translate="no">memory_user_edits</code> ツールを介して実行される "これを記憶する "や "これを削除する "などのコマンドを通じて、メモリを直接管理することができます。</p></li>
</ul>
<p>ChatGPTと比較して、Claudeは長期記憶を洗練、更新、刈り込みする責任をシステム自身に負わせます。そのため、ユーザーが保存されているものを積極的に管理する必要性が低くなっている。</p>
<h3 id="Conversation-History" class="common-anchor-header">会話履歴</h3><p>会話履歴については、クロードはすべてのプロンプトに注入される固定要約に依存しません。その代わりに、3つの異なるメカニズムを使用して、モデルが必要と判断した場合にのみ、過去の文脈を検索する。これにより、無関係な履歴を持ち越さないようにし、トークンの使用量を抑えている。</p>
<table>
<thead>
<tr><th style="text-align:center"><strong>コンポーネント</strong></th><th style="text-align:center"><strong>目的</strong></th><th style="text-align:center"><strong>使用方法</strong></th></tr>
</thead>
<tbody>
<tr><td style="text-align:center"><strong>ローリングウィンドウ（現在の会話）</strong></td><td style="text-align:center">ChatGPTのセッションコンテキストと同様に、現在の会話の完全なメッセージ履歴を保存します。</td><td style="text-align:center">自動的に注入されます。トークンのリミットは~190K; リミットに達すると古いメッセージは削除される</td></tr>
<tr><td style="text-align:center"><code translate="no">conversation_search</code> <strong>ツール</strong></td><td style="text-align:center">トピックまたはキーワードで過去の会話を検索し、会話のリンク、タイトル、ユーザー/アシスタントメッセージの抜粋を返す</td><td style="text-align:center">モデルが過去の詳細が必要だと判断したときにトリガーされる。パラメータには<code translate="no">query</code> (検索語) と<code translate="no">max_results</code> (1-10) が含まれます。</td></tr>
<tr><td style="text-align:center"><code translate="no">recent_chats</code> <strong>ツール</strong></td><td style="text-align:center">指定された時間範囲内（例えば「過去3日間」）の最近の会話を検索します。<code translate="no">conversation_search</code></td><td style="text-align:center">最近の、時間にスコープされたコンテキストが関連する場合にトリガーされる。パラメータには、<code translate="no">n</code> （結果数）、<code translate="no">sort_order</code> 、および時間範囲が含まれる。</td></tr>
</tbody>
</table>
<p>これらのコンポーネントの中で、<code translate="no">conversation_search</code> は特に注目に値する。緩い言い回しや多言語のクエリであっても、関連する結果を表示することができ、単純なキーワードのマッチングに頼るのではなく、意味的なレベルで動作していることを示している。これは、埋め込みベースの検索、あるいは、まずクエリを正規形に翻訳または正規化し、次にキーワード検索またはハイブリッド検索を適用するハイブリッドなアプローチを含んでいる可能性が高い。</p>
<p>全体として、Claudeのオンデマンド検索アプローチには、いくつかの特筆すべき強みがある：</p>
<ul>
<li><p><strong>検索は自動的には行われない</strong>：ツールの呼び出しは、モデル自身の判断によってトリガーされる。例えば、ユーザが<em>「前回議論したプロジェクト</em>」を参照したとき、Claudeは関連するコンテキストを検索するために<code translate="no">conversation_search</code> 。</p></li>
<li><p><strong>必要なときに、よりリッチなコンテキストを</strong>：ChatGPTの要約はユーザーのメッセージのみをキャプチャするのに対し、取得した結果には<strong>アシスタントのレスポンスの抜粋を</strong>含めることができます。このため、Claudeはより深い、またはより正確な会話コンテキストを必要とするユースケースに適しています。</p></li>
<li><p><strong>デフォルトでより良い効率</strong>：履歴コンテキストは必要なとき以外は注入されないため、システムは大量の無関係な履歴を持ち越すことを避け、不必要なトークンの消費を抑えることができます。</p></li>
</ul>
<p>トレードオフも同様に明確です。インデックスの構築と維持、クエリーの実行、結果のランク付け、場合によっては再ランク付けが必要となる。また、エンド・ツー・エンドの待ち時間は、事前に計算され、常にコンテキストが注入される場合よりも予測しにくくなる。さらに、モデルは、いつ検索が必要かを判断することを学習しなければならない。その判断に失敗すると、関連するコンテキストがまったく取得されない可能性がある。</p>
<h2 id="The-Constraints-Behind-Claude-Style-On-Demand-Retrieval" class="common-anchor-header">クロード式オンデマンド検索の背後にある制約条件<button data-href="#The-Constraints-Behind-Claude-Style-On-Demand-Retrieval" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>オンデマンド検索モデルを採用することで、ベクトル・データベースはアーキテクチャの重要な一部となる。会話検索は、ストレージとクエリ実行の両方に異常に高い要求を課しており、システムは同時に4つの制約を満たさなければならない。</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/constraints_b6ed74e454.jpg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="1-Low-Latency-Tolerance" class="common-anchor-header">1.低レイテンシ耐性</h3><p>会話システムでは、P99のレイテンシは通常～20ミリ秒以下に抑える必要があります。それ以上の遅延はすぐにユーザーに気づかれてしまいます。ベクトル検索、メタデータのフィルタリング、結果のランキングはすべて慎重に最適化されなければなりません。ベクトル検索、メタデータフィルタリング、結果ランキングはすべて慎重に最適化されなければならない。どのポイントでもボトルネックがあると、会話体験全体が低下する可能性がある。</p>
<h3 id="2-Hybrid-Search-Requirement" class="common-anchor-header">2.ハイブリッド検索要件</h3><p>ユーザーのクエリはしばしば複数の次元にまたがる。<em>過去1週間のRAGに関するディスカッション」の</em>ようなリクエストは、意味的関連性と時間ベースのフィルタリングを組み合わせている。もしデータベースがベクトル検索しかサポートしていない場合、1,000の意味的に類似した結果を返すかもしれない。実用的であるためには、データベースはベクトルとスカラーを組み合わせたクエリーをネイティブにサポートする必要がある。</p>
<h3 id="3-Storage–Compute-Separation" class="common-anchor-header">3.ストレージと計算の分離</h3><p>会話履歴は、ホット-コールドの明確なアクセスパターンを示す。最近の会話は頻繁にクエリされるが、古い会話はほとんどクエリされない。もしすべてのベクターがメモリ上になければならないとすると、何千万もの会話を保存するために何百ギガバイトものRAMを消費することになる。実行可能であるためには、システムはストレージとコンピュートの分離をサポートし、ホットデータをメモリに、コールドデータをオブジェクトストレージに保持し、必要に応じてベクターをロードする必要がある。</p>
<h3 id="4-Diverse-Query-Patterns" class="common-anchor-header">4.多様なクエリーパターン</h3><p>会話検索は単一のアクセス・パターンに従うものではない。クエリの中には、純粋に意味的なもの（例えば、<em>"私たちが議論したパフォーマンス最適化"）も</em>あれば、純粋に時間的なもの（<em>"先週からのすべての会話"）も</em>あり、また、複数の制約を組み合わせたもの（<em>"過去3ヶ月間のFastAPIに言及したPython関連の議論"）も</em>多くあります。データベース問い合わせプランナは、単一サイズの総当たり検索に頼るのではなく、異なる問い合わせタイプに実行戦略を適応させなければなりません。</p>
<p>これら4つの課題を合わせて、会話型検索の中核となる制約を定義する。Claudeスタイルのオンデマンド検索を実現しようとするシステムは、協調してこれら全てに対処しなければならない。</p>
<h2 id="Why-Milvus-26-Works-Well-for-Conversational-Retrieval" class="common-anchor-header">Milvus 2.6が会話型検索に適している理由<button data-href="#Why-Milvus-26-Works-Well-for-Conversational-Retrieval" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p><a href="https://milvus.io/docs/release_notes.md#v268">Milvus2.</a>6の設計上の選択は、オンデマンド会話検索のコア要件と密接に一致している。以下は、主要な機能の内訳と、それらが実際の会話検索のニーズにどのように対応しているかを示している。</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/milvus_2_6_ce379ff42d.jpg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="Hybrid-Retrieval-with-Dense-and-Sparse-Vectors" class="common-anchor-header">密なベクトルと疎なベクトルのハイブリッド検索</h3><p>Milvus2.6では、密なベクトルと疎なベクトルを同じコレクション内に格納し、クエリ時にそれらの結果を自動的に融合することができます。密なベクトル（例えば、BGE-M3のようなモデルによって生成される768次元の埋め込み）は意味的類似性を捉え、一方、疎なベクトル（一般的にBM25によって生成される）は正確なキーワード信号を保持する。</p>
<p>Milvusは、<em>"先週のRAGに関する議論 "の</em>ようなクエリに対して、意味検索とキーワード検索を並行して実行し、リランキングによって結果をマージする。どちらかのアプローチを単独で使用する場合と比較して、このハイブリッド戦略は実際の会話シナリオにおいて著しく高いリコールを実現する。</p>
<h3 id="Storage–Compute-Separation-and-Query-Optimization" class="common-anchor-header">ストレージと計算の分離とクエリの最適化</h3><p>Milvus 2.6は2つの方法で階層型ストレージをサポートしている：</p>
<ul>
<li><p>メモリ上のホットデータ、オブジェクトストレージ上のコールドデータ</p></li>
<li><p>インデックスをメモリに、生のベクトルデータをオブジェクトストレージに</p></li>
</ul>
<p>この設計では、およそ2GBのメモリと8GBのオブジェクトストレージで100万件の会話エントリを格納することができる。適切なチューニングにより、ストレージとコンピートの分離を有効にしても、P99のレイテンシは20ミリ秒以下に抑えることができる。</p>
<h3 id="JSON-Shredding-and-Fast-Scalar-Filtering" class="common-anchor-header">JSONシュレッダーと高速スカラーフィルタリング</h3><p>Milvus 2.6はデフォルトでJSON Shreddingを有効にし、ネストしたJSONフィールドをカラム型ストレージにフラット化します。これにより、公式ベンチマークによると、スカラーフィルタリングのパフォーマンスが3～5倍向上します（実際の向上はクエリパターンによって異なります）。</p>
<p>会話形式の検索では、ユーザーID、セッションID、時間範囲などのメタデータによるフィルタリングが必要になることがよくあります。JSON Shreddingを使用すると、<em>「過去1週間のユーザーAからのすべての会話」の</em>ようなクエリは、完全なJSONブロブを繰り返し解析することなく、カラムナインデックス上で直接実行できます。</p>
<h3 id="Open-Source-Control-and-Operational-Flexibility" class="common-anchor-header">オープンソースの制御と運用の柔軟性</h3><p>オープンソースシステムであるMilvusは、クローズドでブラックボックス化されたソリューションにはない、アーキテクチャと運用のコントロールが可能です。チームはインデックスパラメータを調整し、データ階層化戦略を適用し、ワークロードに合わせて分散デプロイメントをカスタマイズすることができます。</p>
<p>この柔軟性が参入障壁を下げます。中小規模のチームは、特大のインフラ予算に頼ることなく、100万から数千万規模の会話検索システムを構築することができます。</p>
<h2 id="Why-ChatGPT-and-Claude-Took-Different-Paths" class="common-anchor-header">ChatGPTとクロードが異なる道を歩んだ理由<button data-href="#Why-ChatGPT-and-Claude-Took-Different-Paths" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>ChatGPTとClaudeの記憶システムの違いは、大まかに言えば、それぞれが忘却をどのように扱うかにあります。ChatGPTは積極的な忘却を好みます：メモリが一定の限界を超えると、古いコンテキストは削除されます。これは完全性をシンプルさと予測可能なシステム動作と引き換えにします。Claude は遅延忘却を好みます。理論的には、会話の履歴は、オンデマンドの検索システムに委任されたリコールで、無制限に成長することができます。</p>
<p>では、なぜ2つのシステムは異なる道を選んだのだろうか？上に述べた技術的制約を考慮すれば、その答えは明らかである。<strong>それぞれのアーキテクチャは、基盤となるインフラがそれをサポートできる場合にのみ実行可能</strong>だからである。</p>
<p>クロードのアプローチを2020年に試みても、おそらく実用的ではなかっただろう。当時、ベクターデータベースには数百ミリ秒のレイテンシーが発生することが多く、ハイブリッドクエリのサポートは不十分で、データの増大とともにリソースの使用量は法外にスケールしていった。このような状況下では、オンデマンド検索は過剰なエンジニアリングとして否定されていただろう。</p>
<p>2025年になると、状況は一変する。<strong>Milvus 2.6の</strong>ようなシステムによるインフラストラクチャーの進歩により、ストレージとコンピュートの分離、クエリーの最適化、密と疎のハイブリッド検索、JSONシュレッディングが本番環境で実行可能になった。これらの進歩により、レイテンシが短縮され、コストが抑制され、選択的検索が実用的な規模になりました。その結果、オンデマンドツールと検索ベースのメモリは、実現可能になっただけでなく、特にエージェントスタイルのシステムの基盤として、ますます魅力的になっている。</p>
<p>最終的には、アーキテクチャの選択は、インフラストラクチャが可能にするものに従うことになる。</p>
<h2 id="Conclusion" class="common-anchor-header">結論<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>実世界のシステムでは、メモリ設計は、事前に計算されたコンテキストかオンデマンド検索かの二者択一ではない。最も効果的なアーキテクチャは、両方のアプローチを組み合わせたハイブリッド型であることが一般的である。</p>
<p>一般的なパターンは、スライディングコンテキストウィンドウを通して最近の会話ターンを注入し、固定メモリとして安定したユーザー嗜好を保存し、ベクトル検索を介してオンデマンドで古い履歴を検索することである。製品が成熟するにつれて、このバランスは徐々に変化し、アーキテクチャを破壊的にリセットすることなく、主に事前計算されたコンテキストから、次第に検索主導に移行することができる。</p>
<p>事前計算アプローチから始める場合でも、移行を念頭に置いて設計することが重要である。メモリは、明確な識別子、タイムスタンプ、カテゴリー、ソース参照とともに保存されるべきである。検索が実行可能になれば、既存のメモリに対してエンベッディングを生成し、同じメタデータとともにベクターデータベースに追加することで、最小限の混乱で段階的に検索ロジックを導入することができる。</p>
<p>Milvusの最新機能についてのご質問やディープダイブをご希望ですか？私たちの<a href="https://discord.com/invite/8uyFbECzPX">Discordチャンネルに</a>参加するか、<a href="https://github.com/milvus-io/milvus">GitHubに</a>課題を提出してください。また、<a href="https://milvus.io/blog/join-milvus-office-hours-to-get-support-from-vectordb-experts.md">Milvusオフィスアワーを通して</a>、20分間の1対1のセッションを予約し、洞察、ガイダンス、質問への回答を得ることもできます。</p>
