---
id: how-to-choose-the-right-embedding-model-for-rag.md
title: Word2VecからLLM2Vecへ：RAGに適した埋め込みモデルの選び方
author: Rachel Liu
date: 2025-10-03T00:00:00.000Z
desc: このブログでは、RAGシステムに最適なエンベッディングを選択できるように、エンベッディングを実際に評価する方法を説明します。
cover: assets.zilliz.com/Chat_GPT_Image_Oct_3_2025_05_07_11_PM_36b1ba77eb.png
tag: Tutorials
recommend: false
publishToMedium: true
tags: 'Milvus, vector database, vector search, embedding models'
meta_keywords: 'Milvus, AI Agent, embedding model vector database'
meta_title: |
  How to Choose the Right Embedding Model for RAG
origin: 'https://milvus.io/blog/how-to-choose-the-right-embedding-model-for-rag.md'
---
<p>大規模な言語モデルは強力だが、幻覚というよく知られた弱点がある。<a href="https://zilliz.com/learn/Retrieval-Augmented-Generation">検索補強型生成（RAG</a>）は、この問題に取り組む最も効果的な方法の一つである。RAGは、モデルの記憶だけに頼るのではなく、外部ソースから関連知識を検索し、それをプロンプトに組み込むことで、回答が実際のデータに基づいたものになるようにする。</p>
<p>RAGシステムは通常、LLM本体、情報を保存・検索するための<a href="https://milvus.io/">Milvusの</a>ような<a href="https://zilliz.com/learn/what-is-vector-database">ベクトルデータベース</a>、埋め込みモデルの3つの主要コンポーネントで構成される。エンベッディング・モデルは、人間の言葉を機械が読み取り可能なベクトルに変換するものです。自然言語とデータベースの間の翻訳機と考えてください。この翻訳者の質が、検索されたコンテキストの関連性を決定する。それを正しく行えば、ユーザーは正確で役に立つ答えを見ることができる。それを誤ると、どんなに優れたインフラでも、ノイズやエラー、無駄な計算が発生する。</p>
<p>だからこそ、エンベッディング・モデルを理解することが重要なのです。Word2Vecのような初期の手法から、OpenAIのテキスト埋め込みファミリーのような最新のLLMベースのモデルまで、多くの中から選ぶことができます。それぞれにトレードオフと強みがある。このガイドでは、乱雑さを切り抜け、実際にエンベッディングを評価する方法を示しますので、あなたのRAGシステムに最適なものを選ぶことができます。</p>
<h2 id="What-Are-Embeddings-and-Why-Do-They-Matter" class="common-anchor-header">エンベッディングとは何か？<button data-href="#What-Are-Embeddings-and-Why-Do-They-Matter" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>最も単純なレベルでは、エンベッディングは人間の言語を機械が理解できる数値に変えます。すべての単語、文章、文書は高次元のベクトル空間にマッピングされ、ベクトル間の距離はそれらの間の関係を捉えます。似たような意味を持つテキストは一緒に集まり、関係のないコンテンツは離れていく傾向がある。これがセマンティック検索を可能にするものであり、キーワードのマッチングだけでなく、意味の検索を可能にする。</p>
<p>埋め込みモデルは、すべてが同じように機能するわけではない。一般的に3つのカテゴリーに分類され、それぞれに長所とトレードオフがある：</p>
<ul>
<li><p><a href="https://zilliz.com/learn/sparse-and-dense-embeddings"><strong>スパースベクトル</strong></a>（BM25のような）はキーワードの頻度と文書の長さに注目します。スパース・ベクトル（BM25のような）は、キーワードの頻度とドキュメントの長さに重点を置いており、明示的なマッチングには適しているが、同義語やコンテキストには不向きである。</p></li>
<li><p><a href="https://zilliz.com/learn/sparse-and-dense-embeddings"><strong>密なベクトル</strong></a>（BERTによって生成されるような）は、より深い意味性を捉える。BERTは、共有キーワードがなくても、"Apple releases new phone "が "iPhone product launch "に関連していることを見抜くことができる。欠点は、計算コストが高く、解釈可能性が低いことである。</p></li>
<li><p><strong>ハイブリッドモデル</strong>（BGE-M3など）は、この2つを組み合わせている。BGE-M3は、スパース、デンス、マルチベクトル表現を同時に生成することができ、キーワード検索の精度を保ちながら、意味的なニュアンスも捉えることができる。</p></li>
</ul>
<p>実際には、スピードと透明性を求めるならスパースベクトル、より豊かな意味を求めるならデンスベクトル、両方の長所を求めるならハイブリッドといった具合に、使用するケースによって選択することになる。</p>
<h2 id="Eight-Key-Factors-for-Evaluating-Embedding-Models" class="common-anchor-header">埋め込みモデルを評価するための8つのキーファクター<button data-href="#Eight-Key-Factors-for-Evaluating-Embedding-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="1-Context-Window" class="common-anchor-header"><strong>#1位 コンテキストウィンドウ</strong></h3><p><a href="https://zilliz.com/glossary/context-window"><strong>コンテキスト・ウィンドウは</strong></a>、モデルが一度に処理できるテキストの量を決定する。1トークンはおよそ0.75ワードであるため、この数値は埋め込みを作成する際にモデルが「見る」ことができる文章の長さを直接制限します。大きなウィンドウを使えば、モデルは長い文書の全体の意味を捉えることができます。小さなウィンドウでは、テキストを細かく切り刻まざるを得ず、意味のある文脈が失われる危険があります。</p>
<p>例えば、OpenAIの<a href="https://zilliz.com/ai-models/text-embedding-ada-002"><em>text-embedding-ada-002は</em></a>最大8,192トークンをサポートしており、これは抄録、方法、結論を含む研究論文全体をカバーするのに十分な量です。対照的に、（<em>m3e-baseの</em>ような）512トークンしかないモデルでは、頻繁に切り捨てる必要があり、その結果、重要な詳細が失われる可能性があります。</p>
<p>要点：法的文書や学術論文のような長い文書を使用する場合は、8K以上のトークン・ウィンドウを持つモデルを選択します。カスタマーサポートのチャットのような短いテキストの場合は、2Kトークンウィンドウで十分かもしれません。</p>
<h3 id="2-Tokenization-Unit" class="common-anchor-header"><strong>#2</strong>トークン化ユニット</h3><p>エンベッディングが生成される前に、テキストは<strong>トークンと</strong>呼ばれる小さな塊に分解されなければなりません。このトークン化がどのように行われるかは、モデルが希少語や専門用語、専門的なドメインをどの程度扱えるかに影響します。</p>
<ul>
<li><p><strong>サブワード・トークナイゼーション（BPE）：</strong>単語をより小さな部分に分割する（例："unhappiness" → "un" + "happiness"）。これはGPTやLLaMAのような最新のLLMのデフォルトであり、語彙のない単語に効果的である。</p></li>
<li><p><strong>WordPiece：</strong>BERTで使用されているBPEの改良版で、語彙のカバー率と効率のバランスをより良くするように設計されている。</p></li>
<li><p><strong>単語レベルのトークン化：</strong>単語全体によってのみ分割する。シンプルだが、希少用語や複雑な専門用語に苦戦するため、技術分野には不向き。</p></li>
</ul>
<p>医学や法律のような専門的な分野では、一般的にサブワードベースのモデルが最適である-<em>心筋梗塞や</em> <em>代位弁済の</em>ような用語を正しく処理できる。<strong>NV-Embedの</strong>ような最新のモデルの中には、潜在的な注目レイヤーのような機能拡張を追加することで、トークン化が複雑でドメイン固有の語彙をどのように捉えるかを強化したものもあります。</p>
<h3 id="3-Dimensionality" class="common-anchor-header">#3次元性</h3><p><a href="https://zilliz.com/glossary/dimensionality-reduction"><strong>ベクトル次元とは</strong></a>、埋め込みベクトルの長さのことで、モデルがどれだけの意味的な詳細を捉えることができるかを決定する。高い次元（例えば1,536以上）は、概念間の細かい区別を可能にするが、ストレージの増加、クエリの速度低下、および計算要件の増加という代償を伴う。より低い次元（768次元など）は、より高速で安価ですが、微妙な意味が失われる危険性があります。</p>
<p>重要なのはバランスである。ほとんどの汎用アプリケーションでは、768-1,536次元が効率と精度の適切なミックスになります。学術的、科学的な検索など、高い精度が要求される作業では、2,000次元を超えることは価値があります。一方、リソースに制約のあるシステム（エッジ展開など）では、検索品質が検証されていれば、512次元を効果的に使用することができる。軽量なレコメンデーションやパーソナライゼーション・システムでは、より小さな次元でも十分な場合がある。</p>
<h3 id="4-Vocabulary-Size" class="common-anchor-header">#語彙サイズ</h3><p>モデルの<strong>語彙サイズとは</strong>、そのトークナイザーが認識できるユニークなトークンの数を指します。これは、異なる言語やドメイン特有の用語を扱う能力に直接影響します。単語や文字が語彙にない場合、<code translate="no">[UNK]</code> としてマークされ、意味が失われる可能性があります。</p>
<p>要件はユースケースによって異なる。<a href="https://zilliz.com/ai-models/bge-m3"><em>BGE-M3の</em></a>ように、多言語シナリオでは一般に5万語以上の語彙が必要です。ドメインに特化したアプリケーションでは、専門用語をカバーすることが最も重要です。例えば、法律モデルは<em>「時効</em>」や「<em>善意取得</em>」といった用語をネイティブにサポートする必要がありますが、中国語モデルは何千もの文字や独特の句読点を考慮する必要があります。十分な語彙をカバーしなければ、埋め込み精度はすぐに落ちてしまいます。</p>
<h3 id="-5-Training-Data" class="common-anchor-header"># その5 トレーニングデータ</h3><p><strong>トレーニングデータは</strong>、エンベッディングモデルが "知っている "ことの境界を定義します。例えば、<em>text-embedding-ada-002の</em>ように、ウェブページ、書籍、Wikipediaをミックスしたような、広範で汎用的なデータで学習したモデルは、様々なドメインで良い結果を出す傾向があります。しかし、専門的な分野で精度が必要な場合は、ドメインで訓練されたモデルが勝つことが多い。例えば、<em>LegalBERTと</em> <em>BioBERTは</em>、汎化能力を多少失うものの、法律と生物医学のテキストでは一般的なモデルよりも優れている。</p>
<p>経験則では</p>
<ul>
<li><p><strong>一般的なシナリオ</strong>→ 幅広いデータセットで訓練されたモデルを使用するが、ターゲット言語をカバーしていることを確認する。例えば、中国語アプリケーションには、豊富な中国語コーパスで学習したモデルが必要。</p></li>
<li><p><strong>垂直的なドメイン</strong>→ 精度を高めるために、ドメイン固有のモデルを選択する。</p></li>
<li><p><strong>NV-Embedの</strong>ような新しいモデルは、一般的なデータとドメインに特化したデータの両方を用いて2段階で学習させることで、汎化<em>精度と</em>ドメイン精度の向上が期待できる。</p></li>
</ul>
<h3 id="-6-Cost" class="common-anchor-header"># その6 コスト</h3><p>コストとはAPIの価格設定だけではない。<strong>経済コストと</strong> <strong>計算コストの</strong>両方である。OpenAIのようなホスティング型APIモデルは利用ベースである。そのため、ラピッドプロトタイピングやパイロットプロジェクト、小規模から中規模のワークロードに最適だ。</p>
<p><em>BGEや</em> <em>Sentence-BERTの</em>ようなオープンソースのオプションは、使用は無料ですが、自己管理インフラ（通常はGPUまたはTPUクラスタ）が必要です。これらは、長期的な節約と柔軟性が一時的なセットアップとメンテナンスのコストを相殺する、大規模生産に向いている。</p>
<p>実用的な要点：<strong>APIモデルは迅速なイテレーションに理想的</strong>である一方、<strong>オープンソースモデルは</strong>、総所有コスト（TCO）を考慮すると、<strong>大規模な生産で勝利することが多い</strong>。正しい道を選ぶには、市場投入までのスピードが必要なのか、それとも長期的なコントロールが必要なのかによる。</p>
<h3 id="-7-MTEB-Score" class="common-anchor-header"># 7位 MTEBスコア</h3><p><a href="https://zilliz.com/glossary/massive-text-embedding-benchmark-(mteb)"><strong>Massive Text Embedding Benchmark (MTEB)</strong></a>は、エンベッディングモデルを比較するための最も広く使われている基準です。MTEBは、意味検索、分類、クラスタリングなど、さまざまなタスクのパフォーマンスを評価します。一般的にスコアが高いほど、そのモデルは異なる種類のタスクに対してより強い汎用性を持っていることを意味します。</p>
<p>とはいえ、MTEBは特効薬ではありません。全体的に高いスコアを持つモデルでも、特定のユースケースではまだ性能が劣るかもしれません。例えば、主に英語でトレーニングされたモデルは、MTEBベンチマークでは良い結果を出しても、専門的な医学テキストや英語以外のデータでは苦戦するかもしれません。安全なアプローチは、MTEBを出発点として使用し、コミットする前に<strong>独自のデータセットで</strong>検証することです。</p>
<h3 id="-8-Domain-Specificity" class="common-anchor-header"># その8 ドメイン固有性</h3><p>モデルの中には特定のシナリオのために作られたものがあり、一般的なモデルでは不十分な場合に威力を発揮します：</p>
<ul>
<li><p><strong>それは法律である：</strong> <em>法務：LegalBERT は</em>、<em>弁護</em>対<em>裁判管轄など</em>、細かい法律用語を区別できる。</p></li>
<li><p><strong>生物医学：</strong> <em>BioBERT は</em>、<em>mRNA</em>や<em>標的治療などの</em>技術的な語句を正確に処理します。</p></li>
<li><p><strong>多言語：</strong> <em>BGE-M3は</em>100以上の言語をサポートしており、英語、中国語、その他の言語の橋渡しを必要とするグローバルなアプリケーションに適しています。</p></li>
<li><p><strong>コード検索：</strong> <em>Qwen3-Embeddingは</em>、プログラミング関連のクエリに最適化された<em>MTEB-Codeで</em>トップレベルのスコア（81.0+）を達成しています。</p></li>
</ul>
<p>ユースケースがこれらのドメインのいずれかに該当する場合、ドメインに最適化されたモデルは検索精度を大幅に向上させることができます。しかし、より広範な用途の場合は、テストがそうでないことを示さない限り、汎用モデルにこだわってください。</p>
<h2 id="Additional-Perspectives-for-Evaluating-Embeddings" class="common-anchor-header">エンベッディングを評価するためのその他の視点<button data-href="#Additional-Perspectives-for-Evaluating-Embeddings" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>核となる8つの要素以外にも、より深い評価を望むのであれば、検討する価値のある視点がいくつかあります：</p>
<ul>
<li><p><strong>多言語アライメント</strong>：多言語モデルの場合、単に多くの言語をサポートするだけでは十分ではありません。真のテストは、ベクトル空間が整列しているかどうかである。言い換えれば、意味的に同じ単語、例えば英語の "cat "とスペイン語の "gato "が、ベクトル空間上で近接してマッピングされているかどうかということだ。強力なアライメントによって、一貫した言語横断検索が保証される。</p></li>
<li><p><strong>敵対的テスト</strong>：良い埋め込みモデルは、小さな入力の変化に対しても安定であるべきです。ほぼ同じ文（例えば "The cat sat on the mat "と "The cat sat on a mat"）を入力することで、結果のベクトルが適度にシフトするか、大きく変動するかをテストすることができます。揺らぎが大きいと、ロバスト性が弱いことが多い。</p></li>
<li><p><strong>局所的な意味的一貫性とは</strong>、意味的に類似した単語が局所的な近傍に緊密に集まっているかどうかをテストする現象を指します。例えば、"bank "のような単語がある場合、モデルは関連する単語（"riverbank "や "financial institution "など）を適切にグループ化し、関連しない単語は距離を置く。このような近傍に「侵入的」または無関係な単語が入り込む頻度を測定することは、モデルの品質を比較するのに役立つ。</p></li>
</ul>
<p>このような視点は日常業務では必ずしも必要ではありませんが、多言語、高精度、または敵対的な安定性が本当に重要な本番システムでエンベッディングをストレステストする際には役に立ちます。</p>
<h2 id="Common-Embedding-Models-A-Brief-History" class="common-anchor-header">一般的なエンベッディングモデル簡単な歴史<button data-href="#Common-Embedding-Models-A-Brief-History" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>エンベッディング・モデルの歴史は、機械がどのように言語をより深く理解することを学んできたかという歴史でもあります。静的な単語表現から、ニュアンスに富んだ文脈を捉えることのできる今日の大規模言語モデル（LLM）の埋め込みへと、各世代はその前の世代の限界を超えてきました。</p>
<h3 id="Word2Vec-The-Starting-Point-2013" class="common-anchor-header">Word2Vec：出発点（2013年）</h3><p><a href="https://zilliz.com/glossary/word2vec">GoogleのWord2Vecは</a>、埋め込みを広く実用化した最初のブレークスルーでした。これは言語学における<em>分布仮説</em>（同じような文脈に現れる単語は意味を共有することが多いという考え方）に基づいている。大量のテキストを分析することで、Word2Vecは単語をベクトル空間にマッピングした。例えば、"puma "と "leopard "は、生息地や狩猟の特徴を共有しているため、近くに集まった。</p>
<p>Word2Vecには2種類あった：</p>
<ul>
<li><p><strong>CBOW（Continuous Bag of Words）：</strong>周囲の文脈から欠落している単語を予測する。</p></li>
<li><p><strong>Skip-Gram：</strong>ターゲット単語から周囲の単語を予測する。</p></li>
</ul>
<p>このシンプルだが強力なアプローチにより、次のようなエレガントな類推が可能になった：</p>
<pre><code translate="no">king - man + woman = queen
<button class="copy-code-btn"></button></code></pre>
<p>当時としては、Word2Vecは革命的だった。しかし、2つの重大な制限があった。第一に、<strong>静的であっ</strong>た。各単語は1つのベクトルしか持たなかったので、"bank "は "money "の近くにあっても "river "の近くにあっても同じ意味であった。第二に、<strong>単語レベルでしか</strong>機能しないため、文や文書がその範囲から外れてしまう。</p>
<h3 id="BERT-The-Transformer-Revolution-2018" class="common-anchor-header">BERT：トランスフォーマー革命（2018年）</h3><p>Word2Vecが私たちに意味の最初の地図を与えたとすれば、<a href="https://zilliz.com/learn/what-is-bert"><strong>BERT（Bidirectional Encoder Representations from Transformers）は</strong></a>それをはるかに詳細に描き直した。2018年にGoogleによってリリースされたBERTは、エンベッディングにTransformerアーキテクチャを導入することで、<em>深い意味</em>理解の時代の幕開けを告げた。以前のLSTMとは異なり、Transformerはシーケンス内のすべての単語を同時に、両方向から調べることができ、はるかに豊かな文脈を可能にする。</p>
<p>BERTの魔法は、2つの巧妙な事前学習タスクから生まれた：</p>
<ul>
<li><p><strong>マスク言語モデリング（MLM）：</strong>文中の単語をランダムに隠し、モデルに予測させることで、文脈から意味を推測させる。</p></li>
<li><p><strong>Next Sentence Prediction（NSP）：</strong>2つの文が互いに続いているかどうかを判断するようモデルを訓練し、文全体の関係を学習させる。</p></li>
</ul>
<p>BERTの入力ベクトルは、トークン埋め込み（単語そのもの）、セグメント埋め込み（どの文に属するか）、位置埋め込み（シーケンスのどこに位置するか）の3つの要素を組み合わせている。これらを組み合わせることで、BERT は<strong>文と</strong> <strong>文書の</strong>両方のレベルで複雑な意味的関係を捉える能力を得た。この飛躍により、BERTは質問応答や意味検索のようなタスクにおいて最先端のものとなった。</p>
<p>もちろん、BERTは完璧ではなかった。初期のバージョンは<strong>512トークンのウィンドウに</strong>制限されていたため、長い文書は切り刻まれなければならず、時には意味を失うこともあった。また、密なベクトルは解釈性に欠け、2つのテキストが一致することはわかるが、その理由を説明できるとは限らなかった。<strong>RoBERTaの</strong>ような後の改良型は、強力なMLMトレーニングはそのままに、NSPタスクの利点がほとんどないことが研究で明らかになったため、NSPタスクを削除した。</p>
<h3 id="BGE-M3-Fusing-Sparse-and-Dense-2023" class="common-anchor-header">BGE-M3：スパースとデンスの融合（2023年）</h3><p>2023年までには、この分野は十分に成熟し、単一の埋め込み手法ですべてを達成することはできないと認識されるようになっていた。<a href="https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings">BGE-M3（BAAI</a>General Embedding-M3）は、検索タスクのために設計されたハイブリッドモデルである。密なベクトル、疎なベクトル、複数のベクトルを一度に生成し、それぞれの長所を組み合わせる。</p>
<ul>
<li><p><strong>密なベクトルは</strong>、同義語や言い換え（例えば、「iPhone発売」、≒「アップルが新型携帯を発売」）を処理し、深い意味を捉えます。</p></li>
<li><p><strong>スパース・ベクトルは</strong>、明示的な単語の重みを割り当てます。例えば、"iPhoneの新製品 "を "Apple Inc. "や "スマートフォン "と関連付けることができます。</p></li>
<li><p><strong>マルチベクターは</strong>、各トークンが独自のインタラクションスコアに貢献できるようにすることで、密な埋め込みをさらに洗練させます。</p></li>
</ul>
<p>BGE-M3の学習パイプラインは、この洗練された機能を反映している：</p>
<ol>
<li><p><em>RetroMAE</em>（マスクエンコーダ＋再構成デコーダ）による大量のラベルなしデータでの<strong>事前学習により</strong>、一般的な意味理解を構築。</p></li>
<li><p>100Mのテキストペアで対照学習を用いた<strong>一般的な微調整を</strong>行い、検索性能を研ぎ澄ます。</p></li>
<li><p>シナリオに特化した最適化のための命令チューニングと複雑なネガティブサンプリングによる<strong>タスク微調整</strong>。</p></li>
</ol>
<p>その結果は印象的である：BGE-M3は複数の粒度（単語レベルから文書レベルまで）を処理し、強力な多言語性能（特に中国語）を実現し、精度と効率性のバランスを同業他社よりも優れています。実際には、大規模検索において強力かつ実用的な埋め込みモデルを構築する上で、大きな前進となる。</p>
<h3 id="LLMs-as-Embedding-Models-2023–Present" class="common-anchor-header">埋め込みモデルとしてのLLM（2023年～現在）</h3><p>何年もの間、GPTのようなデコーダのみの大規模言語モデル（LLM）は埋め込みには適さないというのが通説でした。その因果的な注意（前のトークンにしか注目しない）は、深い意味理解を制限すると考えられていたからだ。しかし最近の研究では、その仮定が覆された。適切な調整を加えることで、LLMは専用モデルに匹敵し、時にはそれを凌駕する埋め込みデータを生成することができる。2つの顕著な例は、LLM2VecとNV-Embedである。</p>
<p><strong>LLM2Vecは</strong>、デコーダのみのLLMに3つの重要な変更を加えたものです：</p>
<ul>
<li><p><strong>双方向アテンション変換</strong>：各トークンが全シーケンスにアテンションできるように、因果マスクを置き換える。</p></li>
<li><p><strong>マスクされた次のトークン予測（MNTP）：</strong>双方向の理解を促す新しい学習目的。</p></li>
<li><p><strong>教師なし対照学習：</strong>SimCSEにインスパイアされ、意味的に類似した文をベクトル空間上で近づける。</p></li>
</ul>
<p>一方、<strong>NV-Embedは</strong>より合理的なアプローチをとる：</p>
<ul>
<li><p><strong>潜在的な注目層：</strong>訓練可能な「潜在的な配列」を追加し、シーケンスプーリングを改善する。</p></li>
<li><p><strong>直接双方向学習：</strong>単純に因果関係のあるマスクを取り除き、対比学習で微調整。</p></li>
<li><p><strong>ミーンプーリングの最適化：</strong>トークン間の加重平均を使用し、"ラストトークンバイアス "を回避。</p></li>
</ul>
<p>その結果、最新のLLMベースの埋め込みは、<strong>深い意味</strong>理解と<strong>スケーラビリティを兼ね備えて</strong>いる。<strong>非常に長いコンテキストウィンドウ（8K-32Kトークン）を</strong>扱うことができるため、研究、法律、企業検索などのドキュメントを多用するタスクに特に強い。また、同じLLMバックボーンを再利用しているため、より制約の多い環境でも高品質な埋め込みを実現できる場合がある。</p>
<h2 id="Conclusion-Turning-Theory-into-Practice" class="common-anchor-header">結論理論を実践へ<button data-href="#Conclusion-Turning-Theory-into-Practice" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>エンベッディング・モデルを選択するとき、理論だけではうまくいきません。本当のテストは、<em>あなたの</em>システムで、<em>あなたの</em>データに対して、どれだけうまく機能するかということです。いくつかの実践的なステップを踏むことで、紙の上ではよく見えるモデルでも、本番で実際に機能するモデルとの違いを生み出すことができます：</p>
<ul>
<li><p><strong>MTEBサブセットでスクリーニングする。</strong>ベンチマーク、特に検索タスクを使用して、候補の最初のショートリストを作成する。</p></li>
<li><p><strong>実際のビジネスデータでテストする。</strong>自社の文書から評価セットを作成し、実環境下でのリコール、精度、レイテンシを測定する。</p></li>
<li><p><strong>データベースの互換性をチェックする。</strong>疎なベクトルには逆インデックスのサポートが必要ですが、高次元の密なベクトルにはより多くのストレージと計算が必要です。ベクトル・データベースがあなたの選択に対応できることを確認してください。</p></li>
<li><p><strong>長い文書をスマートに扱う。</strong>効率化のためにスライディングウィンドウなどのセグメンテーション戦略を利用し、意味を保持するために大きなコンテキストウィンドウモデルと組み合わせます。</p></li>
</ul>
<p>Word2Vecのシンプルな静的ベクトルから、32Kのコンテキストを持つLLMを搭載したエンベッディングまで、機械が言語を理解する方法は大きく進歩している。しかし、すべての開発者が最終的に学ぶ教訓がここにある：<em>最もスコアの高い</em>モデルが、ユースケースに<em>最適な</em>モデルとは限らない。</p>
<p>結局のところ、ユーザーはMTEBのリーダーボードやベンチマークチャートなど気にしていません。精度、コスト、そしてシステムとの互換性のバランスが取れたモデルを選択することで、理論上だけでなく、実世界で真に機能するものを構築することができるのです。</p>
