{"codeList":["git clone https://github.com/topikachu/spring-ai-rag\ncd spring-ai-rag\n","# Verify Docker is running correctly\ndocker version\ndocker ps\n\n# Verify Java version\njava -version\n\n# Verify Ollama installation\nollama --version\n","# Pull required models for this project\nollama pull mistral          # Chat model\nollama pull nomic-embed-text # Embedding model\n\n# Verify models are available\nollama list\n","# Ollama Configuration (OpenAI-compatible API)\nspring.ai.openai.base-url=http://localhost:11434\nspring.ai.openai.chat.options.model=mistral\nspring.ai.openai.embedding.options.model=nomic-embed-text\nspring.ai.openai.embedding.options.dimensions=768\n\n# Vector Store Configuration - dimensions must match embedding model\nspring.ai.vectorstore.milvus.embedding-dimension=768\n","public Flux<Document> ingestionFlux() {\n  return documentReader.getDocuments()\n          .flatMap(document -> {\n            var processChunks = Mono.fromRunnable(() -> {\n              var chunks = textSplitter.apply(List.of(document));\n              vectorStore.write(chunks); // expensive operation\n            }).subscribeOn(Schedulers.boundedElastic());\n\n            return Flux.concat(\n                    Flux.just(document),\n                    processChunks.then(Mono.empty())\n            );\n          })\n          .doOnComplete(() -> log.info(\"RunIngestion() finished\"))\n          .doOnError(e -> log.error(\"Error during ingestion\", e));\n}\n","spring.ai.vectorstore.milvus.initialize-schema=true\nspring.ai.vectorstore.milvus.embedding-dimension=768\n","public ChatClient.ChatClientRequestSpec input(String userInput, String conversationId) {\n  return chatClient.prompt()\n          .advisors(\n                  messageChatMemoryAdvisor,\n                  retrievalAugmentationAdvisor\n          )\n          .advisors(spec -> spec.param(CONVERSATION_ID, conversationId))\n          .user(userInput);\n}\n","public Flux<String> stream(String userInput, String conversationId) {\n    return input(userInput, conversationId)\n            .stream().content();\n}\n","@PostMapping(path = \"/chat\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)\npublic Flux<String> chat(@RequestBody ChatRequest chatRequest, @RequestParam() String conversationId, Principal principal) {\n  var conversationKey = String.format(\"%s:%s\", principal.getName(), conversationId);\n  return chatService.stream(chatRequest.userInput, conversationKey)\n          .doOnError(exp -> log.error(\"Error in chat\", exp));\n}\n","@Override\nprotected void configure(HttpSecurity http) throws Exception {\n    http\n        .httpBasic()\n        .and()\n        .authorizeRequests(authz -> authz\n            .antMatchers(\"/api/v1/index\").hasRole(\"ADMIN\")\n            .anyRequest().authenticated()\n        );\n}\n","-javaagent:<path/to/opentelemetry-javaagent.jar> \\\n-Dotel.metrics.exporter=none \\\n-Dotel.logs.exporter=none\n","# HELP gen_ai_client_operation_seconds  \n# TYPE gen_ai_client_operation_seconds summary\ngen_ai_client_operation_seconds_count{...} 1\n","# HELP db_vector_client_operation_seconds\n# TYPE db_vector_client_operation_seconds summary\ndb_vector_client_operation_seconds_count{...} 1\n","management.endpoints.web.exposure.include=prometheus\n","export OPENAI_API_KEY=dummy\nexport SPRING_PROFILES_ACTIVE=ollama-openai\nollama pull mistral            # Pull chat model\nollama pull nomic-embed-text   # Pull embedding model\n\nmvn clean test package\ndocker compose up -d\njava -javaagent:target/otel/opentelemetry-javaagent.jar -Dotel.metrics.exporter=none -Dotel.logs.exporter=none  -Dinput.directory=$PWD/src/test/resources/corpus  -jar target/rag-0.0.1-SNAPSHOT.jar\n\ncurl --location 'localhost:8080/api/v1/index' \\\n--user \"admin:password\" \\\n--header 'Content-Type: application/json' \\\n--data '{}'\n\ncurl --location 'localhost:8080/api/v1/chat?conversationId=flat' \\\n--header 'Content-Type: application/json' \\\n--user \"user:password\" \\\n--data '{\n    \"userInput\": \"Does milvus support FLAT type index?\"\n}'\n\ncurl --location 'localhost:8080/api/v1/chat?conversationId=flat' \\\n--header 'Content-Type: application/json' \\\n--user \"user:password\" \\\n--data '{\n    \"userInput\": \"When shall I use this index type?\"\n}'\n\ncurl --location 'localhost:8080/api/v1/chat?conversationId=hnsw' \\\n--header 'Content-Type: application/json' \\\n--user \"user:password\" \\\n--data '{\n    \"userInput\": \"Does milvus support HNSW type index?\"\n}'\n\ncurl --location 'localhost:8080/api/v1/chat?conversationId=hnsw' \\\n--header 'Content-Type: application/json' \\\n--user \"user:password\" \\\n--data '{\n    \"userInput\": \"When shall I use this index type?\"\n}'\n\ncurl \"http://localhost:8080/actuator/prometheus\"\n"],"headingContent":"","anchorList":[{"label":"何を作るか","href":"What-Well-Build","type":2,"isActive":false},{"label":"使用する主要コンポーネント","href":"Key-Components-We’ll-Use","type":2,"isActive":false},{"label":"前提条件","href":"Prerequisites","type":2,"isActive":false},{"label":"環境セットアップ","href":"Environment-Setup","type":2,"isActive":false},{"label":"ドキュメントETL: 構造化されていないテキストの構造化","href":"Document-ETL-Structuring-Unstructured-Text","type":2,"isActive":false},{"label":"ベクトルストレージ：Milvusによるミリ秒規模のセマンティック検索","href":"Vector-Storage-Millisecond-Scale-Semantic-Search-with-Milvus","type":2,"isActive":false},{"label":"RAG対応チャットの構築メモリ統合によるコンテキストQ&A","href":"Building-a-RAG-Enabled-Chat-Contextual-QA-with-Memory-Integration","type":2,"isActive":false},{"label":"エンタープライズグレードのAPIセキュリティとシステム監視性","href":"Enterprise-Grade-API-Security-and-System-Observability","type":2,"isActive":false},{"label":"プロジェクトの実行エンドツーエンドの実行","href":"Running-the-Project-End-to-End-Execution","type":2,"isActive":false},{"label":"結論","href":"Conclusion","type":2,"isActive":false}]}