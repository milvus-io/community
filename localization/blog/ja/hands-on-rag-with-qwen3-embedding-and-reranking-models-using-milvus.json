{"codeList":["pip install --upgrade pymilvus openai requests tqdm sentence-transformers transformers\n","import os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-***********\"\n","! wget https://github.com/milvus-io/milvus-docs/releases/download/v2.4.6-preview/milvus_docs_2.4.x_en.zip\n! unzip -q milvus_docs_2.4.x_en.zip -d milvus_docs\n","from glob import glob\n\ntext_lines = []\n\nfor file_path in glob(\"milvus_docs/en/faq/*.md\", recursive=True):\n    with open(file_path, \"r\") as file:\n        file_text = file.read()\n\n    text_lines += file_text.split(\"# \")\n","from openai import OpenAI\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM\n\n# Initialize OpenAI client for LLM generation\nopenai_client = OpenAI()\n\n# Load Qwen3-Embedding-0.6B model for text embeddings\nembedding_model = SentenceTransformer(\"Qwen/Qwen3-Embedding-0.6B\")\n\n# Load Qwen3-Reranker-0.6B model for reranking\nreranker_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\", padding_side='left')\nreranker_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-Reranker-0.6B\").eval()\n\n# Reranker configuration\ntoken_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\ntoken_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")\nmax_reranker_length = 8192\n\nprefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\nsuffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\nprefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)\nsuffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)\n","def emb_text(text, is_query=False):\n    \"\"\"\n    Generate text embeddings using Qwen3-Embedding-0.6B model.\n    \n    Args:\n        text: Input text to embed\n        is_query: Whether this is a query (True) or document (False)\n    \n    Returns:\n        List of embedding values\n    \"\"\"\n    if is_query:\n        # For queries, use the \"query\" prompt for better retrieval performance\n        embeddings = embedding_model.encode([text], prompt_name=\"query\")\n    else:\n        # For documents, use default encoding\n        embeddings = embedding_model.encode([text])\n    \n    return embeddings[0].tolist()\n","test_embedding = emb_text(\"This is a test\")\nembedding_dim = len(test_embedding)\nprint(f\"Embedding dimension: {embedding_dim}\")\nprint(f\"First 10 values: {test_embedding[:10]}\")\n","Embedding dimension: 1024\nFirst 10 values: [-0.009923271834850311, -0.030248118564486504, -0.011494234204292297, ...]\n","def format_instruction(instruction, query, doc):\n    \"\"\"Format instruction for reranker input\"\"\"\n    if instruction is None:\n        instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n        instruction=instruction, query=query, doc=doc\n    )\n    return output\n\ndef process_inputs(pairs):\n    \"\"\"Process inputs for reranker\"\"\"\n    inputs = reranker_tokenizer(\n        pairs, padding=False, truncation='longest_first',\n        return_attention_mask=False, max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)\n    )\n    for i, ele in enumerate(inputs['input_ids']):\n        inputs['input_ids'][i] = prefix_tokens + ele + suffix_tokens\n    inputs = reranker_tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=max_reranker_length)\n    for key in inputs:\n        inputs[key] = inputs[key].to(reranker_model.device)\n    return inputs\n\n@torch.no_grad()\ndef compute_logits(inputs, **kwargs):\n    \"\"\"Compute relevance scores using reranker\"\"\"\n    batch_scores = reranker_model(**inputs).logits[:, -1, :]\n    true_vector = batch_scores[:, token_true_id]\n    false_vector = batch_scores[:, token_false_id]\n    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n    scores = batch_scores[:, 1].exp().tolist()\n    return scores\n\ndef rerank_documents(query, documents, task_instruction=None):\n    \"\"\"\n    Rerank documents based on query relevance using Qwen3-Reranker\n    \n    Args:\n        query: Search query\n        documents: List of documents to rerank\n        task_instruction: Task instruction for reranking\n    \n    Returns:\n        List of (document, score) tuples sorted by relevance score\n    \"\"\"\n    if task_instruction is None:\n        task_instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n    \n    # Format inputs for reranker\n    pairs = [format_instruction(task_instruction, query, doc) for doc in documents]\n    \n    # Process inputs and compute scores\n    inputs = process_inputs(pairs)\n    scores = compute_logits(inputs)\n    \n    # Combine documents with scores and sort by score (descending)\n    doc_scores = list(zip(documents, scores))\n    doc_scores.sort(key=lambda x: x[1], reverse=True)\n    \n    return doc_scores\n","from pymilvus import MilvusClient\n\nmilvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n\ncollection_name = \"my_rag_collection\"\n","# Remove existing collection if it exists\nif milvus_client.has_collection(collection_name):\n    milvus_client.drop_collection(collection_name)\n\n# Create new collection with our embedding dimensions\nmilvus_client.create_collection(\n    collection_name=collection_name,\n    dimension=embedding_dim,  # 1024 for Qwen3-Embedding-0.6B\n    metric_type=\"IP\",  # Inner product for similarity\n    consistency_level=\"Strong\",  # Ensure data consistency\n)\n","from tqdm import tqdm\n\ndata = []\n\nfor i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n\nmilvus_client.insert(collection_name=collection_name, data=data)\n","Creating embeddings: 100%|████████████| 72/72 [00:08<00:00, 8.68it/s]\nInserted 72 documents\n","question = \"How is data stored in milvus?\"\n\n# Perform initial dense retrieval to get top candidates\nsearch_res = milvus_client.search(\n    collection_name=collection_name,\n    data=[emb_text(question, is_query=True)],  # Use query prompt\n    limit=10,  # Get top 10 candidates for reranking\n    search_params={\"metric_type\": \"IP\", \"params\": {}},\n    output_fields=[\"text\"],  # Return the actual text content\n)\n\nprint(f\"Found {len(search_res[0])} initial candidates\")\n","# Extract candidate documents\ncandidate_docs = [res[\"entity\"][\"text\"] for res in search_res[0]]\n\n# Rerank using Qwen3-Reranker\nprint(\"Reranking documents...\")\nreranked_docs = rerank_documents(question, candidate_docs)\n\n# Select top 3 after reranking\ntop_reranked_docs = reranked_docs[:3]\nprint(f\"Selected top {len(top_reranked_docs)} documents after reranking\")\n","Reranked results (top 3):\n[\n    [\n        \" Where does Milvus store data?\\n\\nMilvus deals with two types of data, inserted data and metadata. \\n\\nInserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\\n\\nMetadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\\n\\n###\",\n        0.9997891783714294\n    ],\n    [\n        \"How does Milvus flush data?\\n\\nMilvus returns success when inserted data are loaded to the message queue. However, the data are not yet flushed to the disk. Then Milvus' data node writes the data in the message queue to persistent storage as incremental logs. If `flush()` is called, the data node is forced to write all data in the message queue to persistent storage immediately.\\n\\n###\",\n        0.9989748001098633\n    ],\n    [\n        \"Does the query perform in memory? What are incremental data and historical data?\\n\\nYes. When a query request comes, Milvus searches both incremental data and historical data by loading them into memory. Incremental data are in the growing segments, which are buffered in memory before they reach the threshold to be persisted in storage engine, while historical data are from the sealed segments that are stored in the object storage. Incremental data and historical data together constitute the whole dataset to search.\\n\\n###\",\n        0.9984032511711121\n    ]\n]\n\n================================================================================\nOriginal embedding-based results (top 3):\n[\n    [\n        \" Where does Milvus store data?\\n\\nMilvus deals with two types of data, inserted data and metadata. \\n\\nInserted data, including vector data, scalar data, and collection-specific schema, are stored in persistent storage as incremental log. Milvus supports multiple object storage backends, including [MinIO](https://min.io/), [AWS S3](https://aws.amazon.com/s3/?nc1=h_ls), [Google Cloud Storage](https://cloud.google.com/storage?hl=en#object-storage-for-companies-of-all-sizes) (GCS), [Azure Blob Storage](https://azure.microsoft.com/en-us/products/storage/blobs), [Alibaba Cloud OSS](https://www.alibabacloud.com/product/object-storage-service), and [Tencent Cloud Object Storage](https://www.tencentcloud.com/products/cos) (COS).\\n\\nMetadata are generated within Milvus. Each Milvus module has its own metadata that are stored in etcd.\\n\\n###\",\n        0.8306853175163269\n    ],\n    [\n        \"How does Milvus flush data?\\n\\nMilvus returns success when inserted data are loaded to the message queue. However, the data are not yet flushed to the disk. Then Milvus' data node writes the data in the message queue to persistent storage as incremental logs. If `flush()` is called, the data node is forced to write all data in the message queue to persistent storage immediately.\\n\\n###\",\n        0.7302717566490173\n    ],\n    [\n        \"How does Milvus handle vector data types and precision?\\n\\nMilvus supports Binary, Float32, Float16, and BFloat16 vector types.\\n\\n- Binary vectors: Store binary data as sequences of 0s and 1s, used in image processing and information retrieval.\\n- Float32 vectors: Default storage with a precision of about 7 decimal digits. Even Float64 values are stored with Float32 precision, leading to potential precision loss upon retrieval.\\n- Float16 and BFloat16 vectors: Offer reduced precision and memory usage. Float16 is suitable for applications with limited bandwidth and storage, while BFloat16 balances range and efficiency, commonly used in deep learning to reduce computational requirements without significantly impacting accuracy.\\n\\n###\",\n        0.7003671526908875\n    ]\n]\n","context = \"\\n\".join(\n    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n)\n","SYSTEM_PROMPT = \"\"\"\nHuman: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n\"\"\"\nUSER_PROMPT = f\"\"\"\nUse the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n<context>\n{context}\n</context>\n<question>\n{question}\n</question>\n\"\"\"\n","response = openai_client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": USER_PROMPT},\n    ],\n)\nprint(response.choices[0].message.content)\n","In Milvus, data is stored in two main forms: inserted data and metadata. \nInserted data, which includes vector data, scalar data, and collection-specific \nschema, is stored in persistent storage as incremental logs. Milvus supports \nmultiple object storage backends for this purpose, including MinIO, AWS S3, \nGoogle Cloud Storage, Azure Blob Storage, Alibaba Cloud OSS, and Tencent \nCloud Object Storage. Metadata for Milvus is generated by its various modules \nand stored in etcd.\n"],"headingContent":"","anchorList":[{"label":"何を作るか","href":"What-Were-Building","type":2,"isActive":false},{"label":"環境セットアップ","href":"Environment-Setup","type":2,"isActive":false},{"label":"データの準備","href":"Data-Preparation","type":2,"isActive":false},{"label":"モデルのセットアップ","href":"Model-Setup","type":2,"isActive":false},{"label":"埋め込み関数","href":"Embedding-Function","type":2,"isActive":false},{"label":"リランカーの実装","href":"Reranking-Implementation","type":2,"isActive":false},{"label":"Milvusベクトルデータベースのセットアップ","href":"Setting-Up-Milvus-Vector-Database","type":2,"isActive":false},{"label":"Milvusへのデータのロード","href":"Loading-Data-into-Milvus","type":2,"isActive":false},{"label":"リランキングテクノロジーによるRAGの強化","href":"Enhancing-RAG-with-Reranking-Technology","type":2,"isActive":false},{"label":"まとめ","href":"Wrapping-Up","type":2,"isActive":false}]}