---
id: smarter-retrieval-for-rag-late-chunking-with-jina-embeddings-v2-and-milvus.md
title: >-
  Smarter Retrieval for RAG : Late Chunking with Jina Embeddings v2 and Milvus
  (en anglais)
author: Wei Zang
date: 2025-10-11T00:00:00.000Z
desc: >-
  Am√©liorez la pr√©cision de RAG en utilisant Late Chunking et Milvus pour une
  int√©gration efficace et contextuelle des documents et une recherche
  vectorielle plus rapide et plus intelligente.
cover: assets.zilliz.com/Milvus_Meets_Late_Chunking_eaff956df1.png
tag: Tutorials
tags: 'Milvus, Vector Database, Open Source, Vector Embeddings'
recommend: false
meta_keywords: 'Late Chunking, RAG accuracy, vector database, Milvus, document embeddings'
canonicalUrl: >-
  https://milvus.io/blog/smarter-retrieval-for-rag-late-chunking-with-jina-embeddings-v2-and-milvus.md
---
<p>La construction d'un syst√®me RAG robuste commence g√©n√©ralement par le <a href="https://zilliz.com/learn/guide-to-chunking-strategies-for-rag#Chunking"><strong>d√©coupage des</strong></a> <strong>documents</strong> <a href="https://zilliz.com/learn/guide-to-chunking-strategies-for-rag#Chunking"><strong>, c'est-√†-dire la division de</strong></a>textes volumineux en morceaux g√©rables pour l'int√©gration et la r√©cup√©ration. Les strat√©gies les plus courantes sont les suivantes</p>
<ul>
<li><p>des<strong>morceaux de taille fixe</strong> (par exemple, tous les 512 tokens)</p></li>
<li><p><strong>Morceaux de taille variable</strong> (par exemple, √† la limite des paragraphes ou des phrases)</p></li>
<li><p><strong>Fen√™tres coulissantes</strong> (chevauchement des port√©es)</p></li>
<li><p>d√©coupage<strong>r√©cursif</strong> (d√©coupage hi√©rarchique)</p></li>
<li><p>le d√©coupage<strong>s√©mantique</strong> (regroupement par th√®me).</p></li>
</ul>
<p>Bien que ces m√©thodes aient leurs m√©rites, elles ne tiennent pas compte du contexte √† long terme. Pour relever ce d√©fi, Jina AI cr√©e une approche de d√©coupage tardive : incorporez d'abord le document entier, puis d√©coupez vos morceaux.</p>
<p>Dans cet article, nous allons explorer le fonctionnement du Late Chunking et d√©montrer comment sa combinaison avec <a href="https://milvus.io/">Milvus - une</a>base de donn√©es vectorielle open-source haute performance con√ßue pour la recherche de similarit√©s - peut am√©liorer consid√©rablement vos pipelines RAG. Que vous construisiez des bases de connaissances d'entreprise, un support client pilot√© par l'IA ou des applications de recherche avanc√©e, cette pr√©sentation vous montrera comment g√©rer les embeddings de mani√®re plus efficace √† grande √©chelle.</p>
<h2 id="What-Is-Late-Chunking" class="common-anchor-header">Qu'est-ce que le d√©coupage tardif ?<button data-href="#What-Is-Late-Chunking" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Les m√©thodes de regroupement traditionnelles peuvent rompre des connexions importantes lorsque des informations cl√©s s'√©tendent sur plusieurs morceaux, ce qui se traduit par des performances de recherche m√©diocres.</p>
<p>Prenons les notes de mise √† jour de Milvus 2.4.13, divis√©es en deux parties comme ci-dessous :</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Figure1_Chunking_Milvus2_4_13_Release_Note_fe7fbdb833.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Figure 1. D√©coupage des notes de mise √† jour de Milvus 2.4.13</em></p>
<p>Si vous demandez "Quelles sont les nouvelles fonctionnalit√©s de Milvus 2.4.13 ?", un mod√®le d'int√©gration standard risque de ne pas relier "Milvus 2.4.13" (dans le morceau 1) √† ses fonctionnalit√©s (dans le morceau 2). Le r√©sultat ? Des vecteurs plus faibles et une pr√©cision de recherche moindre.</p>
<p>Les solutions heuristiques, telles que les fen√™tres glissantes, les contextes qui se chevauchent et les balayages r√©p√©t√©s, offrent un soulagement partiel, mais aucune garantie.</p>
<p>Le<strong>d√©coupage traditionnel</strong> suit ce processus :</p>
<ol>
<li><p><strong>Pr√©-d√©coupage du</strong> texte (par phrases, paragraphes ou longueur maximale du jeton).</p></li>
<li><p><strong>Int√©grer</strong> chaque morceau s√©par√©ment.</p></li>
<li><p><strong>Agr√©gation de l'</strong> int√©gration des jetons (par exemple, via un regroupement moyen) en un seul vecteur de morceaux.</p></li>
</ol>
<p>Le<strong>d√©coupage tardif</strong> inverse le processus :</p>
<ol>
<li><p><strong>Int√©grer d'abord</strong>: Ex√©cution d'un transformateur de contexte long sur l'ensemble du document, g√©n√©rant des encastrements de jetons riches qui capturent le contexte global.</p></li>
<li><p>Le<strong>d√©coupage ult√©rieur</strong>: La mise en commun moyenne d'√©tendues contigu√´s de ces encastrements de tokens pour former les vecteurs de morceaux finaux.</p></li>
</ol>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Figure2_Naive_Chunkingvs_Late_Chunking_a94d30b6ea.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Figure 2. D√©coupage na√Øf et d√©coupage tardif (</em><a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/"><em>Source</em></a><em>)</em></p>
<p>En pr√©servant le contexte complet du document dans chaque bloc, le d√©coupage tardif permet d'obtenir les r√©sultats suivants</p>
<ul>
<li><p><strong>Une plus grande pr√©cision de recherche - chaque</strong>√©l√©ment est contextuel.</p></li>
<li><p><strong>Moins de morceaux - vous</strong>envoyez un texte plus cibl√© √† votre LLM, ce qui r√©duit les co√ªts et la latence.</p></li>
</ul>
<p>De nombreux mod√®les √† contexte long tels que jina-embeddings-v2-base-en peuvent traiter jusqu'√† 8 192 tokens, soit l'√©quivalent d'une lecture de 20 minutes (environ 5 000 mots), ce qui rend le d√©coupage tardif pratique pour la plupart des documents du monde r√©el.</p>
<p>Maintenant que nous avons compris le "quoi" et le "pourquoi" du d√©coupage tardif, passons au "comment". Dans la section suivante, nous vous guiderons dans la mise en ≈ìuvre pratique du pipeline de d√©coupage tardif, nous comparerons ses performances √† celles du d√©coupage traditionnel et nous validerons son impact dans le monde r√©el √† l'aide de Milvus. Ce parcours pratique fera le lien entre la th√©orie et la pratique, en montrant exactement comment int√©grer le d√©coupage tardif dans vos flux de travail RAG.</p>
<h2 id="Testing-Late-Chunking" class="common-anchor-header">Test du d√©coupage tardif<button data-href="#Testing-Late-Chunking" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Basic-Implementation" class="common-anchor-header">Mise en ≈ìuvre de base</h3><p>Vous trouverez ci-dessous les fonctions de base du d√©coupage tardif. Nous avons ajout√© des documents clairs pour vous guider √† chaque √©tape. La fonction <code translate="no">sentence_chunker</code> divise le document original en morceaux bas√©s sur des paragraphes, renvoyant √† la fois le contenu des morceaux et les informations d'annotation des morceaux <code translate="no">span_annotations</code> (c'est-√†-dire les indices de d√©but et de fin de chaque morceau).</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentence_chunker</span>(<span class="hljs-params">document, batch_size=<span class="hljs-number">10000</span></span>):
    nlp = spacy.blank(<span class="hljs-string">&quot;en&quot;</span>)
    nlp.add_pipe(<span class="hljs-string">&quot;sentencizer&quot;</span>, config={<span class="hljs-string">&quot;punct_chars&quot;</span>: <span class="hljs-literal">None</span>})
    doc = nlp(document)

    docs = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(document), batch_size):
        batch = document[i : i + batch_size]
        docs.append(nlp(batch))

    doc = Doc.from_docs(docs)

    span_annotations = []
    chunks = []
    <span class="hljs-keyword">for</span> i, sent <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(doc.sents):
        span_annotations.append((sent.start, sent.end))
        chunks.append(sent.text)

    <span class="hljs-keyword">return</span> chunks, span_annotations
<button class="copy-code-btn"></button></code></pre>
<p>La fonction <code translate="no">document_to_token_embeddings</code> utilise le mod√®le jinaai/jina-embeddings-v2-base-en et son tokenizer pour produire des embeddings pour l'ensemble du document.</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">document_to_token_embeddings</span>(<span class="hljs-params">model, tokenizer, document, batch_size=<span class="hljs-number">4096</span></span>):
    tokenized_document = tokenizer(document, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
    tokens = tokenized_document.tokens()

    outputs = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(tokens), batch_size):
        
        start = i
        end   = <span class="hljs-built_in">min</span>(i + batch_size, <span class="hljs-built_in">len</span>(tokens))

        batch_inputs = {k: v[:, start:end] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tokenized_document.items()}

        <span class="hljs-keyword">with</span> torch.no_grad():
            model_output = model(**batch_inputs)

        outputs.append(model_output.last_hidden_state)

    model_output = torch.cat(outputs, dim=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> model_output
<button class="copy-code-btn"></button></code></pre>
<p>La fonction <code translate="no">late_chunking</code> prend les ench√¢ssements de jetons du document et les informations d'annotation du bloc original <code translate="no">span_annotations</code>, puis produit les ench√¢ssements de blocs finaux.</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking</span>(<span class="hljs-params">token_embeddings, span_annotation, max_length=<span class="hljs-literal">None</span></span>):
    outputs = []
    <span class="hljs-keyword">for</span> embeddings, annotations <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(token_embeddings, span_annotation):
        <span class="hljs-keyword">if</span> (
            max_length <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        ):
            annotations = [
                (start, <span class="hljs-built_in">min</span>(end, max_length - <span class="hljs-number">1</span>))
                <span class="hljs-keyword">for</span> (start, end) <span class="hljs-keyword">in</span> annotations
                <span class="hljs-keyword">if</span> start &lt; (max_length - <span class="hljs-number">1</span>)
            ]
        pooled_embeddings = []
        <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> annotations:
            <span class="hljs-keyword">if</span> (end - start) &gt;= <span class="hljs-number">1</span>:
                pooled_embeddings.append(
                    embeddings[start:end].<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) / (end - start)
                )
                    
        pooled_embeddings = [
            embedding.detach().cpu().numpy() <span class="hljs-keyword">for</span> embedding <span class="hljs-keyword">in</span> pooled_embeddings
        ]
        outputs.append(pooled_embeddings)

    <span class="hljs-keyword">return</span> outputs
<button class="copy-code-btn"></button></code></pre>
<p>Par exemple, chunking avec jinaai/jina-embeddings-v2-base-en :</p>
<pre><code translate="no">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;jinaai/jina-embeddings-v2-base-en&#x27;</span>, trust_remote_code=<span class="hljs-literal">True</span>)
model     = AutoModel.from_pretrained(<span class="hljs-string">&#x27;jinaai/jina-embeddings-v2-base-en&#x27;</span>, trust_remote_code=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># First chunk the text as normal, to obtain the beginning and end points of the chunks.</span>
chunks, span_annotations = sentence_chunker(document)
<span class="hljs-comment"># Then embed the full document.</span>
token_embeddings = document_to_token_embeddings(model, tokenizer, document)
<span class="hljs-comment"># Then perform the late chunking</span>
chunk_embeddings = late_chunking(token_embeddings, [span_annotations])[<span class="hljs-number">0</span>]
<button class="copy-code-btn"></button></code></pre>
<p><em>Conseil :</em> l'int√©gration de votre pipeline dans des fonctions facilite l'int√©gration d'autres mod√®les de contexte long ou de strat√©gies de d√©coupage.</p>
<h3 id="Comparison-with-Traditional-Embedding-Methods" class="common-anchor-header">Comparaison avec les m√©thodes d'int√©gration traditionnelles</h3><p>Pour mieux d√©montrer les avantages du d√©coupage tardif, nous l'avons √©galement compar√© aux approches d'int√©gration traditionnelles, √† l'aide d'un ensemble de documents et de requ√™tes types.</p>
<p>Reprenons l'exemple de la note de mise √† jour Milvus 2.4.13 :</p>
<pre><code translate="no"><span class="hljs-title class_">Milvus</span> <span class="hljs-number">2.4</span><span class="hljs-number">.13</span> introduces dynamic replica load, allowing users to adjust the number <span class="hljs-keyword">of</span> collection replicas without needing to release and reload the collection. <span class="hljs-title class_">This</span> version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery. <span class="hljs-title class_">Additionally</span>, significant improvements have been made to <span class="hljs-variable constant_">MMAP</span> resource usage and <span class="hljs-keyword">import</span> performance, enhancing overall system efficiency. <span class="hljs-title class_">We</span> highly recommend upgrading to <span class="hljs-variable language_">this</span> release <span class="hljs-keyword">for</span> better performance and stability.
<button class="copy-code-btn"></button></code></pre>
<p>Nous mesurons la <a href="https://zilliz.com/blog/similarity-metrics-for-vector-search#Cosine-Similarity">similarit√© en cosinus</a> entre l'int√©gration de la requ√™te ("milvus 2.4.13") et chaque morceau :</p>
<pre><code translate="no">cos_sim = <span class="hljs-keyword">lambda</span> x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))

milvus_embedding = model.encode(<span class="hljs-string">&#x27;milvus 2.4.13&#x27;</span>)

<span class="hljs-keyword">for</span> chunk, late_chunking_embedding, traditional_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(chunks, chunk_embeddings, embeddings_traditional_chunking):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;similarity_late_chunking(&quot;milvus 2.4.13&quot;, &quot;<span class="hljs-subst">{chunk}</span>&quot;)&#x27;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;late_chunking: &#x27;</span>, cos_sim(milvus_embedding, late_chunking_embedding))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;similarity_traditional(&quot;milvus 2.4.13&quot;, &quot;<span class="hljs-subst">{chunk}</span>&quot;)&#x27;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;traditional_chunking: &#x27;</span>, cos_sim(milvus_embedding, traditional_embeddings))
<button class="copy-code-btn"></button></code></pre>
<p>Le d√©coupage tardif a toujours √©t√© plus performant que le d√©coupage traditionnel, produisant des similitudes en cosinus plus √©lev√©es pour chaque morceau. Cela confirme que l'int√©gration du document complet en premier lieu pr√©serve le contexte global de mani√®re plus efficace.</p>
<pre><code translate="no"><span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Milvus 2.4.13 introduces dynamic replica load, allowing users to adjust the number of collection replicas without needing to release and reload the collection.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.8785206</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Milvus 2.4.13 introduces dynamic replica load, allowing users to adjust the number of collection replicas without needing to release and reload the collection.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.8354263</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;This version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.84828955</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;This version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.7222632</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Additionally, significant improvements have been made to MMAP resource usage and import performance, enhancing overall system efficiency.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.84942204</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Additionally, significant improvements have been made to MMAP resource usage and import performance, enhancing overall system efficiency.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.6907381</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;We highly recommend upgrading to this release for better performance and stability.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.85431844</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;We highly recommend upgrading to this release for better performance and stability.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.71859795</span>
<button class="copy-code-btn"></button></code></pre>
<p>Nous pouvons constater que l'int√©gration du paragraphe complet en premier permet de s'assurer que chaque morceau contient le contexte "<code translate="no">Milvus 2.4.13</code>", ce qui am√©liore les scores de similarit√© et la qualit√© de la recherche.</p>
<h3 id="Testing-Late-Chunking-in-Milvus" class="common-anchor-header"><strong>Test du d√©coupage tardif dans Milvus</strong></h3><p>Une fois que les encastrements de morceaux sont g√©n√©r√©s, nous pouvons les stocker dans Milvus et effectuer des requ√™tes. Le code suivant ins√®re les vecteurs de morceaux dans la collection.</p>
<h4 id="Importing-Embeddings-into-Milvus" class="common-anchor-header"><strong>Importation d'incorporations dans Milvus</strong></h4><pre><code translate="no">batch_data=[]
<span class="hljs-keyword">for</span> i in <span class="hljs-keyword">range</span>(<span class="hljs-built_in">len</span>(chunks)):
    data = {
            <span class="hljs-string">&quot;content&quot;</span>: chunks[i],
            <span class="hljs-string">&quot;embedding&quot;</span>: chunk_embeddings[i].tolist(),
        }

    batch_data.<span class="hljs-built_in">append</span>(data)

res = client.insert(
    collection_name=collection,
    data=batch_data,
)
<button class="copy-code-btn"></button></code></pre>
<h4 id="Querying-and-Validation" class="common-anchor-header">Interrogation et validation</h4><p>Pour valider la pr√©cision des requ√™tes de Milvus, nous comparons ses r√©sultats de recherche aux scores de similarit√© cosinus calcul√©s manuellement par force brute. Si les deux m√©thodes renvoient des r√©sultats top-k coh√©rents, nous pouvons √™tre s√ªrs que la pr√©cision de recherche de Milvus est fiable.</p>
<p>Nous comparons la recherche native de Milvus √† une analyse de similarit√© en cosinus par force brute :</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking_query_by_milvus</span>(<span class="hljs-params">query, top_k = <span class="hljs-number">3</span></span>):
    query_vector = model(**tokenizer(query, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)).last_hidden_state.mean(<span class="hljs-number">1</span>).detach().cpu().numpy().flatten()

    res = client.search(
                collection_name=collection,
                data=[query_vector.tolist()],
                limit=top_k,
                output_fields=[<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>],
            )

    <span class="hljs-keyword">return</span> [item.get(<span class="hljs-string">&quot;entity&quot;</span>).get(<span class="hljs-string">&quot;content&quot;</span>) <span class="hljs-keyword">for</span> items <span class="hljs-keyword">in</span> res <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items]

<span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking_query_by_cosine_sim</span>(<span class="hljs-params">query, k = <span class="hljs-number">3</span></span>):
    cos_sim = <span class="hljs-keyword">lambda</span> x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))
    query_vector = model(**tokenizer(query, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)).last_hidden_state.mean(<span class="hljs-number">1</span>).detach().cpu().numpy().flatten()

    results = np.empty(<span class="hljs-built_in">len</span>(chunk_embeddings))
    <span class="hljs-keyword">for</span> i, (chunk, embedding) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(chunks, chunk_embeddings)):
        results[i] = cos_sim(query_vector, embedding)

    results_order = results.argsort()[::-<span class="hljs-number">1</span>]
    <span class="hljs-keyword">return</span> np.array(chunks)[results_order].tolist()[:k]
<button class="copy-code-btn"></button></code></pre>
<p>Cela confirme que Milvus renvoie les m√™mes top-k chunks qu'une analyse cosinusim manuelle.</p>
<pre><code translate="no">&gt; late_chunking_query_by_milvus(<span class="hljs-string">&quot;What are new features in milvus 2.4.13&quot;</span>, 3)

[<span class="hljs-string">&#x27;\n\n### Features\n\n- Dynamic replica adjustment for loaded collections ([#36417](https://github.com/milvus-io/milvus/pull/36417))\n- Sparse vector MMAP in growing segment types ([#36565](https://github.com/milvus-io/milvus/pull/36565))...
</span><button class="copy-code-btn"></button></code></pre>
<pre><code translate="no">&gt; late_chunking_query_by_cosine_sim(<span class="hljs-string">&quot;What are new features in milvus 2.4.13&quot;</span>, 3)

[<span class="hljs-string">&#x27;\n\n### Features\n\n- Dynamic replica adjustment for loaded collections ([#36417](https://github.com/milvus-io/milvus/pull/36417))\n- Sparse vector MMAP in growing segment types (#36565)...
</span><button class="copy-code-btn"></button></code></pre>
<p>Les deux m√©thodes renvoient donc les m√™mes top-3 chunks, ce qui confirme la pr√©cision de Milvus.</p>
<h2 id="Conclusion" class="common-anchor-header">Conclusion<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Dans cet article, nous avons plong√© dans les m√©canismes et les avantages du d√©coupage tardif. Nous avons commenc√© par identifier les lacunes des approches de d√©coupage traditionnelles, en particulier lors du traitement de longs documents pour lesquels la pr√©servation du contexte est cruciale. Nous avons introduit le concept de d√©coupage tardif, qui consiste √† int√©grer l'ensemble du document avant de le d√©couper en morceaux significatifs, et nous avons montr√© comment cela permet de pr√©server le contexte global, ce qui am√©liore la similarit√© s√©mantique et la pr√©cision de l'extraction.</p>
<p>Nous avons ensuite proc√©d√© √† une mise en ≈ìuvre pratique en utilisant le mod√®le jina-embeddings-v2-base-en de Jina AI et √©valu√© ses performances par rapport aux m√©thodes traditionnelles. Enfin, nous avons d√©montr√© comment int√©grer les chunk embeddings dans Milvus pour une recherche vectorielle √©volutive et pr√©cise.</p>
<p>Le d√©coupage tardif offre une approche de l'incorporation <strong>fond√©e sur le contexte</strong>, parfaite pour les documents longs et complexes o√π le contexte est le plus important. En incorporant l'int√©gralit√© du texte en amont et en proc√©dant √† un d√©coupage ult√©rieur, vous b√©n√©ficiez des avantages suivants :</p>
<ul>
<li><p><strong>üîç Une meilleure pr√©cision de recherche</strong></p></li>
<li><p>‚ö° <strong>Des invites LLM all√©g√©es et cibl√©es</strong></p></li>
<li><p>üõ†Ô∏è <strong>Int√©gration simple</strong> avec n'importe quel mod√®le de contexte long</p></li>
</ul>
