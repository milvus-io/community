---
id: >-
  langchain-and-milvus-build-production-ready-agents-with-real-long-term-memory.md
title: >-
  LangChain 1.0 et Milvus : comment cr√©er des agents pr√™ts pour la production
  avec une v√©ritable m√©moire √† long terme
author: Min Yin
date: 2025-12-19T00:00:00.000Z
cover: assets.zilliz.com/langchain1_0_cover_8c4bc608af.png
tag: Tutorials
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'Milvus, LangChain 1.0, AI Agent, vector database, LangGraph'
meta_title: >
  LangChain 1.0 and Milvus: Build Production-Ready AI Agents with Long-Term
  Memory
desc: >-
  D√©couvrez comment LangChain 1.0 simplifie l'architecture des agents et comment
  Milvus ajoute une m√©moire √† long terme pour des applications d'IA √©volutives
  et pr√™tes √† la production.
origin: >-
  https://milvus.io/blog/langchain-and-milvus-build-production-ready-agents-with-real-long-term-memory.md
---
<p>LangChain est un cadre populaire √† code source ouvert pour le d√©veloppement d'applications aliment√©es par de grands mod√®les de langage (LLM). Il fournit une bo√Æte √† outils modulaire pour construire des agents de raisonnement et d'utilisation d'outils, connecter des mod√®les √† des donn√©es externes et g√©rer des flux d'interaction.</p>
<p>Avec la sortie de <strong>LangChain 1.0</strong>, le cadre fait un pas en avant vers une architecture plus conviviale pour la production. La nouvelle version remplace la conception ant√©rieure bas√©e sur la cha√Æne par une boucle ReAct normalis√©e (Raisonner ‚Üí Appel d'outil ‚Üí Observer ‚Üí D√©cider) et introduit un intergiciel pour g√©rer l'ex√©cution, le contr√¥le et la s√©curit√©.</p>
<p>Cependant, le raisonnement seul n'est pas suffisant. Les agents doivent √©galement pouvoir stocker, rappeler et r√©utiliser des informations. C'est l√† que <a href="https://milvus.io/"><strong>Milvus</strong></a>, une base de donn√©es vectorielle open-source, peut jouer un r√¥le essentiel. Milvus fournit une couche de m√©moire √©volutive et performante qui permet aux agents de stocker, de rechercher et d'extraire des informations de mani√®re efficace par le biais de la similarit√© s√©mantique.</p>
<p>Dans ce billet, nous verrons comment LangChain 1.0 met √† jour l'architecture des agents et comment l'int√©gration de Milvus permet aux agents d'aller au-del√† du raisonnement - en cr√©ant une m√©moire persistante et intelligente pour des cas d'utilisation r√©els.</p>
<h2 id="Why-the-Chain-based-Design-Falls-Short" class="common-anchor-header">Pourquoi la conception bas√©e sur la cha√Æne n'est pas √† la hauteur<button data-href="#Why-the-Chain-based-Design-Falls-Short" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>√Ä ses d√©buts (version 0.x), l'architecture de LangChain √©tait centr√©e sur les cha√Ænes. Chaque cha√Æne d√©finissait une s√©quence fixe et √©tait livr√©e avec des mod√®les pr√©construits qui rendaient l'orchestration LLM simple et rapide. Cette conception √©tait id√©ale pour construire rapidement des prototypes. Mais au fur et √† mesure que l'√©cosyst√®me LLM √©voluait et que les cas d'utilisation du monde r√©el devenaient plus complexes, des fissures dans cette architecture ont commenc√© √† appara√Ætre.</p>
<p><strong>1. Manque de flexibilit√©</strong></p>
<p>Les premi√®res versions de LangChain fournissaient des pipelines modulaires tels que SimpleSequentialChain ou LLMChain, chacun suivant un flux fixe et lin√©aire - cr√©ation d'un message ‚Üí appel de mod√®le ‚Üí traitement de sortie. Cette conception a bien fonctionn√© pour les t√¢ches simples et pr√©visibles et a permis de cr√©er rapidement des prototypes.</p>
<p>Toutefois, √† mesure que les applications devenaient plus dynamiques, ces mod√®les rigides commen√ßaient √† sembler restrictifs. Lorsque la logique m√©tier ne s'inscrit plus dans une s√©quence pr√©d√©finie, deux options peu satisfaisantes s'offrent √† vous : forcer votre logique √† se conformer au cadre ou le contourner enti√®rement en appelant directement l'API LLM.</p>
<p><strong>2. Absence de contr√¥le au niveau de la production</strong></p>
<p>Ce qui fonctionnait bien dans les d√©monstrations se cassait souvent en production. Les cha√Ænes n'incluaient pas les garanties n√©cessaires pour les applications √† grande √©chelle, persistantes ou sensibles. Les probl√®mes les plus fr√©quents sont les suivants</p>
<ul>
<li><p><strong>D√©bordement de contexte :</strong> Les longues conversations pouvaient d√©passer les limites de jetons, provoquant des pannes ou une troncature silencieuse.</p></li>
<li><p><strong>Fuites de donn√©es sensibles :</strong> Des informations personnelles identifiables (comme des courriels ou des identifiants) peuvent √™tre envoy√©es par inadvertance √† des mod√®les tiers.</p></li>
<li><p><strong>Op√©rations non supervis√©es :</strong> Les agents peuvent supprimer des donn√©es ou envoyer des courriels sans l'approbation d'un humain.</p></li>
</ul>
<p><strong>3. Manque de compatibilit√© entre les mod√®les</strong></p>
<p>Chaque fournisseur de LLM - OpenAI, Anthropic et de nombreux mod√®les chinois - met en ≈ìuvre ses propres protocoles de raisonnement et d'appel d'outils. Chaque fois que vous changiez de fournisseur, vous deviez r√©√©crire la couche d'int√©gration : mod√®les d'invite, adaptateurs et analyseurs de r√©ponse. Ce travail r√©p√©titif ralentissait le d√©veloppement et rendait l'exp√©rimentation p√©nible.</p>
<h2 id="LangChain-10-All-in-ReAct-Agent" class="common-anchor-header">LangChain 1.0 : Un agent ReAct tout-en-un<button data-href="#LangChain-10-All-in-ReAct-Agent" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Lorsque l'√©quipe LangChain a analys√© des centaines d'impl√©mentations d'agents de niveau production, une constatation s'est impos√©e : presque tous les agents performants convergeaient naturellement vers le <strong>mod√®le ReAct ("Reasoning + Acting")</strong>.</p>
<p>Qu'il s'agisse d'un syst√®me multi-agents ou d'un agent unique effectuant un raisonnement approfondi, la m√™me boucle de contr√¥le appara√Æt : alternance entre de br√®ves √©tapes de raisonnement et des appels d'outils cibl√©s, puis int√©gration des observations r√©sultantes dans les d√©cisions ult√©rieures jusqu'√† ce que l'agent soit en mesure de fournir une r√©ponse finale.</p>
<p>Pour s'appuyer sur cette structure √©prouv√©e, LangChain 1.0 place la boucle ReAct au c≈ìur de son architecture, ce qui en fait la structure par d√©faut pour construire des agents fiables, interpr√©tables et pr√™ts pour la production.</p>
<p>Pour prendre en charge tous les types d'agents, des plus simples aux plus complexes, LangChain 1.0 adopte une conception en couches qui allie facilit√© d'utilisation et contr√¥le pr√©cis :</p>
<ul>
<li><p><strong>Sc√©narios standard :</strong> Commencez par la fonction create_agent() - une boucle ReAct propre et standardis√©e qui g√®re le raisonnement et les appels d'outils d√®s le d√©part.</p></li>
<li><p><strong>Sc√©narios √©tendus :</strong> Ajoutez un intergiciel pour obtenir un contr√¥le plus fin. Les intergiciels vous permettent d'inspecter ou de modifier ce qui se passe √† l'int√©rieur de l'agent - par exemple, en ajoutant la d√©tection des IPI, des points de contr√¥le d'approbation humaine, des tentatives automatiques ou des crochets de surveillance.</p></li>
<li><p><strong>Sc√©narios complexes :</strong> Pour les flux de travail avec √©tat ou l'orchestration multi-agents, utilisez LangGraph, un moteur d'ex√©cution bas√© sur un graphe qui permet un contr√¥le pr√©cis du flux logique, des d√©pendances et des √©tats d'ex√©cution.</p></li>
</ul>
<p>D√©cortiquons maintenant les trois composants cl√©s qui rendent le d√©veloppement d'agents plus simple, plus s√ªr et plus coh√©rent d'un mod√®le √† l'autre.</p>
<h3 id="1-The-createagent-A-Simpler-Way-to-Build-Agents" class="common-anchor-header">1. La fonction create_agent() : Une mani√®re plus simple de construire des agents</h3><p>L'une des principales avanc√©es de LangChain 1.0 est la r√©duction de la complexit√© de la construction d'agents √† une seule fonction - create_agent(). Vous n'avez plus besoin de g√©rer manuellement la gestion de l'√©tat, la gestion des erreurs ou les sorties en continu. Ces fonctionnalit√©s de niveau production sont d√©sormais g√©r√©es automatiquement par le runtime LangGraph sous-jacent.</p>
<p>Avec seulement trois param√®tres, vous pouvez lancer un agent enti√®rement fonctionnel :</p>
<ul>
<li><p><strong>mod√®le</strong> - soit un identifiant de mod√®le (cha√Æne de caract√®res), soit un objet de mod√®le instanci√©.</p></li>
<li><p><strong>tools</strong> - une liste de fonctions qui donnent √† l'agent ses capacit√©s.</p></li>
<li><p><strong>system_prompt</strong> - l'instruction qui d√©finit le r√¥le, le ton et le comportement de l'agent.</p></li>
</ul>
<p>Sous le capot, create_agent() s'ex√©cute sur la boucle standard d'un agent - appelant un mod√®le, le laissant choisir les outils √† ex√©cuter, et terminant une fois que plus aucun outil n'est n√©cessaire :</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/langchain_chain_1_1192c31ce3.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Il h√©rite √©galement des capacit√©s int√©gr√©es de LangGraph pour la persistance de l'√©tat, la r√©cup√©ration des interruptions et le streaming. Les t√¢ches qui n√©cessitaient auparavant des centaines de lignes de code d'orchestration sont d√©sormais g√©r√©es par une API unique et d√©clarative.</p>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.<span class="hljs-property">agents</span> <span class="hljs-keyword">import</span> create_agent
agent = <span class="hljs-title function_">create_agent</span>(
    model=<span class="hljs-string">&quot;openai:gpt-4o&quot;</span>,
    tools=[get_weather, query_database],
    system_prompt=<span class="hljs-string">&quot;You are a customer service assistant who helps users check the weather and order information.&quot;</span>
)
result = agent.<span class="hljs-title function_">invoke</span>({
    <span class="hljs-string">&quot;messages&quot;</span>: [{<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What‚Äôs the weather like in Shanghai today?&quot;</span>}]
})
<button class="copy-code-btn"></button></code></pre>
<h3 id="2-The-Middleware-A-Composable-Layer-for-Production-Ready-Control" class="common-anchor-header">2. L'intergiciel : Une couche composable pour un contr√¥le pr√™t √† la production</h3><p>L'intergiciel est la passerelle essentielle qui permet √† LangChain de passer du prototype √† la production. Il expose des crochets √† des points strat√©giques de la boucle d'ex√©cution de l'agent, ce qui vous permet d'ajouter une logique personnalis√©e sans r√©√©crire le processus central de ReAct.</p>
<p>La boucle principale d'un agent suit un processus de d√©cision en trois √©tapes - Mod√®le ‚Üí Outil ‚Üí Terminaison :</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/langchain_1_0_chain_902054bde2.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>LangChain 1.0 fournit quelques <a href="https://docs.langchain.com/oss/python/langchain/middleware#built-in-middleware">middlewares pr√©construits</a> pour des mod√®les courants. En voici quatre exemples.</p>
<ul>
<li><strong>D√©tection des informations confidentielles : Toute application traitant des donn√©es sensibles de l'utilisateur</strong></li>
</ul>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> PIIMiddleware


agent = create_agent(
    model=<span class="hljs-string">&quot;gpt-4o&quot;</span>,
    tools=[],  <span class="hljs-comment"># Add tools as needed</span>
    middleware=[
        <span class="hljs-comment"># Redact emails in user input</span>
        PIIMiddleware(<span class="hljs-string">&quot;email&quot;</span>, strategy=<span class="hljs-string">&quot;redact&quot;</span>, apply_to_input=<span class="hljs-literal">True</span>),
        <span class="hljs-comment"># Mask credit cards (show last 4 digits)</span>
        PIIMiddleware(<span class="hljs-string">&quot;credit_card&quot;</span>, strategy=<span class="hljs-string">&quot;mask&quot;</span>, apply_to_input=<span class="hljs-literal">True</span>),
        <span class="hljs-comment"># Custom PII type with regex</span>
        PIIMiddleware(
            <span class="hljs-string">&quot;api_key&quot;</span>,
            detector=<span class="hljs-string">r&quot;sk-[a-zA-Z0-9]{32}&quot;</span>,
            strategy=<span class="hljs-string">&quot;block&quot;</span>,  <span class="hljs-comment"># Raise error if detected</span>
        ),
    ],
)
<button class="copy-code-btn"></button></code></pre>
<ul>
<li><strong>R√©sum√© : R√©sumer automatiquement l'historique des conversations lorsque la limite des jetons est atteinte.</strong></li>
</ul>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> SummarizationMiddleware


agent = create_agent(
    model=<span class="hljs-string">&quot;gpt-4o&quot;</span>,
    tools=[weather_tool, calculator_tool],
    middleware=[
        SummarizationMiddleware(
            model=<span class="hljs-string">&quot;gpt-4o-mini&quot;</span>,  <span class="hljs-comment">#Summarize using a cheaper model  </span>
            max_tokens_before_summary=<span class="hljs-number">4000</span>,  <span class="hljs-comment"># Trigger summarization at 4000 tokens</span>
            messages_to_keep=<span class="hljs-number">20</span>,  <span class="hljs-comment"># Keep last 20 messages after summary</span>
        ),
    ],
)
<button class="copy-code-btn"></button></code></pre>
<ul>
<li><strong>R√©essai d'outil : R√©essayer automatiquement les appels d'outils qui n'ont pas abouti avec un d√©lai exponentiel configurable.</strong></li>
</ul>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> ToolRetryMiddleware
agent = create_agent(
    model=<span class="hljs-string">&quot;gpt-4o&quot;</span>,
    tools=[search_tool, database_tool],
    middleware=[
        ToolRetryMiddleware(
            max_retries=<span class="hljs-number">3</span>,  <span class="hljs-comment"># Retry up to 3 times</span>
            backoff_factor=<span class="hljs-number">2.0</span>,  <span class="hljs-comment"># Exponential backoff multiplier</span>
            initial_delay=<span class="hljs-number">1.0</span>,  <span class="hljs-comment"># Start with 1 second delay</span>
            max_delay=<span class="hljs-number">60.0</span>,  <span class="hljs-comment"># Cap delays at 60 seconds</span>
            jitter=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># Add random jitter to avoid thundering herd (¬±25%)</span>

        ),
    ],
)
<button class="copy-code-btn"></button></code></pre>
<ul>
<li><strong>Middleware personnalis√©</strong></li>
</ul>
<p>En plus des options officielles d'intergiciels pr√©construits, vous pouvez √©galement cr√©er des intergiciels personnalis√©s en utilisant des d√©corateurs ou des classes.</p>
<p>Par exemple, l'extrait ci-dessous montre comment enregistrer les appels de mod√®le avant l'ex√©cution :</p>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> before_model
<span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> AgentState
<span class="hljs-keyword">from</span> langgraph.runtime <span class="hljs-keyword">import</span> Runtime
<span class="hljs-meta">@before_model</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">log_before_model</span>(<span class="hljs-params">state: AgentState, runtime: Runtime</span>) -&gt; <span class="hljs-built_in">dict</span> | <span class="hljs-literal">None</span>:
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;About to call model with <span class="hljs-subst">{<span class="hljs-built_in">len</span>(state[<span class="hljs-string">&#x27;messages&#x27;</span>])}</span> messages&quot;</span>)
    <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span>  <span class="hljs-comment"># Returning None means the normal flow continues</span>
agent = create_agent(
    model=<span class="hljs-string">&quot;openai:gpt-4o&quot;</span>,
    tools=[...],
    middleware=[log_before_model],
)
<button class="copy-code-btn"></button></code></pre>
<h3 id="3-Structured-Output-A-Standardized-Way-to-Handle-Data" class="common-anchor-header">3. Sortie structur√©e : Une mani√®re standardis√©e de traiter les donn√©es</h3><p>Dans le d√©veloppement traditionnel d'agents, la sortie structur√©e a toujours √©t√© difficile √† g√©rer. Chaque fournisseur de mod√®le le g√®re diff√©remment - par exemple, OpenAI offre une API de sortie structur√©e native, tandis que d'autres ne prennent en charge les r√©ponses structur√©es qu'indirectement par le biais d'appels d'outils. Cela signifiait souvent qu'il fallait √©crire des adaptateurs personnalis√©s pour chaque fournisseur, ce qui ajoutait du travail suppl√©mentaire et rendait la maintenance plus p√©nible qu'elle ne devrait l'√™tre.</p>
<p>Dans LangChain 1.0, la sortie structur√©e est g√©r√©e directement par le param√®tre response_format de la fonction create_agent().  Vous n'avez besoin de d√©finir votre sch√©ma de donn√©es qu'une seule fois. LangChain choisit automatiquement la meilleure strat√©gie d'application en fonction du mod√®le que vous utilisez - aucune configuration suppl√©mentaire ou code sp√©cifique au fournisseur n'est n√©cessaire.</p>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> pydantic <span class="hljs-keyword">import</span> BaseModel, Field
<span class="hljs-keyword">class</span> <span class="hljs-title class_">WeatherReport</span>(<span class="hljs-title class_ inherited__">BaseModel</span>):
    location: <span class="hljs-built_in">str</span> = Field(description=<span class="hljs-string">&quot;City name&quot;</span>)
    temperature: <span class="hljs-built_in">float</span> = Field(description=<span class="hljs-string">&quot;Temperature (¬∞C)&quot;</span>)
    condition: <span class="hljs-built_in">str</span> = Field(description=<span class="hljs-string">&quot;Weather condition&quot;</span>)
agent = create_agent(
    model=<span class="hljs-string">&quot;openai:gpt-4o&quot;</span>,
    tools=[get_weather],
    response_format=WeatherReport  <span class="hljs-comment"># Use the Pydantic model as the response schema</span>
)
result = agent.invoke({<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;What‚Äôs the weather like in Shanghai today??&quot;</span>})
weather_data = result[<span class="hljs-string">&#x27;structured_response&#x27;</span>]  <span class="hljs-comment"># Retrieve the structured response</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{weather_data.location}</span>: <span class="hljs-subst">{weather_data.temperature}</span>¬∞C, <span class="hljs-subst">{weather_data.condition}</span>&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<p>LangChain prend en charge deux strat√©gies pour les sorties structur√©es :</p>
<p><strong>1. Strat√©gie du fournisseur :</strong> Certains fournisseurs de mod√®les prennent nativement en charge la sortie structur√©e via leurs API (par exemple OpenAI et Grok). Lorsqu'un tel support est disponible, LangChain utilise directement l'application du sch√©ma int√©gr√© du fournisseur. Cette approche offre le plus haut niveau de fiabilit√© et de coh√©rence, puisque le mod√®le lui-m√™me garantit le format de sortie.</p>
<p><strong>2. Strat√©gie d'appel d'outils :</strong> Pour les mod√®les qui ne supportent pas les sorties structur√©es natives, LangChain utilise le Tool Calling pour obtenir le m√™me r√©sultat.</p>
<p>Vous n'avez pas √† vous soucier de la strat√©gie utilis√©e : le cadre d√©tecte les capacit√©s du mod√®le et s'adapte automatiquement. Cette abstraction vous permet de passer librement d'un fournisseur de mod√®le √† l'autre sans modifier votre logique m√©tier.</p>
<h2 id="How-Milvus-Enhances-Agent-Memory" class="common-anchor-header">Comment Milvus am√©liore la m√©moire des agents<button data-href="#How-Milvus-Enhances-Agent-Memory" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Pour les agents de niveau production, le v√©ritable goulot d'√©tranglement des performances n'est souvent pas le moteur de raisonnement, mais le syst√®me de m√©moire. Dans LangChain 1.0, les bases de donn√©es vectorielles jouent le r√¥le de m√©moire externe de l'agent et permettent une m√©morisation √† long terme gr√¢ce √† la r√©cup√©ration s√©mantique.</p>
<p><a href="https://milvus.io/">Milvus</a> est l'une des bases de donn√©es vectorielles open-source les plus matures disponibles aujourd'hui, con√ßue pour la recherche vectorielle √† grande √©chelle dans les applications d'intelligence artificielle. Elle s'int√®gre nativement √† LangChain, de sorte que vous n'avez pas √† g√©rer manuellement la vectorisation, la gestion de l'index ou la recherche de similarit√©. Le package langchain_milvus int√®gre Milvus comme une interface VectorStore standard, vous permettant de le connecter √† vos agents avec seulement quelques lignes de code.</p>
<p>Ce faisant, Milvus rel√®ve trois d√©fis majeurs dans la construction de syst√®mes de m√©moire d'agents √©volutifs et fiables :</p>
<h4 id="1-Fast-Retrieval-from-Massive-Knowledge-Bases" class="common-anchor-header"><strong>1. Extraction rapide de bases de connaissances massives</strong></h4><p>Lorsqu'un agent doit traiter des milliers de documents, de conversations pass√©es ou de manuels de produits, une simple recherche par mot-cl√© ne suffit pas. Milvus utilise la recherche par similarit√© vectorielle pour trouver des informations s√©mantiquement pertinentes en quelques millisecondes, m√™me si la requ√™te est formul√©e diff√©remment. Cela permet √† votre agent de rappeler des connaissances bas√©es sur le sens, et pas seulement sur des correspondances de texte exactes.</p>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> langchain_milvus <span class="hljs-keyword">import</span> Milvus
<span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> OpenAIEmbeddings
<span class="hljs-comment"># Initialize the vector database as a knowledge base</span>
vectorstore = Milvus(
    embedding=OpenAIEmbeddings(),  
    collection_name=<span class="hljs-string">&quot;company_knowledge&quot;</span>,
    connection_args={<span class="hljs-string">&quot;uri&quot;</span>: <span class="hljs-string">&quot;http://localhost:19530&quot;</span>}  <span class="hljs-comment">#</span>
)
<span class="hljs-comment"># Convert the retriever into a Tool for the Agent</span>
agent = create_agent(
    model=<span class="hljs-string">&quot;openai:gpt-4o&quot;</span>,
    tools=[vectorstore.as_retriever().as_tool(
        name=<span class="hljs-string">&quot;knowledge_search&quot;</span>,
        description=<span class="hljs-string">&quot;Search the company knowledge base to answer professional questions&quot;</span>
    )],
    system_prompt=<span class="hljs-string">&quot;You can retrieve information from the knowledge base to answer questions.&quot;</span>
)
<button class="copy-code-btn"></button></code></pre>
<h4 id="2-Persistent-Long-Term-Memory" class="common-anchor-header"><strong>2. M√©moire √† long terme persistante</strong></h4><p>Le SummarizationMiddleware de LangChain peut condenser l'historique des conversations lorsqu'il devient trop long, mais qu'advient-il de tous les d√©tails qui sont r√©sum√©s ? Milvus les conserve. Chaque conversation, appel d'outil et √©tape de raisonnement peut √™tre vectoris√© et stock√© pour une r√©f√©rence √† long terme. En cas de besoin, l'agent peut rapidement retrouver les souvenirs pertinents par le biais d'une recherche s√©mantique, ce qui permet une v√©ritable continuit√© d'une session √† l'autre.</p>
<pre><code translate="no"><span class="hljs-keyword">from</span> langchain_milvus <span class="hljs-keyword">import</span> Milvus
<span class="hljs-keyword">from</span> langchain.agents <span class="hljs-keyword">import</span> create_agent
<span class="hljs-keyword">from</span> langchain.agents.middleware <span class="hljs-keyword">import</span> SummarizationMiddleware
<span class="hljs-keyword">from</span> langgraph.checkpoint.memory <span class="hljs-keyword">import</span> InMemorySaver
<span class="hljs-comment"># Long-term memory storage(Milvus)</span>
long_term_memory = Milvus.from_documents(
    documents=[],  <span class="hljs-comment"># Initially empty; dynamically updated at runtime</span>
    embedding=OpenAIEmbeddings(),
    connection_args={<span class="hljs-string">&quot;uri&quot;</span>: <span class="hljs-string">&quot;./agent_memory.db&quot;</span>}
)
<span class="hljs-comment"># Short-term memory management(LangGraph Checkpointer + Summarization)</span>
agent = create_agent(
    model=<span class="hljs-string">&quot;openai:gpt-4o&quot;</span>,
    tools=[long_term_memory.as_retriever().as_tool(
        name=<span class="hljs-string">&quot;recall_memory&quot;</span>,
        description=<span class="hljs-string">&quot;Retrieve the agent‚Äôs historical memories and past experiences&quot;</span>
    )],
    checkpointer=InMemorySaver(),  <span class="hljs-comment"># Short-term memory</span>
    middleware=[
        SummarizationMiddleware(
            model=<span class="hljs-string">&quot;openai:gpt-4o-mini&quot;</span>,
            max_tokens_before_summary=<span class="hljs-number">4000</span>  <span class="hljs-comment"># When the threshold is exceeded, summarize and store it in Milvus</span>
        )
    ]
)
<button class="copy-code-btn"></button></code></pre>
<h4 id="3-Unified-Management-of-Multimodal-Content" class="common-anchor-header"><strong>3. Gestion unifi√©e du contenu multimodal</strong></h4><p>Les agents modernes traitent plus que du texte - ils interagissent avec des images, du son et de la vid√©o. Milvus prend en charge le stockage multi-vecteur et le sch√©ma dynamique, ce qui vous permet de g√©rer les incorporations de plusieurs modalit√©s dans un syst√®me unique. Il s'agit d'une base de m√©moire unifi√©e pour les agents multimodaux, qui permet une r√©cup√©ration coh√©rente de diff√©rents types de donn√©es.</p>
<pre><code translate="no"><span class="hljs-comment"># Filter retrievals by source (e.g., search only medical reports)</span>
vectorstore.similarity_search(
    query=<span class="hljs-string">&quot;What is the patient&#x27;s blood pressure reading?&quot;</span>,
    k=<span class="hljs-number">3</span>,
    expr=<span class="hljs-string">&quot;source == &#x27;medical_reports&#x27; AND modality == &#x27;text&#x27;&quot;</span>  <span class="hljs-comment"># Milvus scalar filtering</span>
)
<button class="copy-code-btn"></button></code></pre>
<h2 id="LangChain-vs-LangGraph-How-to-Choose-the-One-That-Fits-for-Your-Agents" class="common-anchor-header">LangChain vs. LangGraph : Comment choisir celui qui convient √† vos agents<button data-href="#LangChain-vs-LangGraph-How-to-Choose-the-One-That-Fits-for-Your-Agents" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>La mise √† jour vers LangChain 1.0 est une √©tape essentielle vers la cr√©ation d'agents de niveau production - mais cela ne signifie pas que c'est toujours le seul ou le meilleur choix pour tous les cas d'utilisation. Le choix du bon framework d√©termine la rapidit√© avec laquelle vous pouvez combiner ces capacit√©s dans un syst√®me fonctionnel et maintenable.</p>
<p>En fait, LangChain 1.0 et LangGraph 1.0 peuvent √™tre consid√©r√©s comme des √©l√©ments d'une m√™me pile, con√ßus pour fonctionner ensemble plut√¥t que pour se remplacer l'un l'autre : LangChain vous aide √† construire rapidement des agents standard, tandis que LangGraph vous permet de contr√¥ler finement des flux de travail complexes. En d'autres termes, LangChain vous aide √† aller vite, tandis que LangGraph vous aide √† aller en profondeur.</p>
<p>Voici une comparaison rapide de leurs diff√©rences de positionnement technique :</p>
<table>
<thead>
<tr><th><strong>Dimension</strong></th><th><strong>LangChain 1.0</strong></th><th><strong>LangChain 1.0</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Niveau d'abstraction</strong></td><td>Abstraction de haut niveau, con√ßue pour les sc√©narios d'agents standards</td><td>Cadre d'orchestration de bas niveau, con√ßu pour les flux de travail complexes</td></tr>
<tr><td><strong>Capacit√© de base</strong></td><td>Boucle ReAct standard (Raison ‚Üí Appel d'outil ‚Üí Observation ‚Üí R√©ponse)</td><td>Machines √† √©tats personnalis√©es et logique de branchement complexe (StateGraph + routage conditionnel)</td></tr>
<tr><td><strong>M√©canisme d'extension</strong></td><td>Logiciel interm√©diaire pour les capacit√©s de production</td><td>Gestion manuelle des n≈ìuds, des ar√™tes et des transitions d'√©tat</td></tr>
<tr><td><strong>Mise en ≈ìuvre sous-jacente</strong></td><td>Gestion manuelle des n≈ìuds, des ar√™tes et des transitions d'√©tat</td><td>Ex√©cution native avec persistance et r√©cup√©ration int√©gr√©es</td></tr>
<tr><td><strong>Cas d'utilisation typiques</strong></td><td>80 % des sc√©narios d'agents standard</td><td>Collaboration multi-agents et orchestration de flux de travail √† long terme</td></tr>
<tr><td><strong>Courbe d'apprentissage</strong></td><td>Construire un agent en ~10 lignes de code</td><td>N√©cessite une compr√©hension des graphes d'√©tat et de l'orchestration des n≈ìuds</td></tr>
</tbody>
</table>
<p>Si vous √™tes novice en mati√®re de construction d'agents ou si vous souhaitez lancer un projet rapidement, commencez par LangChain. Si vous savez d√©j√† que votre cas d'utilisation n√©cessite une orchestration complexe, une collaboration multi-agent ou des workflows de longue dur√©e, passez directement √† LangGraph.</p>
<p>Les deux frameworks peuvent coexister dans le m√™me projet - vous pouvez commencer simplement avec LangChain et int√©grer LangGraph lorsque votre syst√®me a besoin de plus de contr√¥le et de flexibilit√©. L'essentiel est de choisir le bon outil pour chaque partie de votre flux de travail.</p>
<h2 id="Conclusion" class="common-anchor-header">Conclusion<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Il y a trois ans, LangChain a commenc√© comme une enveloppe l√©g√®re pour appeler les LLM. Aujourd'hui, il s'est transform√© en un framework complet de niveau production.</p>
<p>Au c≈ìur du syst√®me, des couches interm√©diaires assurent la s√©curit√©, la conformit√© et l'observabilit√©. LangGraph ajoute l'ex√©cution persistante, le flux de contr√¥le et la gestion d'√©tat. Enfin, au niveau de la m√©moire, <a href="https://milvus.io/">Milvus</a> comble une lacune essentielle en fournissant une m√©moire √† long terme √©volutive et fiable qui permet aux agents de retrouver le contexte, de raisonner sur l'historique et de s'am√©liorer au fil du temps.</p>
<p>Ensemble, LangChain, LangGraph et Milvus forment une cha√Æne d'outils pratique pour l'√®re des agents modernes, permettant un prototypage rapide et un d√©ploiement √† l'√©chelle de l'entreprise, sans sacrifier la fiabilit√© ou les performances.</p>
<p>üöÄ Pr√™t √† doter votre agent d'une m√©moire fiable et √† long terme ? D√©couvrez <a href="https://milvus.io">Milvus</a> et voyez comment il alimente une m√©moire √† long terme intelligente pour les agents LangChain en production.</p>
<p>Vous avez des questions ou souhaitez approfondir une fonctionnalit√© ? Rejoignez notre <a href="https://discord.com/invite/8uyFbECzPX">canal Discord</a> ou d√©posez des questions sur <a href="https://github.com/milvus-io/milvus">GitHub</a>. Vous pouvez √©galement r√©server une session individuelle de 20 minutes pour obtenir des informations, des conseils et des r√©ponses √† vos questions dans le cadre des <a href="https://milvus.io/blog/join-milvus-office-hours-to-get-support-from-vectordb-experts.md">Milvus Office Hours</a>.</p>
