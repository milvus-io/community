---
id: Building-an-AI-Powered-Writing-Assistant-with-WPS-Office.md
title: Création d'un assistant d'écriture doté d'une IA pour WPS Office
author: milvus
date: 2020-07-28T03:35:40.105Z
desc: >-
  Découvrez comment Kingsoft a exploité Milvus, un moteur de recherche de
  similarités open-source, pour créer un moteur de recommandation pour
  l'assistant d'écriture WPS Office alimenté par l'IA.
cover: assets.zilliz.com/wps_thumbnail_6cb7876963.jpg
tag: Scenarios
canonicalUrl: >-
  https://zilliz.com/blog/Building-an-AI-Powered-Writing-Assistant-with-WPS-Office
---
<custom-h1>Création d'un assistant d'écriture doté d'une IA pour WPS Office</custom-h1><p>WPS Office est un outil de productivité développé par Kingsoft qui compte plus de 150 millions d'utilisateurs dans le monde. Le département d'intelligence artificielle (IA) de l'entreprise a conçu un assistant d'écriture intelligent à partir de zéro en utilisant des algorithmes de correspondance sémantique tels que la reconnaissance d'intention et le regroupement de textes. L'outil existe à la fois sous forme d'application web et de <a href="https://walkthechat.com/wechat-mini-programs-simple-introduction/">mini-programme WeChat</a> qui aide les utilisateurs à créer rapidement des plans, des paragraphes individuels et des documents entiers en saisissant simplement un titre et en sélectionnant jusqu'à cinq mots-clés.</p>
<p>Le moteur de recommandation de l'assistant d'écriture utilise Milvus, un moteur de recherche de similarités open-source, pour alimenter son module principal de traitement vectoriel. Nous allons explorer ci-dessous le processus de création de l'assistant d'écriture intelligent de WPS Offices, notamment la manière dont les caractéristiques sont extraites des données non structurées ainsi que le rôle joué par Milvus dans le stockage des données et l'alimentation du moteur de recommandation de l'outil.</p>
<p>Aller à :</p>
<ul>
<li><a href="#building-an-ai-powered-writing-assistant-for-wps-office">Création d'un assistant de rédaction doté d'une IA pour WPS Office</a><ul>
<li><a href="#making-sense-of-unstructured-textual-data">Donner du sens aux données textuelles non structurées</a></li>
<li><a href="#using-the-tfidf-model-to-maximize-feature-extraction">Utilisation du modèle TFIDF pour maximiser l'extraction des caractéristiques</a></li>
<li><a href="#extracting-features-with-the-bi-directional-lstm-cnns-crf-deep-learning-model">Extraction de caractéristiques avec le modèle d'apprentissage profond bidirectionnel LSTM-CNNs-CRF</a></li>
<li><a href="#creating-sentence-embeddings-using-infersent">Création d'enchâssements de phrases à l'aide d'Infersent</a></li>
<li><a href="#storing-and-querying-vectors-with-milvus">Stockage et interrogation de vecteurs avec Milvus</a></li>
<li><a href="#ai-isnt-replacing-writers-its-helping-them-write">L'IA ne remplace pas les écrivains, elle les aide à écrire</a></li>
</ul></li>
</ul>
<h3 id="Making-sense-of-unstructured-textual-data" class="common-anchor-header">Donner un sens aux données textuelles non structurées</h3><p>Comme pour tout problème moderne digne d'être résolu, la construction de l'assistant d'écriture WPS commence par des données désordonnées. Des dizaines de millions de documents textuels denses dont il faut extraire des caractéristiques significatives, pour être un peu plus précis. Pour comprendre la complexité de ce problème, imaginons que deux journalistes de différents organes de presse traitent d'un même sujet.</p>
<p>Bien qu'ils adhèrent tous deux aux règles, principes et processus qui régissent la structure des phrases, ils choisiront des mots différents, créeront des phrases de longueur variable et utiliseront leurs propres structures d'articles pour raconter des histoires similaires (ou peut-être dissemblables). Contrairement aux ensembles de données structurés avec un nombre fixe de dimensions, les corps de texte manquent intrinsèquement de structure, car la syntaxe qui les régit est très malléable. Pour trouver du sens, il faut extraire des caractéristiques lisibles par une machine à partir d'un corpus de documents non structurés. Mais il faut d'abord nettoyer les données.</p>
<p>Il existe de nombreuses façons de nettoyer les données textuelles, dont aucune ne sera abordée en détail dans le présent article. Il s'agit néanmoins d'une étape importante qui précède le traitement des données et qui peut inclure la suppression des balises, des caractères accentués, l'expansion des contractions, la suppression des caractères spéciaux, la suppression des mots vides et bien d'autres choses encore. Une explication détaillée des méthodes de prétraitement et de nettoyage des données textuelles est disponible <a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">ici</a>.</p>
<h3 id="Using-the-TFIDF-model-to-maximize-feature-extraction" class="common-anchor-header">Utilisation du modèle TFIDF pour maximiser l'extraction des caractéristiques</h3><p>Pour commencer à donner un sens aux données textuelles non structurées, le modèle TFIDF (term frequency-inverse document frequency) a été appliqué au corpus dont l'assistant d'écriture WPS tire ses données. Ce modèle utilise une combinaison de deux mesures, la fréquence des termes et la fréquence inverse des documents, pour donner à chaque mot d'un document une valeur TFIDF. La fréquence des termes (TF) représente le nombre brut de termes dans un document divisé par le nombre total de termes dans le document, tandis que la fréquence inverse des documents (IDF) est le nombre de documents dans un corpus divisé par le nombre de documents dans lesquels un terme apparaît.</p>
<p>Le produit de la TF et de l'IDF fournit une mesure de la fréquence d'apparition d'un terme dans un document multipliée par le degré d'unicité du mot dans le corpus. En fin de compte, les valeurs TFIDF sont une mesure de la pertinence d'un mot par rapport à un document au sein d'une collection de documents. Les termes sont triés en fonction des valeurs TFIDF, et ceux qui ont des valeurs faibles (c'est-à-dire les mots courants) peuvent se voir accorder moins de poids lors de l'utilisation de l'apprentissage profond pour extraire des caractéristiques du corpus.</p>
<h3 id="Extracting-features-with-the-bi-directional-LSTM-CNNs-CRF-deep-learning-model" class="common-anchor-header">Extraction de caractéristiques avec le modèle d'apprentissage profond bidirectionnel LSTM-CNNs-CRF</h3><p>En utilisant une combinaison de mémoire à long terme bidirectionnelle (BLSTM), de réseaux neuronaux convolutifs (CNN) et de champs aléatoires conditionnels (CRF), il est possible d'extraire du corpus des représentations au niveau des mots et des caractères. Le <a href="https://arxiv.org/pdf/1603.01354.pdf">modèle BLSTM-CNNs-CRF</a> utilisé pour construire l'assistant d'écriture WPS Office fonctionne comme suit :</p>
<ol>
<li><strong>CNN :</strong> Les enchâssements de caractères sont utilisés comme entrées dans le CNN, puis les structures de mots sémantiquement pertinentes (c'est-à-dire le préfixe ou le suffixe) sont extraites et encodées dans des vecteurs de représentation au niveau des caractères.</li>
<li><strong>BLSTM :</strong> les vecteurs au niveau des caractères sont concaténés avec les vecteurs d'intégration des mots, puis introduits dans le réseau BLSTM. Chaque séquence est présentée en avant et en arrière à deux états cachés distincts pour capturer les informations passées et futures.</li>
<li><strong>CRF :</strong> les vecteurs de sortie du réseau BLSTM sont introduits dans la couche CRF pour décoder conjointement la meilleure séquence d'étiquettes.</li>
</ol>
<p>Le réseau neuronal est désormais capable d'extraire et de classer des entités nommées à partir d'un texte non structuré. Ce processus, appelé <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">reconnaissance des entités nommées (NER)</a>, consiste à localiser et à classer des catégories telles que des noms de personnes, des institutions, des lieux géographiques, etc. Ces entités jouent un rôle important dans le tri et le rappel des données. À partir de là, des phrases, des paragraphes et des résumés clés peuvent être extraits du corpus.</p>
<h3 id="Creating-sentence-embeddings-using-Infersent" class="common-anchor-header">Création d'enchâssements de phrases à l'aide d'Infersent</h3><p><a href="https://github.com/facebookresearch/InferSent">Infersent</a>, une méthode supervisée d'intégration de phrases conçue par Facebook qui intègre des phrases complètes dans un espace vectoriel, est utilisée pour créer des vecteurs qui seront introduits dans la base de données Milvus. Infersent a été entraîné à l'aide du corpus Stanford Natural Language Inference (SNLI), qui contient 570 000 paires de phrases écrites et étiquetées par des humains. Des informations supplémentaires sur le fonctionnement d'Infersent sont disponibles <a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">ici</a>.</p>
<h3 id="Storing-and-querying-vectors-with-Milvus" class="common-anchor-header">Stockage et interrogation de vecteurs avec Milvus</h3><p><a href="https://www.milvus.io/">Milvus</a> est un moteur de recherche de similarités open source qui prend en charge l'ajout, la suppression, la mise à jour et la recherche en temps quasi réel d'embeddings à l'échelle d'un trillion d'octets. Pour améliorer les performances des requêtes, Milvus permet de spécifier un type d'index pour chaque champ de vecteur. L'assistant intelligent WPS Office utilise l'index IVF_FLAT, le type d'index Inverted File (IVF) le plus basique, où "flat" signifie que les vecteurs sont stockés sans compression ni quantification. Le regroupement est basé sur IndexFlat2, qui utilise la recherche exacte pour la distance L2.</p>
<p>Bien que l'index IVF_FLAT ait un taux de rappel de 100 %, son absence de compression se traduit par des vitesses d'interrogation comparativement lentes. La <a href="https://milvus.io/docs/manage-partitions.md">fonction de partitionnement de</a> Milvus est utilisée pour diviser les données en plusieurs parties du stockage physique sur la base de règles prédéfinies, ce qui rend les requêtes plus rapides et plus précises. Lorsque des vecteurs sont ajoutés à Milvus, les balises spécifient la partition à laquelle les données doivent être ajoutées. Les requêtes sur les données vectorielles utilisent des balises pour spécifier la partition sur laquelle la requête doit être exécutée. Les données peuvent être décomposées en segments au sein de chaque partition afin d'améliorer encore la vitesse.</p>
<p>L'assistant d'écriture intelligent utilise également des clusters Kubernetes, permettant aux conteneurs d'applications de s'exécuter sur plusieurs machines et environnements, ainsi que MySQL pour la gestion des métadonnées.</p>
<h3 id="AI-isn’t-replacing-writers-it’s-helping-them-write" class="common-anchor-header">L'IA ne remplace pas les écrivains, elle les aide à écrire</h3><p>L'assistant d'écriture de Kingsoft pour WPS Office s'appuie sur Milvus pour gérer et interroger une base de données de plus de 2 millions de documents. Le système est très flexible, capable d'effectuer des recherches en temps quasi réel sur des ensembles de données à l'échelle du trillion. Les requêtes sont effectuées en 0,2 seconde en moyenne, ce qui signifie que des documents entiers peuvent être générés presque instantanément à partir d'un simple titre ou de quelques mots-clés. Bien que l'IA ne remplace pas les rédacteurs professionnels, la technologie qui existe aujourd'hui est capable d'augmenter le processus d'écriture de manière nouvelle et intéressante. L'avenir est inconnu, mais les écrivains peuvent au moins s'attendre à des méthodes plus productives, et pour certains moins difficiles, pour "mettre le stylo sur le papier".</p>
<p>Les sources suivantes ont été utilisées pour cet article :</p>
<ul>
<li>"<a href="https://arxiv.org/pdf/1603.01354.pdf">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>", Xuezhe Ma et Eduard Hovy.</li>
<li>"<a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">Méthodes traditionnelles pour les données textuelles</a>", Dipanjan (DJ) Sarkar.</li>
<li>"<a href="https://ieeexplore.ieee.org/document/8780663">Text Features Extraction based on TF-IDF Associating Semantic</a>", Qing Liu, Jing Wang, Dehai Zhang, Yun Yang, NaiYao Wang.</li>
<li>"<a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">Understanding Sentence Embeddings using Facebook's Infersent</a>", Rehan Ahmad.</li>
<li>"<a href="https://arxiv.org/pdf/1705.02364.pdf">Apprentissage supervisé de représentations universelles de phrases à partir de données d'inférence en langage naturel</a>", Alexis Conneau, Douwe Kiela, Holger Schwenk, LoÏc Barrault, Antoine Bordes.V1</li>
</ul>
<p>Lisez d'autres <a href="https://zilliz.com/user-stories">témoignages d'utilisateurs</a> pour en savoir plus sur la fabrication de produits avec Milvus.</p>
