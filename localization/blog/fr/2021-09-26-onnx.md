---
id: 2021-09-26-onnx.md
title: Traiter les modèles avec ONNX
date: 2021-09-26T00:00:00.000Z
desc: >-
  comment utiliser des modèles multiples pour la recherche d'images en se basant
  sur ONNX et Milvus
cover: assets.zilliz.com/medium_1_cfb009269a.png
tag: Engineering
canonicalUrl: >-
  https://zilliz.com/blog/combine-ai-models-for-image-search-using-onnx-and-milvus
---
<custom-h1>Combiner des modèles d'IA pour la recherche d'images à l'aide d'ONNX et de Milvus</custom-h1><p>Open Neural Network Exchange (ONNX) est un format ouvert construit pour représenter les modèles d'apprentissage automatique. Depuis son ouverture en 2017, ONNX est devenu une norme pour l'IA, fournissant des blocs de construction pour les modèles d'apprentissage automatique et d'apprentissage profond. ONNX définit un format de fichier commun pour permettre aux développeurs d'IA d'utiliser des modèles avec différents frameworks, outils, runtimes et compilateurs, et contribue à augmenter la vitesse d'innovation dans la communauté de l'intelligence artificielle.</p>
<p>Milvus est une base de données vectorielles open-source très flexible, fiable et extrêmement rapide. Elle prend en charge l'ajout, la suppression, la mise à jour et la recherche en temps quasi réel de vecteurs. Milvus dispose d'un ensemble complet d'API intuitives et prend en charge de nombreuses bibliothèques d'index largement adoptées (par exemple Faiss, NMSLIB et Annoy), ce qui simplifie la sélection de l'index pour un scénario donné. Milvus est simple d'utilisation et a été utilisé dans des centaines d'organisations et d'institutions dans le monde entier, notamment pour la recherche d'images, d'audio et de vidéo, la recommandation, le chatbot, la recherche de nouveaux médicaments, etc.</p>
<p>Cet article vous présentera comment utiliser plusieurs modèles pour la recherche d'images en se basant sur ONNX et Milvus. Il prend les modèles VGG16 et ResNet50 comme exemples, utilise ONNX pour exécuter différents modèles d'IA afin de générer des vecteurs de caractéristiques, et enfin effectue une recherche de vecteurs de caractéristiques dans Milvus pour renvoyer des images similaires.</p>
<h2 id="Process-Models-with-ONNX" class="common-anchor-header">Traiter les modèles avec ONNX<button data-href="#Process-Models-with-ONNX" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Le format ONNX peut être facilement échangé entre les modèles d'IA. Par exemple, le modèle TensorFlow peut être converti au format ONNX et exécuté dans l'environnement Caffe. Dans cet exemple, nous convertissons le modèle ResNet50 pré-entraîné sous le cadre Keras au format ONNX, puis nous appelons le modèle VGG16 au format ONNX pour analyser différents modèles.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> keras.applications.resnet50 <span class="hljs-keyword">import</span> ResNet50
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># load keras-resnet50 model and save as a floder</span>
model_resnet50 = ResNet50(include_top=<span class="hljs-literal">False</span>, pooling=<span class="hljs-string">&#x27;max&#x27;</span>, weights=<span class="hljs-string">&#x27;imagenet&#x27;</span>)
tf.saved_model.save(model_resnet50, <span class="hljs-string">&quot;keras_resnet50_model&quot;</span>)

<span class="hljs-comment"># convert resnet50 model to onnx</span>
! python -m tf2onnx.convert --saved-model <span class="hljs-string">&quot;keras_resnet50_model&quot;</span> --output <span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>
<button class="copy-code-btn"></button></code></pre>
<p>Remarque : lorsque nous avons utilisé l'interface <code translate="no">keras2onnx.convert_keras(model, model.name)</code> pour convertir le modèle, elle renvoie l'erreur <code translate="no">AttributeError:'KerasTensor' object has no attribute'graph'</code>. Nous pouvons alors utiliser la commande Bash de Python pour convertir le modèle en suivant la solution proposée par Stack Overflow.</p>
<h2 id="Extract-Feature-Vectors-using-Models" class="common-anchor-header">Extraction des vecteurs de caractéristiques à l'aide des modèles<button data-href="#Extract-Feature-Vectors-using-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Après avoir converti le modèle ResNet50 au format ONNX, vous pouvez extraire le vecteur de caractéristiques de l'image directement par l'inférence. Remarque : les vecteurs de caractéristiques doivent être normalisés après l'extraction.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># get the image vectors with onnx model</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_onnx_vectors</span>(<span class="hljs-params">onnx_model, img_path</span>):
    img = image.load_img(img_path, target_size=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=<span class="hljs-number">0</span>)
    x = preprocess_input(x)
    
    sess = onnxruntime.InferenceSession(onnx_model)
    x = x <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(x, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> [x]
    feed = <span class="hljs-built_in">dict</span>([(<span class="hljs-built_in">input</span>.name, x[n]) <span class="hljs-keyword">for</span> n, <span class="hljs-built_in">input</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sess.get_inputs())])
    feat = sess.run(<span class="hljs-literal">None</span>, feed)[<span class="hljs-number">0</span>]
    
    norm_feat = feat[<span class="hljs-number">0</span>] / LA.norm(feat[<span class="hljs-number">0</span>])
    norm_feat = [i.item() <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> norm_feat]
    <span class="hljs-keyword">return</span> norm_feat
<button class="copy-code-btn"></button></code></pre>
<p>Utilisez le modèle VGG16 au format ONNX pour traiter les données d'image :</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># generate vectors with ResNet50 and VGG16 ONNX model</span>
2vec_resnet = get_onnx_vectors(<span class="hljs-string">&quot;onnx_resnet50.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
3vec_vgg = get_onnx_vectors(<span class="hljs-string">&quot;onnx_vgg16.onnx&quot;</span>, <span class="hljs-string">&quot;./pic/example.jpg&quot;</span>)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Store-Vector-Data" class="common-anchor-header">Stocker les données vectorielles<button data-href="#Store-Vector-Data" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Les données non structurées telles que les images ne peuvent pas être traitées directement par un ordinateur, mais elles peuvent être converties en vecteurs par le biais d'un modèle d'IA, puis analysées par un ordinateur. La base de données vectorielles Milvus est conçue pour l'analyse massive de données non structurées. Elle peut stocker des données vectorielles et effectuer des analyses en temps quasi réel. Tout d'abord, créez une collection du modèle correspondant dans Milvus, puis insérez les vecteurs d'image.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">from</span> milvus <span class="hljs-keyword">import</span> *

<span class="hljs-comment"># create collections in Milvus</span>
milvus.create_collection(resnet_collection_param)
milvus.create_collection(vgg_collection_param)

<span class="hljs-comment"># insert data to Milvus and return ids</span>
status, resnet_ids = milvus.insert(resnet_collection_name, resnet_vectors)
status, vgg_ids = milvus.insert(vgg_collection_name, vgg_vectors)
<button class="copy-code-btn"></button></code></pre>
<p>Après avoir inséré les données avec succès, Milvus renvoie l'ID correspondant au vecteur, ce qui permet de trouver l'image par ID. Étant donné que Milvus 1.1 utilisé dans ce cas ne prend pas en charge le filtrage scalaire (que Milvus 2.0 prend désormais en charge), Redis est utilisé pour stocker l'ID du vecteur et la clé-valeur du chemin d'accès à l'image.</p>
<pre><code translate="no" class="language-python"><span class="hljs-keyword">import</span> redis
<span class="hljs-keyword">def</span> <span class="hljs-title function_">img_ids_to_redis</span>(<span class="hljs-params">img_directory, res_ids</span>):
  <span class="hljs-keyword">for</span> img, ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(images, res_ids):
    redis.<span class="hljs-built_in">set</span>(ids, img)
<button class="copy-code-btn"></button></code></pre>
<h2 id="Search-for-Similar-Images" class="common-anchor-header">Recherche d'images similaires<button data-href="#Search-for-Similar-Images" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Après avoir stocké les données, nous pouvons récupérer le vecteur. Milvus prend en charge plusieurs méthodes de calcul de la distance, notamment la distance euclidienne, le produit intérieur et la distance de Hamming. La recherche de similarité d'images dans cet article adopte le calcul de la distance euclidienne entre les vecteurs dans Milvus, renvoie l'ID du vecteur similaire, puis trouve l'image correspondant à l'ID dans Redis.</p>
<pre><code translate="no" class="language-python"><span class="hljs-comment"># search in Milvus and return the similarly results with ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">search_in_milvus</span>(<span class="hljs-params">collection_name, search_vector</span>):
    status, results = milvus.search(collection_name, TOP_K, [search_vector])
    <span class="hljs-built_in">print</span>(status)
    re_ids = [x.<span class="hljs-built_in">id</span> <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    re_distance = [x.distance <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> results[<span class="hljs-number">0</span>]]
    <span class="hljs-keyword">return</span> re_ids, re_distance
    
<span class="hljs-comment"># get the images according the result ids</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">get_sim_imgs</span>(<span class="hljs-params">collection_name, search_vector</span>):
    ids, distance = search_in_milvus(collection_name, search_vector)
    img = [red.get(i).decode(<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> ids]
    <span class="hljs-keyword">return</span> ids, distance, img
<button class="copy-code-btn"></button></code></pre>
<p>En prenant les modèles VGG16 et ResNet50 comme exemples, cet article montre le traitement de plusieurs modèles par ONNX et la combinaison de plusieurs modèles avec Milvus pour la recherche de vecteurs similaires afin d'obtenir des images similaires. Les deux modèles ci-dessus sont basés sur le cadre Keras, qui peut rapidement extraire des vecteurs de caractéristiques. Le carnet de notes montre que, bien que les résultats de la recherche d'images par Milvus sur l'ensemble de données COCO basée sur ces deux modèles soient similaires, leurs distances euclidiennes ne sont pas les mêmes. Vous pouvez également essayer de comparer les résultats de recherche des deux modèles en utilisant d'autres ensembles de données.</p>
<p>Milvus est une base de données vectorielles hautement performante et disponible qui peut être utilisée pour traiter les vecteurs de caractéristiques générés à partir de données massives non structurées. Pour plus de solutions, vous pouvez vous référer à <a href="https://github.com/milvus-io/bootcamp">Milvus bootcamp</a>.</p>
<h2 id="References" class="common-anchor-header">Références<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ol>
<li>https://github.com/onnx/onnx</li>
<li>https://onnx.ai/</li>
<li>https://milvus.io/cn/</li>
<li>https://github.com/milvus-io/bootcamp</li>
</ol>
<h3 id="About-author" class="common-anchor-header">À propos de l'auteur</h3><p>Shiyu Chen, ingénieur en données chez Zilliz, est diplômée en informatique de l'université de Xidian. Depuis qu'elle a rejoint Zilliz, elle a exploré des solutions pour Milvus dans divers domaines, tels que l'analyse audio et vidéo, la recherche de formules de molécules, etc. Elle explore actuellement d'autres solutions intéressantes. Pendant son temps libre, elle aime le sport et la lecture.</p>
