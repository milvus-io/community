{"codeList":["pip install google-adk pymilvus google-generativeai  \n","\"\"\"  \nADK + Milvus + Gemini Long-term Memory Agent  \nDemonstrates how to implement a cross-session memory recall system  \n\"\"\"  \nimport os  \nimport asyncio  \nimport time  \nfrom pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility  \nimport google.generativeai as genai  \nfrom google.adk.agents import Agent  \nfrom google.adk.tools import FunctionTool  \nfrom google.adk.runners import Runner  \nfrom google.adk.sessions import InMemorySessionService  \nfrom google.genai import types  \n","wget <https://github.com/Milvus-io/Milvus/releases/download/v2.5.12/Milvus-standalone-docker-compose.yml> -O docker-compose.yml  \n","docker-compose up -d  \n","docker-compose ps -a  \n","# ==================== Configuration ====================  \n# 1. Gemini API configuration  \nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")  \nif not GOOGLE_API_KEY:  \n   raise ValueError(\"Please set the GOOGLE_API_KEY environment variable\")  \ngenai.configure(api_key=GOOGLE_API_KEY)  \n# 2. Milvus connection configuration  \nMILVUS_HOST = os.getenv(\"MILVUS_HOST\", \"localhost\")  \nMILVUS_PORT = os.getenv(\"MILVUS_PORT\", \"19530\")  \n# 3. Model selection (best combination within the free tier limits)  \nLLM_MODEL = \"gemini-2.5-flash-lite\"  # LLM model: 1000 RPD  \nEMBEDDING_MODEL = \"models/text-embedding-004\"  # Embedding model: 1000 RPD  \nEMBEDDING_DIM = 768  # Vector dimension  \n# 4. Application configuration  \nAPP_NAME = \"tech_support\"  \nUSER_ID = \"user_123\"  \nprint(f\"‚úì Using model configuration:\")  \nprint(f\"  LLM: {LLM_MODEL}\")  \nprint(f\"  Embedding: {EMBEDDING_MODEL} (dimension: {EMBEDDING_DIM})\")  \n","# ==================== Initialize Milvus ====================  \ndef init_milvus():  \n   \"\"\"Initialize Milvus connection and collection\"\"\"  \n   # Step 1: Establish connection  \n   Try:  \n       connections.connect(  \n           alias=\"default\",  \n           host=MILVUS_HOST,  \n           port=MILVUS_PORT  \n       )  \n       print(f\"‚úì Connected to Milvus: {MILVUS_HOST}:{MILVUS_PORT}\")  \n   except Exception as e:  \n       print(f\"‚úó Failed to connect to Milvus: {e}\")  \n       print(\"Hint: make sure Milvus is running\")  \n       Raise  \n   # Step 2: Define data schema  \n   fields = [  \n       FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),  \n       FieldSchema(name=\"user_id\", dtype=DataType.VARCHAR, max_length=100),  \n       FieldSchema(name=\"session_id\", dtype=DataType.VARCHAR, max_length=100),  \n       FieldSchema(name=\"question\", dtype=DataType.VARCHAR, max_length=2000),  \n       FieldSchema(name=\"solution\", dtype=DataType.VARCHAR, max_length=5000),  \n       FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=EMBEDDING_DIM),  \n       FieldSchema(name=\"timestamp\", dtype=DataType.INT64)  \n   ]  \n   schema = CollectionSchema(fields, description=\"Tech support memory\")  \n   collection_name = \"support_memory\"  \n   # Step 3: Create or load the collection  \n   if utility.has_collection(collection_name):  \n       memory_collection = Collection(name=collection_name)  \n       print(f\"‚úì Collection '{collection_name}' already exists\")  \n   Else:  \n       memory_collection = Collection(name=collection_name, schema=schema)  \n   # Step 4: Create vector index  \n   index_params = {  \n       \"index_type\": \"IVF_FLAT\",  \n       \"metric_type\": \"COSINE\",  \n       \"params\": {\"nlist\": 128}  \n   }  \n   memory_collection.create_index(field_name=\"embedding\", index_params=index_params)  \n   print(f\"‚úì Created collection '{collection_name}' and index\")  \n   return memory_collection  \n# Run initialization  \nmemory_collection = init_milvus()  \n","# ==================== Memory Operation Functions ====================  \ndef store_memory(question: str, solution: str) -> str:  \n   \"\"\"  \n   Store a solution record into the memory store  \n   Args:  \n       question: the user's question  \n       solution: the solution  \n   Returns:  \n       str: result message  \n   \"\"\"  \n   Try:  \n       print(f\"\\\\n[Tool Call] store_memory\")  \n       print(f\" - question: {question[:50]}...\")  \n       print(f\" - solution: {solution[:50]}...\")  \n       # Use global USER_ID (in production, this should come from ToolContext)  \n       user_id = USER_ID  \n       session_id = f\"session_{int(time.time())}\"  \n       # Key step 1: convert the question into a 768-dimensional vector  \n       embedding_result = genai.embed_content(  \n           model=EMBEDDING_MODEL,  \n           content=question,  \n           task_type=\"retrieval_document\",  # specify document indexing task  \n           output_dimensionality=EMBEDDING_DIM  \n       )  \n       embedding = embedding_result[\"embedding\"]  \n       # Key step 2: insert into Milvus  \n       memory_collection.insert([{  \n           \"user_id\": user_id,  \n           \"session_id\": session_id,  \n           \"question\": question,  \n           \"solution\": solution,  \n           \"embedding\": embedding,  \n           \"timestamp\": int(time.time())  \n       }])  \n       # Key step 3: flush to disk (ensure data persistence)  \n       memory_collection.flush()  \n       result = \"‚úì Successfully stored in memory\"  \n       print(f\"[Tool Result] {result}\")  \n       return result  \n   except Exception as e:  \n       error_msg = f\"‚úó Storage failed: {str(e)}\"  \n       print(f\"[Tool Error] {error_msg}\")  \n       return error_msg  \n","def recall_memory(query: str, top_k: int = 3) -> str:  \n   \"\"\"  \n   Retrieve relevant historical cases from the memory store  \n   Args:  \n       query: query question  \n       top_k: number of most similar results to return  \n   Returns:  \n       str: retrieval result  \n   \"\"\"  \n   Try:  \n       print(f\"\\\\n[Tool Call] recall_memory\")  \n       print(f\" - query: {query}\")  \n       print(f\" - top_k: {top_k}\")  \n       user_id = USER_ID  \n       # Key step 1: convert the query into a vector  \n       embedding_result = genai.embed_content(  \n           model=EMBEDDING_MODEL,  \n           content=query,  \n           task_type=\"retrieval_query\",  # specify query task (different from indexing)  \n           output_dimensionality=EMBEDDING_DIM  \n       )  \n       query_embedding = embedding_result[\"embedding\"]  \n       # Key step 2: load the collection into memory (required for the first query)  \n       memory_collection.load()  \n       # Key step 3: hybrid search (vector similarity + scalar filtering)  \n       results = memory_collection.search(  \n           data=[query_embedding],  \n           anns_field=\"embedding\",  \n           param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},  \n           limit=top_k,  \n           expr=f'user_id == \"{user_id}\"',  # üîë key to user isolation  \n           output_fields=[\"question\", \"solution\", \"timestamp\"]  \n       )  \n       # Key step 4: format results  \n       if not results[0]:  \n           result = \"No relevant historical cases found\"  \n           print(f\"[Tool Result] {result}\")  \n           return result  \n       result_text = f\"Found {len(results[0])} relevant cases:\\\\n\\\\n\"  \n       for i, hit in enumerate(results[0]):  \n           result_text += f\"Case {i+1} (similarity: {hit.score:.2f}):\\\\n\"  \n           result_text += f\"Question: {hit.entity.get('question')}\\\\n\"  \n           result_text += f\"Solution: {hit.entity.get('solution')}\\\\n\\\\n\"  \n       print(f\"[Tool Result] Found {len(results[0])} cases\")  \n       return result_text  \n   except Exception as e:  \n       error_msg = f\"Retrieval failed: {str(e)}\"  \n       print(f\"[Tool Error] {error_msg}\")  \n       return error_msg  \n","# Usage  \n# Wrap functions with FunctionTool  \nstore_memory_tool = FunctionTool(func=store_memory)  \nrecall_memory_tool = FunctionTool(func=recall_memory)  \nmemory_tools = [store_memory_tool, recall_memory_tool]  \n","# ==================== Create Agent ====================  \nsupport_agent = Agent(  \n   model=LLM_MODEL,  \n   name=\"support_agent\",  \n   description=\"Technical support expert agent that can remember and recall historical cases\",  \n   # Key: the instruction defines the agent‚Äôs behavior  \n   instruction=\"\"\"  \nYou are a technical support expert. Strictly follow the process below:  \n<b>When the user asks a technical question:</b>  \n1. Immediately call the recall_memory tool to search for historical cases  \n  - Parameter query: use the user‚Äôs question text directly  \n  - Do not ask for any additional information; call the tool directly  \n2. Answer based on the retrieval result:  \n  - If relevant cases are found: explain that similar historical cases were found and answer by referencing their solutions  \n  - If no cases are found: explain that this is a new issue and answer based on your own knowledge  \n3. After answering, ask: ‚ÄúDid this solution resolve your issue?‚Äù  \n<b>When the user confirms the issue is resolved:</b>  \n- Immediately call the store_memory tool to save this Q&A  \n- Parameter question: the user‚Äôs original question  \n- Parameter solution: the complete solution you provided  \n<b>Important rules:</b>  \n- You must call a tool before answering  \n- Do not ask for user_id or any other parameters  \n- Only store memory when you see confirmation phrases such as ‚Äúresolved‚Äù, ‚Äúit works‚Äù, or ‚Äúthanks‚Äù  \n\"\"\",  \n   tools=memory_tools  \n)  \n","# ==================== Main Program ====================  \nasync def main():  \n   \"\"\"Demonstrate cross-session memory recall\"\"\"  \n   # Create Session service and Runner  \n   session_service = InMemorySessionService()  \n   runner = Runner(  \n       agent=support_agent,  \n       app_name=APP_NAME,  \n       session_service=session_service  \n   )  \n   # ========== First round: build memory ==========  \n   print(\"\\\\n\" + \"=\" \\* 60)  \n   print(\"First conversation: user asks a question and the solution is stored\")  \n   print(\"=\" \\* 60)  \n   session1 = await session_service.create_session(  \n       app_name=APP_NAME,  \n       user_id=USER_ID,  \n       session_id=\"session_001\"  \n   )  \n   # User asks the first question  \n   print(\"\\\\n[User]: What should I do if Milvus connection times out?\")  \n   content1 = types.Content(  \n       role='user',  \n       parts=[types.Part(text=\"What should I do if Milvus connection times out?\")]  \n   )  \n   async for event in runner.run_async(  \n       user_id=USER_ID,  \n       session_id=[session1.id](http://session1.id),  \n       new_message=content1  \n   ):  \n       if event.content and event.content.parts:  \n           for part in event.content.parts:  \n               if hasattr(part, 'text') and part.text:  \n                   print(f\"[Agent]: {part.text}\")  \n   # User confirms the issue is resolved  \n   print(\"\\\\n[User]: The issue is resolved, thanks!\")  \n   content2 = types.Content(  \n       role='user',  \n       parts=[types.Part(text=\"The issue is resolved, thanks!\")]  \n   )  \n   async for event in runner.run_async(  \n       user_id=USER_ID,  \n       session_id=[session1.id](http://session1.id),  \n       new_message=content2  \n   ):  \n       if event.content and event.content.parts:  \n           for part in event.content.parts:  \n               if hasattr(part, 'text') and part.text:  \n                   print(f\"[Agent]: {part.text}\")  \n   # ========== Second round: recall memory ==========  \n   print(\"\\\\n\" + \"=\" \\* 60)  \n   print(\"Second conversation: new session with memory recall\")  \n   print(\"=\" \\* 60)  \n   session2 = await session_service.create_session(  \n       app_name=APP_NAME,  \n       user_id=USER_ID,  \n       session_id=\"session_002\"  \n   )  \n   # User asks a similar question in a new session  \n   print(\"\\\\n[User]: Milvus can't connect\")  \n   content3 = types.Content(  \n       role='user',  \n       parts=[types.Part(text=\"Milvus can't connect\")]  \n   )  \n   async for event in runner.run_async(  \n       user_id=USER_ID,  \n       session_id=[session2.id](http://session2.id),  \n       new_message=content3  \n   ):  \n       if event.content and event.content.parts:  \n           for part in event.content.parts:  \n               if hasattr(part, 'text') and part.text:  \n                   print(f\"[Agent]: {part.text}\")\n\n  \n# Program entry point  \nif __name__ == \"__main__\":  \n   Try:  \n       asyncio.run(main())  \n   except KeyboardInterrupt:  \n       print(\"\\\\n\\\\nProgram exited\")  \n   except Exception as e:  \n       print(f\"\\\\n\\\\nProgram error: {e}\")  \n       import traceback  \n       traceback.print_exc()  \n   Finally:  \n       Try:  \n           connections.disconnect(alias=\"default\")  \n           print(\"\\\\n‚úì Disconnected from Milvus\")  \n       Except:  \n           pass  \n","export GOOGLE_API_KEY=\"your-gemini-api-key\"  \n","python milvus_agent.py  \n"],"headingContent":"","anchorList":[{"label":"Principes de conception d'ADK","href":"ADK‚Äôs-Core-Design-Principles","type":2,"isActive":false},{"label":"Milvus en tant que backend m√©moire pour ADK","href":"Milvus-as-the-Memory-Backend-for-ADK","type":2,"isActive":false},{"label":"Construction d'un agent avec Long-TermMemory aliment√© par Milvus","href":"Building-an-Agent-with-Long-TermMemory-Powered-by-Milvus","type":2,"isActive":false},{"label":"Conclusion","href":"Conclusion","type":2,"isActive":false}]}