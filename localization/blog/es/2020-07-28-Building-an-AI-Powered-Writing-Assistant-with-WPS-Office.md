---
id: Building-an-AI-Powered-Writing-Assistant-with-WPS-Office.md
title: >-
  Creación de un asistente de escritura basado en inteligencia artificial para
  WPS Office
author: milvus
date: 2020-07-28T03:35:40.105Z
desc: >-
  Descubra cómo Kingsoft aprovechó Milvus, un motor de búsqueda de similitudes
  de código abierto, para crear un motor de recomendaciones para el asistente de
  escritura con IA de WPS Office.
cover: assets.zilliz.com/wps_thumbnail_6cb7876963.jpg
tag: Scenarios
canonicalUrl: >-
  https://zilliz.com/blog/Building-an-AI-Powered-Writing-Assistant-with-WPS-Office
---
<custom-h1>Creación de un asistente de escritura con IA para WPS Office</custom-h1><p>WPS Office es una herramienta de productividad desarrollada por Kingsoft con más de 150 millones de usuarios en todo el mundo. El departamento de inteligencia artificial (IA) de la empresa ha creado desde cero un asistente de escritura inteligente utilizando algoritmos de correspondencia semántica como el reconocimiento de intenciones y la agrupación de textos. La herramienta existe como aplicación web y como <a href="https://walkthechat.com/wechat-mini-programs-simple-introduction/">miniprograma de WeChat</a> que ayuda a los usuarios a crear rápidamente esquemas, párrafos individuales y documentos completos con sólo introducir un título y seleccionar hasta cinco palabras clave.</p>
<p>El motor de recomendación del asistente de escritura utiliza Milvus, un motor de búsqueda de similitudes de código abierto, para alimentar su módulo central de procesamiento vectorial. A continuación exploraremos el proceso de creación del asistente de escritura inteligente de WPS Offices, incluyendo cómo se extraen las características de los datos no estructurados, así como el papel que desempeña Milvus en el almacenamiento de datos y en el motor de recomendación de la herramienta.</p>
<p>Ir a:</p>
<ul>
<li><a href="#building-an-ai-powered-writing-assistant-for-wps-office">Creación de un asistente de escritura basado en IA para WPS Office</a><ul>
<li><a href="#making-sense-of-unstructured-textual-data">Dar sentido a los datos textuales no estructurados</a></li>
<li><a href="#using-the-tfidf-model-to-maximize-feature-extraction">Uso del modelo TFIDF para maximizar la extracción de características</a></li>
<li><a href="#extracting-features-with-the-bi-directional-lstm-cnns-crf-deep-learning-model">Extracción de características con el modelo de aprendizaje profundo bidireccional LSTM-CNNs-CRF</a></li>
<li><a href="#creating-sentence-embeddings-using-infersent">Creación de incrustaciones de frases con Infersent</a></li>
<li><a href="#storing-and-querying-vectors-with-milvus">Almacenamiento y consulta de vectores con Milvus</a></li>
<li><a href="#ai-isnt-replacing-writers-its-helping-them-write">La IA no sustituye a los escritores, sino que les ayuda a escribir</a></li>
</ul></li>
</ul>
<h3 id="Making-sense-of-unstructured-textual-data" class="common-anchor-header">Dar sentido a los datos textuales no estructurados</h3><p>Al igual que cualquier problema moderno que merezca la pena resolver, la creación del asistente de escritura WPS comienza con datos desordenados. Decenas de millones de documentos de texto denso de los que hay que extraer características significativas, para ser un poco más precisos. Para entender la complejidad de este problema, piense en cómo dos periodistas de diferentes medios de comunicación pueden informar sobre el mismo tema.</p>
<p>Aunque ambos respetarán las normas, los principios y los procesos que rigen la estructura de las frases, elegirán palabras diferentes, crearán frases de distinta longitud y utilizarán sus propias estructuras de artículos para contar historias similares (o quizá diferentes). A diferencia de los conjuntos de datos estructurados con un número fijo de dimensiones, los cuerpos de texto carecen intrínsecamente de estructura porque la sintaxis que los rige es muy maleable. Para encontrar un significado, hay que extraer características legibles por máquina de un corpus de documentos no estructurados. Pero primero hay que limpiar los datos.</p>
<p>Hay varias formas de limpiar los datos textuales, ninguna de las cuales se tratará en profundidad en este artículo. No obstante, se trata de un paso importante que precede al procesamiento de los datos y que puede incluir la eliminación de etiquetas, la eliminación de caracteres acentuados, la expansión de contracciones, la eliminación de caracteres especiales, la eliminación de palabras vacías, etc. <a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">Aquí</a> encontrará una explicación detallada de los métodos de preprocesamiento y limpieza de datos de texto.</p>
<h3 id="Using-the-TFIDF-model-to-maximize-feature-extraction" class="common-anchor-header">Uso del modelo TFIDF para maximizar la extracción de características</h3><p>Para empezar a dar sentido a los datos textuales no estructurados, se aplicó el modelo de frecuencia de términos-frecuencia inversa de documentos (TFIDF) al corpus del que se nutre el asistente de escritura WPS. Este modelo utiliza una combinación de dos métricas, la frecuencia de términos y la frecuencia inversa de documentos, para asignar a cada palabra de un documento un valor TFIDF. La frecuencia de términos (TF) representa el recuento bruto de un término en un documento dividido por el número total de términos en el documento, mientras que la frecuencia inversa de documentos (IDF) es el número de documentos en un corpus dividido por el número de documentos en los que aparece un término.</p>
<p>El producto de TF e IDF proporciona una medida de la frecuencia con la que aparece un término en un documento multiplicada por la singularidad de la palabra en el corpus. En última instancia, los valores TFIDF son una medida de la relevancia de una palabra en un documento dentro de una colección de documentos. Los términos se ordenan por valores TFIDF, y aquellos con valores bajos (es decir, palabras comunes) pueden tener menos peso cuando se utiliza el aprendizaje profundo para extraer características del corpus.</p>
<h3 id="Extracting-features-with-the-bi-directional-LSTM-CNNs-CRF-deep-learning-model" class="common-anchor-header">Extracción de características con el modelo de aprendizaje profundo bidireccional LSTM-CNNs-CRF</h3><p>Utilizando una combinación de memoria bidireccional a corto plazo (BLSTM), redes neuronales convolucionales (CNN) y campos aleatorios condicionales (CRF) se pueden extraer del corpus representaciones tanto a nivel de palabra como de carácter. El <a href="https://arxiv.org/pdf/1603.01354.pdf">modelo BLSTM-CNNs-CRF</a> utilizado para construir el asistente de escritura WPS Office funciona de la siguiente manera:</p>
<ol>
<li><strong>CNN:</strong> Las incrustaciones de caracteres se utilizan como entradas para la CNN, después se extraen las estructuras de palabras semánticamente relevantes (es decir, el prefijo o el sufijo) y se codifican en vectores de representación a nivel de caracteres.</li>
<li>BLSTM<strong>:</strong> los vectores de caracteres se concatenan con los vectores de palabras y se introducen en la red BLSTM. Cada secuencia se presenta hacia delante y hacia atrás en dos estados ocultos separados para capturar información pasada y futura.</li>
<li>CRF<strong>:</strong> los vectores de salida de la red BLSTM se introducen en la capa CRF para decodificar conjuntamente la mejor secuencia de etiquetas.</li>
</ol>
<p>La red neuronal es ahora capaz de extraer y clasificar entidades con nombre a partir de texto no estructurado. Este proceso se denomina <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">reconocimiento de entidades con nombre (NER, por</a> sus <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">siglas en inglés)</a> y consiste en localizar y clasificar categorías como nombres de personas, instituciones, ubicaciones geográficas, etcétera. Estas entidades desempeñan un papel importante en la clasificación y recuperación de datos. A partir de aquí pueden extraerse del corpus frases, párrafos y resúmenes clave.</p>
<h3 id="Creating-sentence-embeddings-using-Infersent" class="common-anchor-header">Creación de incrustaciones de frases con Infersent</h3><p><a href="https://github.com/facebookresearch/InferSent">Infersent</a>, un método supervisado de incrustación de frases diseñado por Facebook que incrusta frases completas en un espacio vectorial, se utiliza para crear vectores que se introducirán en la base de datos Milvus. Infersent se entrenó utilizando el corpus Stanford Natural Language Inference (SNLI), que contiene 570.000 pares de frases escritas y etiquetadas por humanos. Puede encontrar más información sobre el funcionamiento de Infersent <a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">aquí</a>.</p>
<h3 id="Storing-and-querying-vectors-with-Milvus" class="common-anchor-header">Almacenamiento y consulta de vectores con Milvus</h3><p><a href="https://www.milvus.io/">Milvus</a> es un motor de búsqueda de similitudes de código abierto que admite la adición, eliminación, actualización y búsqueda casi en tiempo real de incrustaciones a escala de un billón de bytes. Para mejorar el rendimiento de las consultas, Milvus permite especificar un tipo de índice para cada campo vectorial. El asistente inteligente WPS Office utiliza el índice IVF_FLAT, el tipo de índice de archivo invertido (IVF) más básico, donde "plano" significa que los vectores se almacenan sin compresión ni cuantificación. La agrupación se basa en IndexFlat2, que utiliza la búsqueda exacta para la distancia L2.</p>
<p>Aunque IVF_FLAT tiene una tasa de recuperación de consultas del 100%, su falta de compresión se traduce en velocidades de consulta comparativamente lentas. La <a href="https://milvus.io/docs/manage-partitions.md">función de partición</a> de Milvus se utiliza para dividir los datos en varias partes del almacenamiento físico basándose en reglas predefinidas, lo que hace que las consultas sean más rápidas y precisas. Cuando se añaden vectores a Milvus, las etiquetas especifican a qué partición deben añadirse los datos. Las consultas de los datos vectoriales utilizan etiquetas para especificar en qué partición debe ejecutarse la consulta. Los datos pueden dividirse en segmentos dentro de cada partición para mejorar aún más la velocidad.</p>
<p>El asistente de escritura inteligente también utiliza clústeres Kubernetes, lo que permite que los contenedores de aplicaciones se ejecuten en múltiples máquinas y entornos, así como MySQL para la gestión de metadatos.</p>
<h3 id="AI-isn’t-replacing-writers-it’s-helping-them-write" class="common-anchor-header">La IA no sustituye a los escritores, sino que les ayuda a escribir</h3><p>El asistente de escritura de Kingsoft para WPS Office se basa en Milvus para gestionar y consultar una base de datos de más de 2 millones de documentos. El sistema es muy flexible, capaz de ejecutar búsquedas casi en tiempo real en conjuntos de datos a escala de billones. Las consultas se completan en 0,2 segundos de media, lo que significa que se pueden generar documentos enteros casi instantáneamente utilizando sólo un título o unas pocas palabras clave. Aunque la IA no sustituye a los escritores profesionales, la tecnología actual es capaz de aumentar el proceso de escritura de formas novedosas e interesantes. El futuro es desconocido, pero al menos los escritores pueden esperar métodos más productivos, y para algunos menos difíciles, de "poner la pluma sobre el papel".</p>
<p>Para este artículo se han utilizado las siguientes fuentes</p>
<ul>
<li>"<a href="https://arxiv.org/pdf/1603.01354.pdf">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>", Xuezhe Ma y Eduard Hovy.</li>
<li>"<a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">Métodos tradicionales para datos de texto</a>", Dipanjan (DJ) Sarkar.</li>
<li>"<a href="https://ieeexplore.ieee.org/document/8780663">Text Features Extraction based on TF-IDF Associating Semantic</a>," Qing Liu, Jing Wang, Dehai Zhang, Yun Yang, NaiYao Wang.</li>
<li>"<a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">Understanding Sentence Embeddings using Facebook's Infersent</a>," Rehan Ahmad</li>
<li>"<a href="https://arxiv.org/pdf/1705.02364.pdf">Aprendizaje supervisado de representaciones universales de frases a partir de datos de inferencia de lenguaje natural</a>", Alexis Conneau, Douwe Kiela, Holger Schwenk, LoÏc Barrault, Antoine Bordes.V1</li>
</ul>
<p>Lea otras <a href="https://zilliz.com/user-stories">historias de usuarios</a> para saber más sobre cómo hacer cosas con Milvus.</p>
