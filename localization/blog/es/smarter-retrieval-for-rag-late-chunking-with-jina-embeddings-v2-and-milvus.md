---
id: smarter-retrieval-for-rag-late-chunking-with-jina-embeddings-v2-and-milvus.md
title: >-
  Recuperaci√≥n m√°s inteligente para GAR: fragmentaci√≥n tard√≠a con Jina
  Embeddings v2 y Milvus
author: Wei Zang
date: 2025-10-11T00:00:00.000Z
desc: >-
  Aumente la precisi√≥n de la RAG utilizando Late Chunking y Milvus para una
  incrustaci√≥n de documentos eficiente y consciente del contexto y una b√∫squeda
  vectorial m√°s r√°pida e inteligente.
cover: >-
  assets.zilliz.com/Milvus_Meets_Late_Chunking_Smarter_Retrieval_for_RAG_4f9640fffd.png
tag: Tutorials
tags: 'Milvus, Vector Database, Open Source, Vector Embeddings'
recommend: false
meta_keywords: 'Late Chunking, RAG accuracy, vector database, Milvus, document embeddings'
canonicalUrl: >-
  https://milvus.io/blog/smarter-retrieval-for-rag-late-chunking-with-jina-embeddings-v2-and-milvus.md
---
<p>La creaci√≥n de un sistema GAR s√≥lido suele comenzar con la <a href="https://zilliz.com/learn/guide-to-chunking-strategies-for-rag#Chunking"><strong>fragmentaci√≥n de</strong></a> <strong>documentos</strong> <a href="https://zilliz.com/learn/guide-to-chunking-strategies-for-rag#Chunking"><strong>, es decir, la divisi√≥n</strong></a>de textos extensos en fragmentos manejables para su incrustaci√≥n y recuperaci√≥n. Las estrategias m√°s comunes son</p>
<ul>
<li><p>trozos<strong>de tama√±o fijo</strong> (por ejemplo, cada 512 tokens)</p></li>
<li><p><strong>Trozos de tama√±o variable</strong> (por ejemplo, en los l√≠mites de p√°rrafos o frases).</p></li>
<li><p><strong>Ventanas deslizantes</strong> (espacios superpuestos).</p></li>
<li><p><strong>Chunking recursivo</strong> (divisi√≥n jer√°rquica)</p></li>
<li><p><strong>Chunking sem√°ntico</strong> (agrupaci√≥n por temas)</p></li>
</ul>
<p>Aunque estos m√©todos tienen sus ventajas, a menudo rompen el contexto de largo alcance. Para hacer frente a este reto, Jina AI crea un m√©todo de chunking tard√≠o: primero se incrusta todo el documento y luego se esculpen los trozos.</p>
<p>En este art√≠culo, exploraremos c√≥mo funciona Late Chunking y demostraremos c√≥mo su combinaci√≥n con <a href="https://milvus.io/">Milvus (una</a>base de datos vectorial de c√≥digo abierto de alto rendimiento creada para la b√∫squeda de similitudes) puede mejorar dr√°sticamente sus procesos RAG. Tanto si est√° creando bases de conocimientos empresariales, como si est√° creando un servicio de atenci√≥n al cliente basado en IA o aplicaciones de b√∫squeda avanzada, este tutorial le mostrar√° c√≥mo gestionar las incrustaciones de forma m√°s eficaz a gran escala.</p>
<h2 id="What-Is-Late-Chunking" class="common-anchor-header">¬øQu√© es la fragmentaci√≥n tard√≠a?<button data-href="#What-Is-Late-Chunking" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Los m√©todos tradicionales de fragmentaci√≥n pueden romper conexiones importantes cuando la informaci√≥n clave abarca varios fragmentos, lo que da como resultado un rendimiento de recuperaci√≥n deficiente.</p>
<p>Considere estas notas de la versi√≥n 2.4.13 de Milvus, divididas en dos trozos como se muestra a continuaci√≥n:</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Figure1_Chunking_Milvus2_4_13_Release_Note_fe7fbdb833.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Figura 1. Fragmentaci√≥n de las notas de publicaci√≥n de Milvus 2.4.13</em></p>
<p>Si pregunta: "¬øCu√°les son las nuevas caracter√≠sticas de Milvus 2.4.13?", un modelo de incrustaci√≥n est√°ndar puede fallar a la hora de vincular "Milvus 2.4.13" (en el trozo 1) con sus caracter√≠sticas (en el trozo 2). ¬øCu√°l es el resultado? Vectores m√°s d√©biles y menor precisi√≥n de recuperaci√≥n.</p>
<p>Las soluciones heur√≠sticas, como las ventanas deslizantes, los contextos superpuestos y las exploraciones repetidas, proporcionan un alivio parcial, pero no garant√≠as.</p>
<p><strong>El chunking tradicional</strong> sigue este proceso:</p>
<ol>
<li><p><strong>Clasificaci√≥n previa</strong> del texto (por frases, p√°rrafos o longitud m√°xima de los tokens).</p></li>
<li><p><strong>Incrustar</strong> cada fragmento por separado.</p></li>
<li><p><strong>Se agregan</strong> las incrustaciones de tokens (por ejemplo, mediante la agrupaci√≥n de promedios) en un √∫nico vector de trozos.</p></li>
</ol>
<p><strong>La fragmentaci√≥n tard√≠a</strong> invierte el proceso:</p>
<ol>
<li><p><strong>Incrustar primero</strong>: Ejecutar un transformador de contexto largo sobre el documento completo, generando incrustaciones ricas en tokens que capturan el contexto global.</p></li>
<li><p><strong>Trocear despu√©s</strong>: Combine tramos contiguos de esas incrustaciones de tokens para formar los vectores de trozos finales.</p></li>
</ol>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Figure2_Naive_Chunkingvs_Late_Chunking_a94d30b6ea.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><em>Figura 2. Chunking ingenuo frente a chunking tard√≠o</em><em>Naive Chunking frente a Late Chunking (</em><a href="https://jina.ai/news/late-chunking-in-long-context-embedding-models/"><em>Fuente</em></a><em>)</em></p>
<p>Al preservar el contexto completo del documento en cada trozo, el Late Chunking ofrece:</p>
<ul>
<li><p><strong>Mayor precisi√≥n en la recuperaci√≥n: cada</strong>trozo tiene en cuenta el contexto.</p></li>
<li><p><strong>Menos trozos:</strong>se env√≠a un texto m√°s espec√≠fico al LLM, lo que reduce los costes y la latencia.</p></li>
</ul>
<p>Muchos modelos de contexto largo, como jina-embeddings-v2-base-es, pueden procesar hasta 8.192 tokens, lo que equivale a una lectura de 20 minutos (unas 5.000 palabras) y hace que Late Chunking resulte pr√°ctico para la mayor√≠a de los documentos del mundo real.</p>
<p>Ahora que entendemos el "qu√©" y el "por qu√©" del Late Chunking, pasemos al "c√≥mo". En la siguiente secci√≥n, le guiaremos a trav√©s de una implementaci√≥n pr√°ctica del canal de Late Chunking, compararemos su rendimiento con el del chunking tradicional y validaremos su impacto en el mundo real utilizando Milvus. Este recorrido pr√°ctico tender√° un puente entre la teor√≠a y la pr√°ctica, mostrando exactamente c√≥mo integrar Late Chunking en sus flujos de trabajo RAG.</p>
<h2 id="Testing-Late-Chunking" class="common-anchor-header">Pruebas de Late Chunking<button data-href="#Testing-Late-Chunking" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="Basic-Implementation" class="common-anchor-header">Implementaci√≥n b√°sica</h3><p>A continuaci√≥n se presentan las funciones b√°sicas de Late Chunking. Hemos a√±adido docstrings claros para guiarle a trav√©s de cada paso. La funci√≥n <code translate="no">sentence_chunker</code> divide el documento original en trozos basados en p√°rrafos, devolviendo tanto el contenido del trozo como la informaci√≥n de anotaci√≥n del trozo <code translate="no">span_annotations</code> (es decir, los √≠ndices de inicio y fin de cada trozo).</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentence_chunker</span>(<span class="hljs-params">document, batch_size=<span class="hljs-number">10000</span></span>):
    nlp = spacy.blank(<span class="hljs-string">&quot;en&quot;</span>)
    nlp.add_pipe(<span class="hljs-string">&quot;sentencizer&quot;</span>, config={<span class="hljs-string">&quot;punct_chars&quot;</span>: <span class="hljs-literal">None</span>})
    doc = nlp(document)

    docs = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(document), batch_size):
        batch = document[i : i + batch_size]
        docs.append(nlp(batch))

    doc = Doc.from_docs(docs)

    span_annotations = []
    chunks = []
    <span class="hljs-keyword">for</span> i, sent <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(doc.sents):
        span_annotations.append((sent.start, sent.end))
        chunks.append(sent.text)

    <span class="hljs-keyword">return</span> chunks, span_annotations
<button class="copy-code-btn"></button></code></pre>
<p>La funci√≥n <code translate="no">document_to_token_embeddings</code> utiliza el modelo jinaai/jina-embeddings-v2-base-es y su tokenizador para producir incrustaciones de todo el documento.</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">document_to_token_embeddings</span>(<span class="hljs-params">model, tokenizer, document, batch_size=<span class="hljs-number">4096</span></span>):
    tokenized_document = tokenizer(document, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
    tokens = tokenized_document.tokens()

    outputs = []
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(tokens), batch_size):
        
        start = i
        end   = <span class="hljs-built_in">min</span>(i + batch_size, <span class="hljs-built_in">len</span>(tokens))

        batch_inputs = {k: v[:, start:end] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> tokenized_document.items()}

        <span class="hljs-keyword">with</span> torch.no_grad():
            model_output = model(**batch_inputs)

        outputs.append(model_output.last_hidden_state)

    model_output = torch.cat(outputs, dim=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> model_output
<button class="copy-code-btn"></button></code></pre>
<p>La funci√≥n <code translate="no">late_chunking</code> toma las incrustaciones de tokens del documento y la informaci√≥n de anotaci√≥n del chunk original <code translate="no">span_annotations</code>, y luego produce las incrustaciones finales del chunk.</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking</span>(<span class="hljs-params">token_embeddings, span_annotation, max_length=<span class="hljs-literal">None</span></span>):
    outputs = []
    <span class="hljs-keyword">for</span> embeddings, annotations <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(token_embeddings, span_annotation):
        <span class="hljs-keyword">if</span> (
            max_length <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>
        ):
            annotations = [
                (start, <span class="hljs-built_in">min</span>(end, max_length - <span class="hljs-number">1</span>))
                <span class="hljs-keyword">for</span> (start, end) <span class="hljs-keyword">in</span> annotations
                <span class="hljs-keyword">if</span> start &lt; (max_length - <span class="hljs-number">1</span>)
            ]
        pooled_embeddings = []
        <span class="hljs-keyword">for</span> start, end <span class="hljs-keyword">in</span> annotations:
            <span class="hljs-keyword">if</span> (end - start) &gt;= <span class="hljs-number">1</span>:
                pooled_embeddings.append(
                    embeddings[start:end].<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>) / (end - start)
                )
                    
        pooled_embeddings = [
            embedding.detach().cpu().numpy() <span class="hljs-keyword">for</span> embedding <span class="hljs-keyword">in</span> pooled_embeddings
        ]
        outputs.append(pooled_embeddings)

    <span class="hljs-keyword">return</span> outputs
<button class="copy-code-btn"></button></code></pre>
<p>Por ejemplo, chunking con jinaai/jina-embeddings-v2-base-es:</p>
<pre><code translate="no">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;jinaai/jina-embeddings-v2-base-en&#x27;</span>, trust_remote_code=<span class="hljs-literal">True</span>)
model     = AutoModel.from_pretrained(<span class="hljs-string">&#x27;jinaai/jina-embeddings-v2-base-en&#x27;</span>, trust_remote_code=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># First chunk the text as normal, to obtain the beginning and end points of the chunks.</span>
chunks, span_annotations = sentence_chunker(document)
<span class="hljs-comment"># Then embed the full document.</span>
token_embeddings = document_to_token_embeddings(model, tokenizer, document)
<span class="hljs-comment"># Then perform the late chunking</span>
chunk_embeddings = late_chunking(token_embeddings, [span_annotations])[<span class="hljs-number">0</span>]
<button class="copy-code-btn"></button></code></pre>
<p><em>Sugerencia:</em> Envolver el proceso en funciones facilita el intercambio con otros modelos de contexto largo o estrategias de fragmentaci√≥n.</p>
<h3 id="Comparison-with-Traditional-Embedding-Methods" class="common-anchor-header">Comparaci√≥n con los m√©todos de incrustaci√≥n tradicionales</h3><p>Para seguir demostrando las ventajas de Late Chunking, tambi√©n lo comparamos con los m√©todos de incrustaci√≥n tradicionales, utilizando un conjunto de documentos y consultas de ejemplo.</p>
<p>Volvamos a nuestro ejemplo de Milvus 2.4.13 release note:</p>
<pre><code translate="no"><span class="hljs-title class_">Milvus</span> <span class="hljs-number">2.4</span><span class="hljs-number">.13</span> introduces dynamic replica load, allowing users to adjust the number <span class="hljs-keyword">of</span> collection replicas without needing to release and reload the collection. <span class="hljs-title class_">This</span> version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery. <span class="hljs-title class_">Additionally</span>, significant improvements have been made to <span class="hljs-variable constant_">MMAP</span> resource usage and <span class="hljs-keyword">import</span> performance, enhancing overall system efficiency. <span class="hljs-title class_">We</span> highly recommend upgrading to <span class="hljs-variable language_">this</span> release <span class="hljs-keyword">for</span> better performance and stability.
<button class="copy-code-btn"></button></code></pre>
<p>Medimos <a href="https://zilliz.com/blog/similarity-metrics-for-vector-search#Cosine-Similarity">la similitud coseno</a> entre la incrustaci√≥n de la consulta ("milvus 2.4.13") y cada trozo:</p>
<pre><code translate="no">cos_sim = <span class="hljs-keyword">lambda</span> x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))

milvus_embedding = model.encode(<span class="hljs-string">&#x27;milvus 2.4.13&#x27;</span>)

<span class="hljs-keyword">for</span> chunk, late_chunking_embedding, traditional_embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(chunks, chunk_embeddings, embeddings_traditional_chunking):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;similarity_late_chunking(&quot;milvus 2.4.13&quot;, &quot;<span class="hljs-subst">{chunk}</span>&quot;)&#x27;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;late_chunking: &#x27;</span>, cos_sim(milvus_embedding, late_chunking_embedding))
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;similarity_traditional(&quot;milvus 2.4.13&quot;, &quot;<span class="hljs-subst">{chunk}</span>&quot;)&#x27;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;traditional_chunking: &#x27;</span>, cos_sim(milvus_embedding, traditional_embeddings))
<button class="copy-code-btn"></button></code></pre>
<p>El chunking tard√≠o super√≥ sistem√°ticamente al chunking tradicional, obteniendo mayores similitudes coseno en cada trozo. Esto confirma que incrustar primero el documento completo preserva el contexto global de forma m√°s eficaz.</p>
<pre><code translate="no"><span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Milvus 2.4.13 introduces dynamic replica load, allowing users to adjust the number of collection replicas without needing to release and reload the collection.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.8785206</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Milvus 2.4.13 introduces dynamic replica load, allowing users to adjust the number of collection replicas without needing to release and reload the collection.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.8354263</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;This version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.84828955</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;This version also addresses several critical bugs related to bulk importing, expression parsing, load balancing, and failure recovery.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.7222632</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Additionally, significant improvements have been made to MMAP resource usage and import performance, enhancing overall system efficiency.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.84942204</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;Additionally, significant improvements have been made to MMAP resource usage and import performance, enhancing overall system efficiency.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.6907381</span>

<span class="hljs-title function_">similarity_late_chunking</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;We highly recommend upgrading to this release for better performance and stability.&quot;</span>)
<span class="hljs-attr">late_chunking</span>: <span class="hljs-number">0.85431844</span>
<span class="hljs-title function_">similarity_traditional</span>(<span class="hljs-string">&quot;milvus 2.4.13&quot;</span>, <span class="hljs-string">&quot;We highly recommend upgrading to this release for better performance and stability.&quot;</span>)
<span class="hljs-attr">traditional_chunking</span>: <span class="hljs-number">0.71859795</span>
<button class="copy-code-btn"></button></code></pre>
<p>Podemos ver que incrustar primero el p√°rrafo completo garantiza que cada trozo contenga el contexto "<code translate="no">Milvus 2.4.13</code>", lo que aumenta las puntuaciones de similitud y la calidad de la recuperaci√≥n.</p>
<h3 id="Testing-Late-Chunking-in-Milvus" class="common-anchor-header"><strong>Prueba de la fragmentaci√≥n tard√≠a en Milvus</strong></h3><p>Una vez generados los chunk embeddings, podemos almacenarlos en Milvus y realizar consultas. El siguiente c√≥digo inserta vectores de trozos en la colecci√≥n.</p>
<h4 id="Importing-Embeddings-into-Milvus" class="common-anchor-header"><strong>Importaci√≥n de incrustaciones en Milvus</strong></h4><pre><code translate="no">batch_data=[]
<span class="hljs-keyword">for</span> i in <span class="hljs-keyword">range</span>(<span class="hljs-built_in">len</span>(chunks)):
    data = {
            <span class="hljs-string">&quot;content&quot;</span>: chunks[i],
            <span class="hljs-string">&quot;embedding&quot;</span>: chunk_embeddings[i].tolist(),
        }

    batch_data.<span class="hljs-built_in">append</span>(data)

res = client.insert(
    collection_name=collection,
    data=batch_data,
)
<button class="copy-code-btn"></button></code></pre>
<h4 id="Querying-and-Validation" class="common-anchor-header">Consultas y validaci√≥n</h4><p>Para validar la precisi√≥n de las consultas de Milvus, comparamos sus resultados de recuperaci√≥n con las puntuaciones de similitud coseno de fuerza bruta calculadas manualmente. Si ambos m√©todos devuelven resultados top-k coherentes, podemos estar seguros de que la precisi√≥n de la b√∫squeda de Milvus es fiable.</p>
<p>Comparamos la b√∫squeda nativa de Milvus con un an√°lisis de similitud coseno de fuerza bruta:</p>
<pre><code translate="no"><span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking_query_by_milvus</span>(<span class="hljs-params">query, top_k = <span class="hljs-number">3</span></span>):
    query_vector = model(**tokenizer(query, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)).last_hidden_state.mean(<span class="hljs-number">1</span>).detach().cpu().numpy().flatten()

    res = client.search(
                collection_name=collection,
                data=[query_vector.tolist()],
                limit=top_k,
                output_fields=[<span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>],
            )

    <span class="hljs-keyword">return</span> [item.get(<span class="hljs-string">&quot;entity&quot;</span>).get(<span class="hljs-string">&quot;content&quot;</span>) <span class="hljs-keyword">for</span> items <span class="hljs-keyword">in</span> res <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> items]

<span class="hljs-keyword">def</span> <span class="hljs-title function_">late_chunking_query_by_cosine_sim</span>(<span class="hljs-params">query, k = <span class="hljs-number">3</span></span>):
    cos_sim = <span class="hljs-keyword">lambda</span> x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))
    query_vector = model(**tokenizer(query, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)).last_hidden_state.mean(<span class="hljs-number">1</span>).detach().cpu().numpy().flatten()

    results = np.empty(<span class="hljs-built_in">len</span>(chunk_embeddings))
    <span class="hljs-keyword">for</span> i, (chunk, embedding) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-built_in">zip</span>(chunks, chunk_embeddings)):
        results[i] = cos_sim(query_vector, embedding)

    results_order = results.argsort()[::-<span class="hljs-number">1</span>]
    <span class="hljs-keyword">return</span> np.array(chunks)[results_order].tolist()[:k]
<button class="copy-code-btn"></button></code></pre>
<p>Esto confirma que Milvus devuelve los mismos trozos top-k que un escaneo manual de similitud coseno.</p>
<pre><code translate="no">&gt; late_chunking_query_by_milvus(<span class="hljs-string">&quot;What are new features in milvus 2.4.13&quot;</span>, 3)

[<span class="hljs-string">&#x27;\n\n### Features\n\n- Dynamic replica adjustment for loaded collections ([#36417](https://github.com/milvus-io/milvus/pull/36417))\n- Sparse vector MMAP in growing segment types ([#36565](https://github.com/milvus-io/milvus/pull/36565))...
</span><button class="copy-code-btn"></button></code></pre>
<pre><code translate="no">&gt; late_chunking_query_by_cosine_sim(<span class="hljs-string">&quot;What are new features in milvus 2.4.13&quot;</span>, 3)

[<span class="hljs-string">&#x27;\n\n### Features\n\n- Dynamic replica adjustment for loaded collections ([#36417](https://github.com/milvus-io/milvus/pull/36417))\n- Sparse vector MMAP in growing segment types (#36565)...
</span><button class="copy-code-btn"></button></code></pre>
<p>As√≠ pues, ambos m√©todos devuelven los mismos trozos top-3, lo que confirma la precisi√≥n de Milvus.</p>
<h2 id="Conclusion" class="common-anchor-header">Conclusi√≥n<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>En este art√≠culo hemos profundizado en la mec√°nica y las ventajas del Late Chunking. Empezamos identificando las deficiencias de los enfoques tradicionales del chunking, especialmente cuando se manejan documentos largos en los que es crucial preservar el contexto. Introdujimos el concepto de Late Chunking -integrar todo el documento antes de dividirlo en trozos significativos- y demostramos c√≥mo esto preserva el contexto global, lo que mejora la similitud sem√°ntica y la precisi√≥n de la recuperaci√≥n.</p>
<p>A continuaci√≥n, realizamos una implementaci√≥n pr√°ctica con el modelo jina-embeddings-v2-base-en de Jina AI y evaluamos su rendimiento en comparaci√≥n con los m√©todos tradicionales. Por √∫ltimo, demostramos c√≥mo integrar los chunk embeddings en Milvus para una b√∫squeda vectorial escalable y precisa.</p>
<p>Late Chunking ofrece un enfoque de incrustaci√≥n <strong>que da prioridad al</strong> contexto, perfecto para documentos largos y complejos en los que el contexto es lo m√°s importante. Al incrustar todo el texto por adelantado y trocearlo m√°s tarde, usted gana:</p>
<ul>
<li><p><strong>üîç Mayor precisi√≥n en la recuperaci√≥n</strong></p></li>
<li><p>‚ö° A <strong>visos LLM √°giles y centrados</strong></p></li>
<li><p>üõ†Ô∏è <strong>Integraci√≥n sencilla</strong> con cualquier modelo de contexto largo</p></li>
</ul>
