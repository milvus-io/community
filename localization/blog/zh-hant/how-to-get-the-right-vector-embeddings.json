{"codeList":["# Load the embedding model with the last layer removed\nmodel = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True) model = torch.nn.Sequential(*(list(model.children())[:-1]))\nmodel.eval()\n\n\ndef embed(data):\nwith torch.no_grad():\noutput = model(torch.stack(data[0])).squeeze()\nreturn output\n","# Load model directly\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification\n\n\nextractor = AutoFeatureExtractor.from_pretrained(\"microsoft/resnet-50\")\nmodel = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n\n\nfrom PIL import Image\n\n\nimage = Image.open(\"<image path>\")\n# image = Resize(size=(256, 256))(image)\n\n\ninputs = extractor(images=image, return_tensors=\"pt\")\n# print(inputs)\n\n\noutputs = model(**inputs)\nvector_embeddings = outputs[1][-1].squeeze()\n","from sentence_transformers import SentenceTransformer\n\n\nmodel = SentenceTransformer(\"<model-name>\")\nvector_embeddings = model.encode(“<input>”)\n","# Load model directly\nfrom transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n\n\nprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\nmodel = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14\")\nfrom PIL import Image\n\n\nimage = Image.open(\"<image path>\")\n# image = Resize(size=(256, 256))(image)\n\n\ninputs = extractor(images=image, return_tensors=\"pt\")\n# print(inputs)\n\n\noutputs = model(**inputs)\nvector_embeddings = outputs[1][-1].squeeze()\n","import torch\nfrom transformers import AutoFeatureExtractor, WhisperModel\nfrom datasets import load_dataset\n\n\nmodel = WhisperModel.from_pretrained(\"openai/whisper-base\")\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"openai/whisper-base\")\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\ninputs = feature_extractor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\ninput_features = inputs.input_features\ndecoder_input_ids = torch.tensor([[1, 1]]) * model.config.decoder_start_token_id\nvector_embedding = model(input_features, decoder_input_ids=decoder_input_ids).last_hidden_state\n","def autoencode_video(images, audio):\n     # only create entire video once as inputs\n     inputs = {'image': torch.from_numpy(np.moveaxis(images, -1, 2)).float().to(device),\n               'audio': torch.from_numpy(audio).to(device),\n               'label': torch.zeros((images.shape[0], 700)).to(device)}\n     nchunks = 128\n     reconstruction = {}\n     for chunk_idx in tqdm(range(nchunks)):\n          image_chunk_size = np.prod(images.shape[1:-1]) // nchunks\n          audio_chunk_size = audio.shape[1] // SAMPLES_PER_PATCH // nchunks\n          subsampling = {\n               'image': torch.arange(\n                    image_chunk_size * chunk_idx, image_chunk_size * (chunk_idx + 1)),\n               'audio': torch.arange(\n                    audio_chunk_size * chunk_idx, audio_chunk_size * (chunk_idx + 1)),\n               'label': None,\n          }\n     # forward pass\n          with torch.no_grad():\n               outputs = model(inputs=inputs, subsampled_output_points=subsampling)\n\n\n          output = {k:v.cpu() for k,v in outputs.logits.items()}\n          reconstruction['label'] = output['label']\n          if 'image' not in reconstruction:\n               reconstruction['image'] = output['image']\n               reconstruction['audio'] = output['audio']\n          else:\n               reconstruction['image'] = torch.cat(\n                    [reconstruction['image'], output['image']], dim=1)\n               reconstruction['audio'] = torch.cat(\n                    [reconstruction['audio'], output['audio']], dim=1)\n          vector_embeddings = outputs[1][-1].squeeze()\n# finally, reshape image and audio modalities back to original shape\n     reconstruction['image'] = torch.reshape(reconstruction['image'], images.shape)\n     reconstruction['audio'] = torch.reshape(reconstruction['audio'], audio.shape)\n     return reconstruction\n\n\n     return None\n"],"headingContent":"","anchorList":[{"label":"向量內嵌是如何產生的？","href":"How-are-vector-embeddings-created","type":2,"isActive":false},{"label":"向量嵌入是什麼意思？","href":"What-does-a-vector-embedding-mean","type":2,"isActive":false},{"label":"產生正確的向量內嵌","href":"Generate-the-right-vector-embeddings","type":2,"isActive":false},{"label":"使用向量資料庫儲存、索引和搜尋向量內嵌值","href":"Storing-indexing-and-searching-vector-embeddings-with-vector-databases","type":2,"isActive":false},{"label":"摘要","href":"Summary","type":2,"isActive":false}]}