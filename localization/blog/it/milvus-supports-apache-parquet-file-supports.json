{"codeList":["remote_files = []\ntry:\n    print(\"Prepare upload files\")\n    minio_client = Minio(endpoint=MINIO_ADDRESS, access_key=MINIO_ACCESS_KEY, secret_key=MINIO_SECRET_KEY,\n                         secure=False)\n    found = minio_client.bucket_exists(bucket_name)\n    if not found:\n        minio_client.make_bucket(bucket_name)\n        print(\"MinIO bucket '{}' doesn't exist\".format(bucket_name))\n        return False, []\n\n    # set your remote data path\n    remote_data_path = \"milvus_bulkinsert\"\n\n    def upload_file(f: str):\n        file_name = os.path.basename(f)\n        minio_file_path = os.path.join(remote_data_path, \"parquet\", file_name)\n        minio_client.fput_object(bucket_name, minio_file_path, f)\n        print(\"Upload file '{}' to '{}'\".format(f, minio_file_path))\n        remote_files.append(minio_file_path)\n\n    upload_file(data_file)\n\nexcept S3Error as e:\n    print(\"Failed to connect MinIO server {}, error: {}\".format(MINIO_ADDRESS, e))\n    return False, []\n\nprint(\"Successfully upload files: {}\".format(remote_files))\nreturn True, remote_files\n","import numpy as np\nimport json\n\nfrom pymilvus import (\n    RemoteBulkWriter,\n    BulkFileType,\n)\n\nremote_writer = RemoteBulkWriter(\n        schema=your_collection_schema,\n        remote_path=\"your_remote_data_path\",\n        connect_param=RemoteBulkWriter.ConnectParam(\n            endpoint=YOUR_MINIO_ADDRESS,\n            access_key=YOUR_MINIO_ACCESS_KEY,\n            secret_key=YOUR_MINIO_SECRET_KEY,\n            bucket_name=\"a-bucket\",\n        ),\n        file_type=BulkFileType.PARQUET,\n)\n\n# append your data\nbatch_count = 10000\nfor i in range(batch_count):\n    row = {\n        \"id\": i,\n        \"bool\": True if i % 5 == 0 else False,\n        \"int8\": i % 128,\n        \"int16\": i % 1000,\n        \"int32\": i % 100000,\n        \"int64\": i,\n        \"float\": i / 3,\n        \"double\": i / 7,\n        \"varchar\": f\"varchar_{i}\",\n        \"json\": {\"dummy\": i, \"ok\": f\"name_{i}\"},\n        \"vector\": gen_binary_vector() if bin_vec else gen_float_vector(),\n        f\"dynamic_{i}\": i,\n    }\n    remote_writer.append_row(row)\n\n# append rows by numpy type\nfor i in range(batch_count):\n    remote_writer.append_row({\n        \"id\": np.int64(i + batch_count),\n        \"bool\": True if i % 3 == 0 else False,\n        \"int8\": np.int8(i % 128),\n        \"int16\": np.int16(i % 1000),\n        \"int32\": np.int32(i % 100000),\n        \"int64\": np.int64(i),\n        \"float\": np.float32(i / 3),\n        \"double\": np.float64(i / 7),\n        \"varchar\": f\"varchar_{i}\",\n        \"json\": json.dumps({\"dummy\": i, \"ok\": f\"name_{i}\"}),\n        \"vector\": gen_binary_vector() if bin_vec else gen_float_vector(),\n        f\"dynamic_{i}\": i,\n    })\n\nprint(f\"{remote_writer.total_row_count} rows appends\")\nprint(f\"{remote_writer.buffer_row_count} rows in buffer not flushed\")\nprint(\"Generate data files...\")\nremote_writer.commit()\nprint(f\"Data files have been uploaded: {remote_writer.batch_files}\")\nremote_files = remote_writer.batch_files\n","remote_files = [remote_file_path]\ntask_id = utility.do_bulk_insert(collection_name=collection_name,\n                                 files=remote_files)\n\ntask_ids = [task_id]         \nstates = wait_tasks_to_state(task_ids, BulkInsertState.ImportCompleted)\ncomplete_count = 0\nfor state in states:\n    if state.state == BulkInsertState.ImportCompleted:\n        complete_count = complete_count + 1\n"],"headingContent":"","anchorList":[{"label":"Cos'è Apache Parquet?","href":"What-Is-Apache-Parquet","type":2,"isActive":false},{"label":"Come gli utenti di Milvus beneficiano del supporto per le importazioni di file Parquet","href":"How-Milvus-Users-Benefit-from-the-Support-for-Parquet-File-Imports","type":2,"isActive":false},{"label":"Come importare i file Parquet in Milvus","href":"How-to-Import-Parquet-Files-into-Milvus","type":2,"isActive":false},{"label":"Cosa succederà in futuro?","href":"Whats-Next","type":2,"isActive":false}]}