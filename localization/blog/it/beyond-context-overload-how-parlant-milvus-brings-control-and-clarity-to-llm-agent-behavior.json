{"codeList":["await agent.create_guideline(\n    condition=\"The user asks about a refund and the order amount exceeds 500 RMB\",\n    action=\"First call the order status check tool to confirm whether the refund conditions are met, then provide a detailed explanation of the refund process\",\n    tools=[check_order_status, calculate_refund_amount]\n)\n","# The balance inquiry tool is exposed only when the condition \"the user wants to make a transfer\" is met\nawait agent.create_guideline(\n    condition=\"The user wants to make a transfer\",\n    action=\"First check the account balance. If the balance is below 500 RMB, remind the user that an overdraft fee may apply.\",\n    tools=[get_user_account_balance]\n)\n","from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\nimport parlant.sdk as p\n\n# Connect to Milvus\nconnections.connect(host=\"localhost\", port=\"19530\")\n\n# Define the schema for the guideline collection\nfields = [\n    FieldSchema(name=\"guideline_id\", dtype=DataType.VARCHAR, max_length=100, is_primary=True),\n    FieldSchema(name=\"condition_vector\", dtype=DataType.FLOAT_VECTOR, dim=768),\n    FieldSchema(name=\"condition_text\", dtype=DataType.VARCHAR, max_length=1000),\n    FieldSchema(name=\"action_text\", dtype=DataType.VARCHAR, max_length=2000),\n    FieldSchema(name=\"priority\", dtype=DataType.INT64),\n    FieldSchema(name=\"business_domain\", dtype=DataType.VARCHAR, max_length=50)\n]\nschema = CollectionSchema(fields=fields, description=\"Agent Guidelines\")\nguideline_collection = Collection(name=\"agent_guidelines\", schema=schema)\n\n# Create an HNSW index for high-performance retrieval\nindex_params = {\n    \"index_type\": \"HNSW\",\n    \"metric_type\": \"COSINE\",\n    \"params\": {\"M\": 16, \"efConstruction\": 200}\n}\nguideline_collection.create_index(field_name=\"condition_vector\", index_params=index_params)\n","# store user’s past interactions\nuser_memory_fields = [\n    FieldSchema(name=\"interaction_id\", dtype=DataType.VARCHAR, max_length=100, is_primary=True),\n    FieldSchema(name=\"user_id\", dtype=DataType.VARCHAR, max_length=50),\n    FieldSchema(name=\"interaction_vector\", dtype=DataType.FLOAT_VECTOR, dim=768),\n    FieldSchema(name=\"interaction_summary\", dtype=DataType.VARCHAR, max_length=500),\n    FieldSchema(name=\"timestamp\", dtype=DataType.INT64)\n]\nmemory_collection = Collection(name=\"user_memory\", schema=CollectionSchema(user_memory_fields))\n","# Install required Python packages\npip install pymilvus parlant openai\n# Or, if you’re using a Conda environment:\nconda activate your_env_name\npip install pymilvus parlant openai\n","# Set your OpenAI API key\nexport OPENAI_API_KEY=\"your_openai_api_key_here\"\n# Verify that the variable is set correctly\necho $OPENAI_API_KEY\n","class OpenAIEmbedder(p.Embedder):\n    # Converts text into vector embeddings with built-in timeout and retry\n    # Dimension: 1536 (text-embedding-3-small)\n    # Timeout: 60 seconds; Retries: up to 2 times\n","@p.tool\nasync def vector_search(query: str, top_k: int = 5, min_score: float = 0.35):\n    # 1. Convert user query into a vector\n    # 2. Perform similarity search in Milvus\n    # 3. Return results with relevance above threshold\n","# Guideline 1: Run vector search for factual or policy-related questions\nawait agent.create_guideline(\n            condition=\"User asks a factual question about policy, refund, exchange, or shipping\",\n            action=(\n                \"Call vector_search with the user's query. \"\n                \"If evidence is found, synthesize an answer by quoting key sentences and cite doc_id/title. \"\n                \"If evidence is insufficient, ask a clarifying question before answering.\"\n            ),\n            tools=[vector_search],\n\n# Guideline 2: Use a standardized, structured response when evidence is available\nawait agent.create_guideline(\n            condition=\"Evidence is available\",\n            action=(\n                \"Answer with the following template:\\\\n\"\n                \"Summary: provide a concise conclusion.\\\\n\"\n                \"Key points: 2-3 bullets distilled from evidence.\\\\n\"\n                \"Sources: list doc_id and title.\\\\n\"\n                \"Note: if confidence is low, state limitations and ask for clarification.\"\n            ),\n            tools=[],\n        )\n\n    tools=[],\n)\n","import os\nimport asyncio\nimport json\nfrom typing import List, Dict, Any\nimport parlant.sdk as p\nfrom pymilvus import MilvusClient, DataType\n# 1) Environment variables: using OpenAI (as both the default generation model and embedding service)\n# Make sure the OPENAI_API_KEY is set\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise RuntimeError(\"Please set OPENAI_API_KEY environment variable\")\n# 2) Initialize Milvus Lite (runs locally, no standalone service required)\n# MilvusClient runs in Lite mode using a local file path (requires pymilvus >= 2.x)\nclient = MilvusClient(\"./milvus_demo.db\")  # Lite mode uses a local file path\nCOLLECTION = \"kb_articles\"\n# 3) Example data: three policy or FAQ entries (in practice, you can load and chunk data from files)\nDOCS = [\n    {\"doc_id\": \"POLICY-001\", \"title\": \"Refund Policy\", \"chunk\": \"Refunds are available within 30 days of purchase if the product is unused.\"},\n    {\"doc_id\": \"POLICY-002\", \"title\": \"Exchange Policy\", \"chunk\": \"Exchanges are permitted within 15 days; original packaging required.\"},\n    {\"doc_id\": \"FAQ-101\", \"title\": \"Shipping Time\", \"chunk\": \"Standard shipping usually takes 3–5 business days within the country.\"},\n]\n# 4) Generate embeddings using OpenAI (you can replace this with another embedding service)\n# Here we use Parlant’s built-in OpenAI embedder for simplicity, but you could also call the OpenAI SDK directly.\nclass OpenAIEmbedder(p.Embedder):\n    async def embed(self, texts: List[str], hints: Dict[str, Any] = {}) -> p.EmbeddingResult:\n        # Generate text embeddings using the OpenAI API, with timeout and retry handling\n        import openai\n        try:\n            client = openai.AsyncOpenAI(\n                api_key=OPENAI_API_KEY,\n                timeout=60.0,  # 60-second timeout\n                max_retries=2  # Retry up to 2 times\n            )\n            print(f\"Generating embeddings for {len(texts)} texts...\")\n            response = await client.embeddings.create(\n                model=\"text-embedding-3-small\",\n                input=texts\n            )\n            vectors = [data.embedding for data in response.data]\n            print(f\"Successfully generated {len(vectors)} embeddings.\")\n            return p.EmbeddingResult(vectors=vectors)\n        except Exception as e:\n            print(f\"OpenAI API call failed: {e}\")\n            # Return mock vectors for testing Milvus connectivity\n            print(\"Using mock vectors for testing...\")\n            import random\n            vectors = [[random.random() for _ in range(1536)] for _ in texts]\n            return p.EmbeddingResult(vectors=vectors)\n    @property\n    def id(self) -> str:\n        return \"text-embedding-3-small\"\n    @property\n    def max_tokens(self) -> int:\n        return 8192\n    @property\n    def tokenizer(self) -> p.EstimatingTokenizer:\n        from parlant.core.nlp.tokenization import ZeroEstimatingTokenizer\n        return ZeroEstimatingTokenizer()\n    @property\n    def dimensions(self) -> int:\n        return 1536\nembedder = OpenAIEmbedder()\nasync def ensure_collection_and_load():\n    # Create the collection (schema: primary key, vector field, additional fields)\n    if not client.has_collection(COLLECTION):\n        client.create_collection(\n            collection_name=COLLECTION,\n            dimension=len((await embedder.embed([\"dimension_probe\"])).vectors[0]),\n            # Default metric: COSINE (can be changed with metric_type=\"COSINE\")\n            auto_id=True,\n        )\n        # Create an index to speed up retrieval (HNSW used here as an example)\n        client.create_index(\n            collection_name=COLLECTION,\n            field_name=\"vector\",\n            index_type=\"HNSW\",\n            metric_type=\"COSINE\",\n            params={\"M\": 32, \"efConstruction\": 200}\n        )\n    # Insert data (skip if already exists; simple idempotent logic for the demo)\n    # Generate embeddings\n    chunks = [d[\"chunk\"] for d in DOCS]\n    embedding_result = await embedder.embed(chunks)\n    vectors = embedding_result.vectors\n    # Check if the same doc_id already exists; this is for demo purposes only — real applications should use stricter deduplication\n    # Here we insert directly. In production, use an upsert operation or an explicit primary key\n    client.insert(\n        COLLECTION,\n        data=[\n            {\"vector\": vectors[i], \"doc_id\": DOCS[i][\"doc_id\"], \"title\": DOCS[i][\"title\"], \"chunk\": DOCS[i][\"chunk\"]}\n            for i in range(len(DOCS))\n        ],\n    )\n    # Load into memory\n    client.load_collection(COLLECTION)\n# 5) Define the vector search tool (Parlant Tool)\n@p.tool\nasync def vector_search(context: p.ToolContext, query: str, top_k: int = 5, min_score: float = 0.35) -> p.ToolResult:\n    # 5.1 Generate the query vector\n    embed_res = await embedder.embed([query])\n    qvec = embed_res.vectors[0]\n    # 5.2 Search Milvus\n    results = client.search(\n        collection_name=COLLECTION,\n        data=[qvec],\n        limit=top_k,\n        output_fields=[\"doc_id\", \"title\", \"chunk\"],\n        search_params={\"metric_type\": \"COSINE\", \"params\": {\"ef\": 128}},\n    )\n    # 5.3 Assemble structured evidence and filter by score threshold\n    hits = []\n    for hit in results[0]:\n        score = hit[\"distance\"] if \"distance\" in hit else hit.get(\"score\", 0.0)\n        if score >= min_score:\n            hits.append({\n                \"doc_id\": hit[\"entity\"][\"doc_id\"],\n                \"title\": hit[\"entity\"][\"title\"],\n                \"chunk\": hit[\"entity\"][\"chunk\"],\n                \"score\": float(score),\n            })\n    return p.ToolResult({\"evidence\": hits})\n# 6) Run Parlant Server and create the Agent + Guidelines\nasync def main():\n    await ensure_collection_and_load()\n    async with p.Server() as server:\n        agent = await server.create_agent(\n            name=\"Policy Assistant\",\n            description=\"Rule-controlled RAG assistant with Milvus Lite\",\n        )\n        # Example variable: current time (can be used in templates or logs)\n        @p.tool\n        async def get_datetime(context: p.ToolContext) -> p.ToolResult:\n            from datetime import datetime\n            return p.ToolResult({\"now\": datetime.now().isoformat()})\n        await agent.create_variable(name=\"current-datetime\", tool=get_datetime)\n        # Core Guideline 1: Run vector search for factual or policy-related questions\n        await agent.create_guideline(\n            condition=\"User asks a factual question about policy, refund, exchange, or shipping\",\n            action=(\n                \"Call vector_search with the user's query. \"\n                \"If evidence is found, synthesize an answer by quoting key sentences and cite doc_id/title. \"\n                \"If evidence is insufficient, ask a clarifying question before answering.\"\n            ),\n            tools=[vector_search],\n        )\n        # Core Guideline 2: Use a standardized, structured response when evidence is available\n        await agent.create_guideline(\n            condition=\"Evidence is available\",\n            action=(\n                \"Answer with the following template:\\\\n\"\n                \"Summary: provide a concise conclusion.\\\\n\"\n                \"Key points: 2-3 bullets distilled from evidence.\\\\n\"\n                \"Sources: list doc_id and title.\\\\n\"\n                \"Note: if confidence is low, state limitations and ask for clarification.\"\n            ),\n            tools=[],\n        )\n        # Hint: Local Playground URL\n        print(\"Playground: <http://localhost:8800>\")\nif __name__ == \"__main__\":\n    asyncio.run(main())\n","# Run the main program\npython main.py\n","<http://localhost:8800>\n"],"headingContent":"","anchorList":[{"label":"Perché i framework di agenti tradizionali cadono a pezzi","href":"Why-Traditional-Agent-Frameworks-Fall-Apart","type":2,"isActive":false},{"label":"Cos'è Parlant e come funziona","href":"What-is-Parlant-and-How-It-Works","type":2,"isActive":false},{"label":"Come Milvus alimenta Parlant","href":"How-Milvus-Powers-Parlant","type":2,"isActive":false},{"label":"Dimostrazione pratica: Come costruire un sistema intelligente di domande e risposte con Parlant e Milvus Lite","href":"Hands-on-Demo-How-to-Build-a-Smart-QA-System-with-Parlant-and-Milvus-Lite","type":2,"isActive":false},{"label":"Parlant vs. LangChain/LlamaIndex: Come si differenziano e come lavorano insieme","href":"Parlant-vs-LangChainLlamaIndex-How-They-Differ-and-How-They-Work-Together","type":2,"isActive":false},{"label":"Conclusione","href":"Conclusion","type":2,"isActive":false}]}