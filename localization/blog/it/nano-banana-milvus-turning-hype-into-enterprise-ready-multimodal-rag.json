{"codeList":["# Install necessary packages\n%pip install --upgrade pymilvus pillow matplotlib\n%pip install git+https://github.com/openai/CLIP.git\n","import os\nimport clip\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom pymilvus import MilvusClient\nfrom glob import glob\nimport math\n\nprint(\"All libraries imported successfully!\")\n","# Initialize Milvus client\nmilvus_client = MilvusClient(uri=\"http://localhost:19530\",token=\"root:Miluvs\")\nprint(\"Milvus client initialized successfully!\")\n","# Load CLIP model\nmodel_name = \"ViT-B/32\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(model_name, device=device)\nmodel.eval()\n\nprint(f\"CLIP model '{model_name}' loaded successfully, running on device: {device}\")\nprint(f\"Model input resolution: {model.visual.input_resolution}\")\nprint(f\"Context length: {model.context_length}\")\nprint(f\"Vocabulary size: {model.vocab_size}\")\n","CLIP model `ViT-B/32` loaded successfully, running on: cpu\n Model input resolution: 224\n Context length: 77\n Vocabulary size: 49,408\n","def encode_image(image_path):\n    \"\"\"Encode image into normalized feature vector\"\"\"\n    try:\n        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n        \n        with torch.no_grad():\n            image_features = model.encode_image(image)\n            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize\n        \n        return image_features.squeeze().cpu().tolist()\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return None\ndef encode_text(text):\n    \"\"\"Encode text into normalized feature vector\"\"\"\n    text_tokens = clip.tokenize([text]).to(device)\n    \n    with torch.no_grad():\n        text_features = model.encode_text(text_tokens)\n        text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize\n    \n    return text_features.squeeze().cpu().tolist()\n\nprint(\"Feature extraction functions defined successfully!\")\n","collection_name = \"production_image_collection\"\n# If collection already exists, delete it\nif milvus_client.has_collection(collection_name):\n    milvus_client.drop_collection(collection_name)\n    print(f\"Existing collection deleted: {collection_name}\")\n\n# Create new collection\nmilvus_client.create_collection(\n    collection_name=collection_name,\n    dimension=512,  # CLIP ViT-B/32 embedding dimension\n    auto_id=True,  # Auto-generate ID\n    enable_dynamic_field=True,  # Enable dynamic fields\n    metric_type=\"COSINE\"  # Use cosine similarity\n)\n\nprint(f\"Collection '{collection_name}' created successfully!\")\nprint(f\"Collection info: {milvus_client.describe_collection(collection_name)}\")\n","Existing collection deleted: production_image_collection\nCollection 'production_image_collection' created successfully!\nCollection info: {'collection_name': 'production_image_collection', 'auto_id': True, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 512}}, {'field_id': 102, 'name': 'function': [], 'aliases': [], 'collection_id': 460508990706033544, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 460511723827494913, 'updated_timestamp': 460511723827494913}\n","# Set image directory path\nimage_dir = \"./production_image\"\nraw_data = []\n\n# Get all supported image formats\nimage_extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPEG', '*.JPG', '*.PNG']\nimage_paths = []\n\nfor ext in image_extensions:\n    image_paths.extend(glob(os.path.join(image_dir, ext)))\n\nprint(f\"Found {len(image_paths)} images in {image_dir}\")\n\n# Process images and generate embeddings\nsuccessful_count = 0\nfor i, image_path in enumerate(image_paths):\n    print(f\"Processing progress: {i+1}/{len(image_paths)} - {os.path.basename(image_path)}\")\n    \n    image_embedding = encode_image(image_path)\n    if image_embedding is not None:\n        image_dict = {\n            \"vector\": image_embedding,\n            \"filepath\": image_path,\n            \"filename\": os.path.basename(image_path)\n        }\n        raw_data.append(image_dict)\n        successful_count += 1\n\nprint(f\"Successfully processed {successful_count} images\")\n","Found 50 images in ./production_image\nProcessing progress: 1/50 - download (5).jpeg\nProcessing progress: 2/50 - images (2).jpeg\nProcessing progress: 3/50 - download (23).jpeg\nProcessing progress: 4/50 - download.jpeg\nProcessing progress: 5/50 - images (14).jpeg\nProcessing progress: 6/50 - images (16).jpeg\nâ€¦\nProcessing progress: 44/50 - download (10).jpeg\nProcessing progress: 45/50 - images (18).jpeg\nProcessing progress: 46/50 - download (9).jpeg\nProcessing progress: 47/50 - download (12).jpeg\nProcessing progress: 48/50 - images (1).jpeg\nProcessing progress: 49/50 - download.png\nProcessing progress: 50/50 - images.png\nSuccessfully processed 50 images\n","# Insert data into Milvus\nif raw_data:\n    print(\"Inserting data into Milvus...\")\n    insert_result = milvus_client.insert(collection_name=collection_name, data=raw_data)\n    \n    print(f\"Successfully inserted {insert_result['insert_count']} images into Milvus\")\n    print(f\"Sample inserted IDs: {insert_result['ids'][:5]}...\")  # Show first 5 IDs\nelse:\n    print(\"No successfully processed image data to insert\")\n","def search_images_by_text(query_text, top_k=3):\n    \"\"\"Search images based on text query\"\"\"\n    print(f\"Search query: '{query_text}'\")\n    \n    # Encode query text\n    query_embedding = encode_text(query_text)\n    \n    # Search in Milvus\n    search_results = milvus_client.search(\n        collection_name=collection_name,\n        data=[query_embedding],\n        limit=top_k,\n        output_fields=[\"filepath\", \"filename\"]\n    )\n    \n    return search_results[0]\n\n\ndef visualize_search_results(query_text, results):\n    \"\"\"Visualize search results\"\"\"\n    num_images = len(results)\n    \n    if num_images == 0:\n        print(\"No matching images found\")\n        return\n    \n    # Create subplots\n    fig, axes = plt.subplots(1, num_images, figsize=(5*num_images, 5))\n    fig.suptitle(f'Search Results: \"{query_text}\" (Top {num_images})', fontsize=16, fontweight='bold')\n    \n    # Handle single image case\n    if num_images == 1:\n        axes = [axes]\n    \n    # Display images\n    for i, result in enumerate(results):\n        try:\n            img_path = result['entity']['filepath']\n            filename = result['entity']['filename']\n            score = result['distance']\n            \n            # Load and display image\n            img = Image.open(img_path)\n            axes[i].imshow(img)\n            axes[i].set_title(f\"{filename}\\nSimilarity: {score:.3f}\", fontsize=10)\n            axes[i].axis('off')\n            \n            print(f\"{i+1}. File: {filename}, Similarity score: {score:.4f}\")\n            \n        except Exception as e:\n            axes[i].text(0.5, 0.5, f'Error loading image\\n{str(e)}',\n                        ha='center', va='center', transform=axes[i].transAxes)\n            axes[i].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\nprint(\"Search and visualization functions defined successfully!\")\n","# Example search 1\nquery1 = \"a golden watch\"\nresults1 = search_images_by_text(query1, top_k=3)\nvisualize_search_results(query1, results1)\n","Search query: 'a golden watch'\n1. File: images (19).jpeg, Similarity score: 0.2934\n2. File: download (26).jpeg, Similarity score: 0.3073\n3. File: images (17).jpeg, Similarity score: 0.2717\n","%pip install google-generativeai\n%pip install requests\nprint(\"Google Generative AI SDK installation complete!\")\n","import google.generativeai as genai\nfrom PIL import Image\nfrom io import BytesIO\ngenai.configure(api_key=\"<your_api_key>\")\n","prompt = (\n    \"An European male model wearing a suit, carrying a gold watch.\"\n)\n\nimage = Image.open(\"/path/to/image/watch.jpg\")\n\nmodel = genai.GenerativeModel('gemini-2.5-flash-image-preview')\nresponse = model.generate_content([prompt, image])\n\nfor part in response.candidates[0].content.parts:\n    if part.text is not None:\n        print(part.text)\n    elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.save(\"generated_image.png\")\n        image.show()\n"],"headingContent":"","anchorList":[{"label":"Creazione di un motore di recupero da testo a immagine","href":"Building-a-Text-to-Image-Retrieval-Engine","type":2,"isActive":false},{"label":"Utilizzo di Nano-banana per creare immagini promozionali del marchio","href":"Using-Nano-banana-to-Create-Brand-Promotional-Images","type":2,"isActive":false},{"label":"Cosa significa per il vostro flusso di lavoro di sviluppo","href":"What-This-Means-for-Your-Development-Workflow","type":2,"isActive":false},{"label":"Conclusione","href":"Conclusion","type":2,"isActive":false}]}