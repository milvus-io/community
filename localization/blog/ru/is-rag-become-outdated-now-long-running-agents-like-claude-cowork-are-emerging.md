---
id: >-
  is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md
title: >-
  Устаревает ли RAG сейчас, когда появляются такие долгоиграющие агенты, как
  Claude Cowork?
author: Min Yin
date: 2026-1-27
desc: >-
  Глубокий анализ долговременной памяти Клода Коуорка, памяти агентов с
  возможностью записи, компромиссов в RAG и почему векторные базы данных все еще
  имеют значение.
cover: assets.zilliz.com/RAG_vs_Long_Running_Agents_fc67810cf8.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database, claude, RAG'
meta_keywords: >-
  Claude Cowork long-term memory, RAG vs Claude Cowork, vector databases for AI
  agents
meta_title: |
  RAG vs Long-Running Agents: Is RAG Obsolete? 
origin: >-
  https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md
---
<p><a href="https://support.claude.com/en/articles/13345190-getting-started-with-cowork">Claude Cowork</a> - это новая функция агента в приложении Claude Desktop. С точки зрения разработчика, это, по сути, автоматическая программа для выполнения задач, обернутая вокруг модели: она может читать, изменять и генерировать локальные файлы, а также планировать многоэтапные задачи без необходимости вручную запрашивать каждый шаг. Считайте, что это тот же цикл, что и в Claude Code, но выведенный на рабочий стол, а не в терминал.</p>
<p>Ключевая особенность Cowork - способность работать в течение длительного времени без потери состояния. В нем нет обычного таймаута разговора или сброса контекста. Он может продолжать работу, отслеживать промежуточные результаты и повторно использовать предыдущую информацию в разных сессиях. Это создает впечатление "долговременной памяти", хотя механика, лежащая в основе, больше похожа на постоянное состояние задачи + перенос контекста. В любом случае опыт отличается от традиционной модели чата, где все сбрасывается, если вы не создадите свой собственный слой памяти.</p>
<p>Это поднимает два практических вопроса для разработчиков:</p>
<ol>
<li><p><strong>Если модель уже может запоминать прошлую информацию, то куда в таком случае вписывается RAG или агентский RAG? Будет ли RAG заменен?</strong></p></li>
<li><p><strong>Если мы хотим иметь локального агента в стиле Cowork, как нам самим реализовать долговременную память?</strong></p></li>
</ol>
<p>В остальной части статьи мы подробно рассмотрим эти вопросы и объясним, как векторные базы данных вписываются в этот новый ландшафт "модельной памяти".</p>
<h2 id="Claude-Cowork-vs-RAG-What’s-the-Difference" class="common-anchor-header">Claude Cowork vs. RAG: в чем разница?<button data-href="#Claude-Cowork-vs-RAG-What’s-the-Difference" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Как я уже говорил, Claude Cowork - это агентский режим внутри Claude Desktop, который может читать и записывать локальные файлы, разбивать задачи на более мелкие шаги и продолжать работу без потери состояния. Он поддерживает свой собственный рабочий контекст, поэтому многочасовые задачи не сбрасываются, как обычная сессия чата.</p>
<p><strong>RAG</strong> (Retrieval-Augmented Generation) решает другую проблему: предоставляет модели доступ к внешним знаниям. Вы индексируете данные в векторной базе данных, извлекаете соответствующие фрагменты для каждого запроса и передаете их в модель. Эта система широко используется, поскольку она обеспечивает LLM-приложениям форму "долговременной памяти" для документов, журналов, данных о продуктах и т. д.</p>
<p>Если обе системы помогают модели "запоминать", то в чем, собственно, разница?</p>
<h3 id="How-Cowork-Handles-Memory" class="common-anchor-header">Как Cowork работает с памятью</h3><p>Память Cowork работает по принципу "чтение-запись". Агент решает, какая информация из текущей задачи или разговора является актуальной, сохраняет ее в памяти и извлекает позже по мере выполнения задачи. Это позволяет Cowork поддерживать непрерывность длительных рабочих процессов - особенно тех, которые по мере выполнения создают новые промежуточные состояния.</p>
<h3 id="How-RAG-and-Agentic-RAG-Handle-Memory" class="common-anchor-header">Как RAG и Agentic RAG работают с памятью</h3><p>Стандартный RAG - это поиск по запросу: пользователь спрашивает что-то, система находит соответствующие документы, и модель использует их для ответа. Корпус поиска остается стабильным и версифицированным, а разработчики контролируют, что именно в него попадает.</p>
<p>Современный агентный RAG расширяет этот паттерн. Модель может решать, когда получить информацию, что получить и как ее использовать во время планирования или выполнения рабочего процесса. Такие системы могут выполнять длительные задачи и вызывать инструменты, подобно Cowork. Но даже в агентных RAG уровень поиска остается ориентированным на знания, а не на состояние. Агент извлекает авторитетные факты; он не записывает свое изменяющееся состояние задачи обратно в корпус.</p>
<p>Другой способ взглянуть на это:</p>
<ul>
<li><p><strong>Память Cowork ориентирована на задачи:</strong> агент пишет и читает свое собственное развивающееся состояние.</p></li>
<li><p><strong>RAG ориентирована на знания:</strong> система извлекает устоявшуюся информацию, на которую должна опираться модель.</p></li>
</ul>
<h2 id="Reverse-Engineering-Claude-Cowork-How-It-Builds-Long-Running-Agent-Memory" class="common-anchor-header">Реверс-инжиниринг Claude Cowork: Как он строит долговременную память агента<button data-href="#Reverse-Engineering-Claude-Cowork-How-It-Builds-Long-Running-Agent-Memory" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Cowork получил много шумихи, потому что он справляется с многоэтапными задачами, не забывая постоянно о том, что он делал. С точки зрения разработчика, мне интересно <strong>, как он сохраняет состояние в течение таких длительных сессий?</strong> Anthropic не опубликовала внутреннюю информацию, но, основываясь на предыдущих экспериментах разработчиков с модулем памяти Клода, мы можем собрать воедино приличную ментальную модель.</p>
<p>Похоже, что Клод использует гибридную систему: <strong>постоянный слой долговременной памяти и инструменты извлечения информации по требованию.</strong> Вместо того чтобы запихивать полный текст разговора в каждый запрос, Клод выборочно извлекает прошлый контекст только тогда, когда решает, что он уместен. Это позволяет модели поддерживать высокую точность, не расходуя токены каждый раз.</p>
<p>Если разобрать структуру запроса, то она выглядит примерно так:</p>
<pre><code translate="no">[<span class="hljs-meta">0</span>] Static system instructions
[<span class="hljs-meta">1</span>] <span class="hljs-function">User <span class="hljs-title">memory</span> (<span class="hljs-params"><span class="hljs-built_in">long</span>-term</span>)
[2] Retrieved / pruned conversation history
[3] Current user message
</span><button class="copy-code-btn"></button></code></pre>
<p>Интересна не сама структура, а то, как модель решает, что обновлять и когда запускать поиск.</p>
<h3 id="User-Memory-The-Persistent-Layer" class="common-anchor-header">Память пользователя: Постоянный слой</h3><p>Клод хранит долговременную память, которая обновляется с течением времени. И в отличие от более предсказуемой системы памяти ChatGPT, система Клода кажется более "живой". Он хранит воспоминания в блоках, похожих на XML, и обновляет их двумя способами:</p>
<ul>
<li><p><strong>Неявные обновления:</strong> Иногда модель просто решает, что что-то является устойчивым предпочтением или фактом, и тихо записывает это в память. Эти обновления не мгновенны; они появляются через несколько ходов, а старые воспоминания могут исчезнуть, если исчезнет связанный с ними разговор.</p></li>
<li><p><strong>Явные обновления:</strong> Пользователи могут напрямую изменять память с помощью инструмента <code translate="no">memory_user_edits</code> ("запомнить X", "забыть Y"). Эти записи происходят немедленно и больше похожи на операции CRUD.</p></li>
</ul>
<p>Claude выполняет фоновую эвристику, чтобы решить, что стоит сохранить, и не ждет явных инструкций.</p>
<h3 id="Conversation-Retrieval-The-On-Demand-Part" class="common-anchor-header">Поиск разговоров: Часть по требованию</h3><p>Claude <em>не</em> хранит сводку, как многие системы LLM. Вместо этого у него есть набор функций поиска, которые он может вызывать каждый раз, когда считает, что ему не хватает контекста. Эти вызовы не происходят каждый раз - модель запускает их на основе своих внутренних суждений.</p>
<p>Особо выделяется <code translate="no">conversation_search</code>. Когда пользователь говорит что-то неопределенное, например "тот проект в прошлом месяце", Клод часто запускает этот инструмент, чтобы найти соответствующие развороты. Примечательно, что он срабатывает и тогда, когда фраза двусмысленна или написана на другом языке. Это довольно очевидно:</p>
<ul>
<li><p>Какое-то семантическое соответствие (вкрапления).</p></li>
<li><p>Возможно, в сочетании с нормализацией или облегченным переводом</p></li>
<li><p>поиск по ключевым словам для повышения точности.</p></li>
</ul>
<p>В общем, это очень похоже на миниатюрную систему RAG, встроенную в инструментарий модели.</p>
<h3 id="How-Claude’s-Retrieval-Behavior-Differs-From-Basic-History-Buffers" class="common-anchor-header">Чем поведение поиска Клода отличается от базовых буферов истории</h3><p>Из тестирования и журналов можно выделить несколько особенностей:</p>
<ul>
<li><p><strong>Извлечение не является автоматическим.</strong> Модель сама выбирает, когда ее вызывать. Если она считает, что у нее уже достаточно контекста, она даже не будет беспокоиться.</p></li>
<li><p><strong>Извлеченные фрагменты включают</strong> <strong>сообщения</strong> <em>как</em> <strong>пользователя, так и помощника.</strong> Это полезно - сохраняется больше нюансов, чем в резюме только для пользователя.</p></li>
<li><p><strong>Использование токенов остается нормальным.</strong> Поскольку история не пополняется каждый раз, длительные сессии не приводят к непредсказуемым последствиям.</p></li>
</ul>
<p>В целом, это похоже на LLM с расширенным поиском, только поиск происходит в рамках собственного цикла рассуждений модели.</p>
<p>Эта архитектура умна, но не бесплатна:</p>
<ul>
<li><p>Поиск добавляет задержку и больше "движущихся частей" (индексирование, ранжирование, повторное ранжирование).</p></li>
<li><p>Модель иногда неправильно определяет, нужен ли ей контекст, что приводит к классической "забывчивости LLM", даже если данные <em>были</em> доступны.</p></li>
<li><p>Отладка становится сложнее, потому что поведение модели зависит от невидимых триггеров инструментов, а не только от ввода подсказок.</p></li>
</ul>
<h3 id="Claude-Cowork-vs-Claude-Codex-in-handling-long-term-memory" class="common-anchor-header">Клод Коворк против Клода Кодекса в работе с долговременной памятью</h3><p>В отличие от модели Клода, в которой много внимания уделяется извлечению информации, ChatGPT работает с памятью гораздо более структурированным и предсказуемым образом. Вместо того чтобы выполнять семантический поиск или рассматривать старые разговоры как мини-векторное хранилище, ChatGPT вводит память непосредственно в каждую сессию через следующие многоуровневые компоненты:</p>
<ul>
<li><p>Память пользователя</p></li>
<li><p>метаданные сессии</p></li>
<li><p>Сообщения текущей сессии</p></li>
</ul>
<p><strong>Память пользователя</strong></p>
<p>Пользовательская память - это основной слой долговременного хранения данных - часть, которая сохраняется в течение всех сессий и может редактироваться пользователем. В ней хранятся довольно стандартные вещи: имя, биография, текущие проекты, предпочтения в обучении и тому подобное. Каждый новый разговор получает этот блок в самом начале, поэтому модель всегда начинается с последовательного представления о пользователе.</p>
<p>ChatGPT обновляет этот слой двумя способами:</p>
<ul>
<li><p><strong>Явные обновления:</strong> Пользователи могут сказать модели "запомнить это" или "забыть это", и память немедленно изменится. По сути, это CRUD API, который модель раскрывает через естественный язык.</p></li>
<li><p><strong>Неявные обновления:</strong> Если модель обнаружит информацию, которая соответствует правилам OpenAI для долгосрочной памяти, например должность или предпочтения, и пользователь не отключил память, она спокойно добавит ее самостоятельно.</p></li>
</ul>
<p>С точки зрения разработчика, этот слой прост, детерминирован и удобен для рассуждений. Никаких встроенных поисков, никаких эвристик о том, что нужно получить.</p>
<p><strong>Метаданные сессии</strong></p>
<p>Метаданные сеанса находятся на противоположном конце спектра. Они недолговечны, непостоянны и вводятся только один раз в начале сеанса. Считайте, что это переменные среды для общения. Сюда входят такие сведения, как:</p>
<ul>
<li><p>на каком устройстве вы находитесь</p></li>
<li><p>состояние учетной записи/подписки</p></li>
<li><p>примерные модели использования (активные дни, распределение моделей, средняя длина разговора).</p></li>
</ul>
<p>Эти метаданные помогают модели формировать ответы в соответствии с текущей средой - например, писать более короткие ответы на мобильных устройствах, не загрязняя долгосрочную память.</p>
<p><strong>Сообщения текущей сессии</strong></p>
<p>Это стандартная история со скользящим окном: все сообщения в текущем разговоре до тех пор, пока не будет достигнут лимит маркеров. Когда окно становится слишком большим, старые сообщения автоматически удаляются.</p>
<p>Очень важно, что это вытеснение <strong>не</strong> затрагивает память пользователя или межсессионные сводки. Уменьшается только локальная история разговора.</p>
<p>Самое большое расхождение с Клодом проявляется в том, как ChatGPT обрабатывает "недавние, но не текущие" разговоры. Claude вызывает поисковый инструмент для извлечения прошлого контекста, если считает его релевантным. ChatGPT этого не делает.</p>
<p>Вместо этого ChatGPT хранит очень легкую <strong>межсессионную сводку</strong>, которая вставляется в каждый разговор. Несколько ключевых деталей об этом слое:</p>
<ul>
<li><p>Он обобщает <strong>только сообщения пользователей</strong>, но не сообщения помощников.</p></li>
<li><p>Он хранит очень небольшой набор элементов - примерно 15 - достаточно, чтобы отразить устойчивые темы или интересы.</p></li>
<li><p>В нем <strong>нет ни вычислений встраивания, ни ранжирования по сходству, ни вызовов поиска</strong>. По сути, это готовый контекст, а не динамический поиск.</p></li>
</ul>
<p>С инженерной точки зрения такой подход обменивает гибкость на предсказуемость. Нет вероятности странного сбоя при поиске, а задержка вывода остается стабильной, поскольку ничего не подбирается на лету. Недостатком является то, что ChatGPT не будет извлекать случайные сообщения шестимесячной давности, если они не попали в слой сводок.</p>
<h2 id="Challenges-to-Making-Agent-Memory-Writable" class="common-anchor-header">Проблемы, связанные с возможностью записи в память агента<button data-href="#Challenges-to-Making-Agent-Memory-Writable" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Когда агент переходит от <strong>памяти только для чтения</strong> (типичная RAG) к <strong>памяти для записи, где</strong>он может регистрировать действия, решения и предпочтения пользователей, сложность быстро возрастает. Вы больше не просто извлекаете документы; вы поддерживаете растущее состояние, от которого зависит модель.</p>
<p>Система с записываемой памятью должна решить три реальные проблемы:</p>
<ol>
<li><p><strong>Что запоминать:</strong> Агенту нужны правила для принятия решений о том, какие события, предпочтения или наблюдения стоит сохранить. Без этого память либо увеличивается в размерах, либо заполняется шумом.</p></li>
<li><p><strong>Как хранить и распределять память:</strong> Не вся память одинакова. Недавние события, долгосрочные факты и эфемерные заметки нуждаются в разных уровнях хранения, политиках сохранения и стратегиях индексирования.</p></li>
<li><p><strong>Как писать быстро, не нарушая процесса извлечения:</strong> Память должна записываться постоянно, но частые обновления могут ухудшить качество индекса или замедлить запросы, если система не рассчитана на высокопроизводительные вставки.</p></li>
</ol>
<h3 id="Challenge-1-What-Is-Worth-Remembering" class="common-anchor-header">Задача 1: Что стоит запоминать?</h3><p>Не все, что делает пользователь, должно оставаться в долговременной памяти. Если кто-то создает временный файл, а через пять минут удаляет его, запись об этом навсегда никому не поможет. В этом и заключается основная трудность: <strong>как система решает, что действительно важно?</strong></p>
<p><strong>(1) Распространенные способы определения важности</strong></p>
<p>Команды обычно полагаются на сочетание эвристик:</p>
<ul>
<li><p><strong>Основанные на времени</strong>: недавние действия имеют большее значение, чем старые</p></li>
<li><p><strong>На основе частоты</strong>: файлы или действия, к которым обращаются неоднократно, более важны</p></li>
<li><p><strong>На основе типа</strong>: некоторые объекты по своей сути более важны (например, файлы конфигурации проекта по сравнению с файлами кэша).</p></li>
</ul>
<p><strong>(2) Когда правила противоречат друг другу</strong></p>
<p>Эти сигналы часто противоречат друг другу. Файл, созданный на прошлой неделе, но сильно отредактированный сегодня, - что должно победить: возраст или активность? Единого "правильного" ответа не существует, поэтому оценка важности быстро становится беспорядочной.</p>
<p><strong>(3) Как помогают векторные базы данных</strong></p>
<p>Векторные базы данных дают вам механизмы для обеспечения соблюдения правил важности без ручной очистки:</p>
<ul>
<li><p><strong>TTL:</strong> Milvus может автоматически удалять данные по истечении заданного времени.</p></li>
<li><p><strong>Распад:</strong> старые векторы могут быть понижены в весе, так что они естественным образом исчезают из поиска.</p></li>
</ul>
<h3 id="Challenge-2-Memory-Tiering-in-Practice" class="common-anchor-header">Задача 2: многоуровневое распределение памяти на практике</h3><p>По мере того как агенты работают дольше, память накапливается. Хранить все в быстром хранилище невозможно, поэтому системе нужен способ разделить память на <strong>"горячий"</strong> (часто используемый) и <strong>"холодный"</strong> (редко используемый) уровни.</p>
<p><strong>(1) Решение о том, когда память становится холодной</strong></p>
<p>В этой модели <em>горячая память</em> относится к данным, хранящимся в оперативной памяти для доступа с малым временем ожидания, а <em>холодная память</em> - к данным, перемещенным на диск или в объектное хранилище для снижения стоимости.</p>
<p>Решение о том, когда память становится холодной, может приниматься по-разному. Некоторые системы используют облегченные модели для оценки семантической важности действия или файла на основе его значения и недавнего использования. Другие полагаются на простую логику, основанную на правилах, например перемещение памяти, к которой не обращались в течение 30 дней или которая не появлялась в результатах поиска в течение недели. Пользователи также могут явно отмечать определенные файлы или действия как важные, чтобы они всегда оставались "горячими".</p>
<p><strong>(2) Где хранятся "горячие" и "холодные" воспоминания</strong></p>
<p>После классификации горячая и холодная память хранятся по-разному. Горячая память остается в оперативной памяти и используется для часто используемого содержимого, например контекста активной задачи или последних действий пользователя. Холодная память перемещается на диск или в системы хранения объектов, такие как S3, где доступ к ней медленнее, но стоимость хранения гораздо ниже. Такой компромисс хорошо работает, поскольку холодная память нужна редко и к ней обычно обращаются только для долгосрочного хранения.</p>
<p><strong>(3) Как помогают векторные базы данных</strong></p>
<p><strong>Milvus и Zilliz Cloud</strong> поддерживают эту схему, обеспечивая многоуровневое хранение данных в режиме "горячий-холодный" при сохранении единого интерфейса запросов, поэтому часто используемые векторы остаются в памяти, а старые данные автоматически перемещаются в более дешевое хранилище.</p>
<h3 id="Challenge-3-How-Fast-Should-Memory-Be-Written" class="common-anchor-header">Задача 3: Как быстро должна записываться память?</h3><p>Традиционные системы RAG обычно записывают данные партиями. Индексы перестраиваются в автономном режиме - часто за одну ночь - и становятся доступными для поиска только позже. Такой подход подходит для статичных баз знаний, но не подходит для памяти агентов.</p>
<p><strong>(1) Почему память агента нуждается в записях в реальном времени</strong></p>
<p>Память агента должна фиксировать действия пользователя в момент их совершения. Если действие не будет записано немедленно, то в следующем разговоре может отсутствовать критический контекст. По этой причине системы памяти с возможностью записи требуют записи в реальном времени, а не отложенных, автономных обновлений.</p>
<p><strong>(2) Противоречие между скоростью записи и качеством извлечения информации</strong></p>
<p>Память реального времени требует очень низкой задержки записи. В то же время высокое качество поиска зависит от хорошо построенных индексов, а построение индекса требует времени. Перестраивать индекс при каждой записи слишком дорого, но задержка индексации означает, что вновь записанные данные остаются временно невидимыми для поиска. Этот компромисс находится в центре проектирования памяти с возможностью записи.</p>
<p><strong>(3) Как помогают векторные базы данных</strong></p>
<p>Векторные базы данных решают эту проблему, отделяя запись от индексирования. Общим решением является потоковая запись и инкрементное построение индексов. В примере с <strong>Milvus</strong> новые данные сначала записываются в буфер в памяти, что позволяет системе эффективно обрабатывать высокочастотные записи. Еще до создания полного индекса буферизованные данные могут быть запрошены в течение нескольких секунд с помощью динамического слияния или приблизительного поиска.</p>
<p>Когда буфер достигает заданного порога, система строит индексы партиями и сохраняет их. Это повышает производительность долгосрочного поиска, не блокируя запись в реальном времени. Разделяя быструю запись и медленное построение индексов, Milvus достигает практического баланса между скоростью записи и качеством поиска, который хорошо подходит для агентской памяти.</p>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Cowork дает нам представление о новом классе агентов - персистентных, с состоянием и способных переносить контекст на большие временные отрезки. Но он также дает понять и другое: долговременная память - это только половина картины. Для создания автономных и надежных агентов, готовых к производству, нам все еще нужен структурированный поиск по большим, развивающимся базам знаний.</p>
<p>RAG работает с фактами мира; записываемая память - с внутренним состоянием агента. Векторные базы данных находятся на пересечении, обеспечивая индексацию, гибридный поиск и масштабируемое хранение, которые позволяют обоим слоям работать вместе.</p>
<p>По мере развития агентов, работающих в течение длительного времени, их архитектуры, скорее всего, будут сходиться к этой гибридной конструкции. Cowork - это сильный сигнал того, куда все движется - не к миру без RAG, а к агентам с более богатыми стеками памяти, подкрепленными векторными базами данных.</p>
<p>Если вы хотите изучить эти идеи или получить помощь в создании собственной системы, <strong>присоединяйтесь к нашему</strong> <a href="https://milvusio.slack.com/join/shared_invite/zt-3nntzngkz-gYwhrdSE4~76k0VMyBfD1Q#/shared-invite/email">Slack-каналу</a>, чтобы пообщаться с инженерами Milvus. А для более практического руководства вы всегда можете <strong>заказать</strong> <strong>сеанс</strong> <a href="https://milvus.io/blog/join-milvus-office-hours-to-get-support-from-vectordb-experts.md"><strong>Milvus Office Hours</strong></a> <strong>.</strong></p>
