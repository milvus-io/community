---
id: llm-context-pruning-a-developers-guide-to-better-rag-and-agentic-ai-results.md
title: >-
  LLM Context Pruning: Руководство разработчика для улучшения результатов RAG и
  агентного ИИ
author: Cheney Zhang
date: 2026-01-15T00:00:00.000Z
cover: assets.zilliz.com/context_pruning_cover_d1b034ba67.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'Context Pruning, RAG, long context LLMs, context engineering'
meta_title: |
  LLM Context Pruning: Improving RAG and Agentic AI Systems
desc: >-
  Узнайте, как работает обрезка контекста в системах RAG с длинным контекстом,
  почему это важно, и как модели типа Provence обеспечивают семантическую
  фильтрацию и работают на практике.
origin: >-
  https://milvus.io/blog/llm-context-pruning-a-developers-guide-to-better-rag-and-agentic-ai-results.md
---
<p>Контекстные окна в LLM в последнее время стали огромными. Некоторые модели могут принимать миллион лексем или больше за один проход, и каждый новый релиз, кажется, увеличивает это число. Это интересно, но если вы действительно создали что-то, использующее длинный контекст, вы знаете, что существует разрыв между тем, что <em>возможно,</em> и тем, что <em>полезно</em>.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/LLM_Leaderboard_7c64e4a18c.PNG" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Если модель <em>может</em> прочитать целую книгу за одну подсказку, это не значит, что вы должны ей ее давать. Большинство длинных запросов полны ненужного модели материала. Как только вы начинаете сбрасывать в запрос сотни тысяч лексем, вы обычно получаете более медленные ответы, более высокие затраты на вычисления, а иногда и менее качественные ответы, потому что модель пытается уделить внимание всему сразу.</p>
<p>Поэтому, несмотря на то, что контекстные окна становятся все больше и больше, главный вопрос заключается в том, <strong>что мы должны туда поместить?</strong> Вот тут-то и приходит на помощь <strong>обрезка контекста</strong>. По сути, это процесс обрезки тех частей полученного или собранного контекста, которые не помогают модели ответить на вопрос. Если все сделать правильно, система будет работать быстро, стабильно и гораздо более предсказуемо.</p>
<p>В этой статье мы поговорим о том, почему длинный контекст часто ведет себя не так, как вы ожидаете, как обрезка помогает держать ситуацию под контролем и как инструменты обрезки, такие как <strong>Provence</strong>, вписываются в реальные RAG-конвейеры, не усложняя вашу настройку.</p>
<h2 id="Four-Common-Failure-Modes-in-Long-Context-Systems" class="common-anchor-header">Четыре распространенных способа отказа в системах с длинным контекстом<button data-href="#Four-Common-Failure-Modes-in-Long-Context-Systems" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Увеличение контекстного окна не делает модель волшебным образом умнее. Как только вы начинаете запихивать в него тонны информации, вы открываете целый ряд новых возможностей, которые могут пойти не так. Вот четыре проблемы, с которыми вы постоянно сталкиваетесь при построении систем с длинным контекстом или RAG.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Four_Failure_Modes_e9b9bcb3b2.PNG" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="1-Context-Clash" class="common-anchor-header">1. Столкновение контекстов</h3><p>Столкновение контекстов происходит, когда информация, накопленная за несколько оборотов, становится внутренне противоречивой.</p>
<p>Например, пользователь может сказать: "Я люблю яблоки" в начале разговора, а позже заявить: "Я не люблю фрукты". Когда оба утверждения остаются в контексте, у модели нет надежного способа разрешить конфликт, что приводит к непоследовательным или нерешительным ответам.</p>
<h3 id="2-Context-Confusion" class="common-anchor-header">2. Контекстная путаница</h3><p>Путаница в контексте возникает, когда контекст содержит большое количество нерелевантной или слабо связанной информации, что затрудняет выбор правильного действия или инструмента для модели.</p>
<p>Эта проблема особенно заметна в системах, дополненных инструментами. Когда контекст загроможден не относящимися к делу деталями, модель может неверно истолковать намерения пользователя и выбрать неправильный инструмент или действие - не потому, что нужный вариант отсутствует, а потому, что сигнал погребен под шумом.</p>
<h3 id="3-Context-Distraction" class="common-anchor-header">3. Отвлечение контекста</h3><p>Отвлечение контекста происходит, когда избыточная контекстная информация доминирует над вниманием модели, снижая ее зависимость от предварительно выученных знаний и общих рассуждений.</p>
<p>Вместо того чтобы полагаться на широко изученные закономерности, модель придает чрезмерное значение недавним деталям в контексте, даже если они неполны или ненадежны. Это может привести к неглубоким или хрупким рассуждениям, которые слишком близко отражают контекст, а не применяют понимание более высокого уровня.</p>
<h3 id="4-Context-Poisoning" class="common-anchor-header">4. Отравление контекста</h3><p>Отравление контекста происходит, когда неверная информация попадает в контекст и неоднократно упоминается и подкрепляется в течение нескольких поворотов.</p>
<p>Одно неверное утверждение, представленное в начале разговора, может стать основой для последующих рассуждений. По мере продолжения диалога модель опирается на это ошибочное предположение, усугубляя ошибку и все больше отдаляясь от правильного ответа.</p>
<h2 id="What-Is-Context-Pruning-and-Why-It-Matters" class="common-anchor-header">Что такое обрезка контекста и почему она важна<button data-href="#What-Is-Context-Pruning-and-Why-It-Matters" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Как только вы начинаете работать с длинными контекстами, вы быстро понимаете, что вам нужно больше, чем один трюк, чтобы держать все под контролем. В реальных системах команды обычно сочетают несколько тактик - RAG, загрузку инструментов, подведение итогов, помещение определенных сообщений в карантин, выгрузку старой истории и так далее. Все они помогают по-разному. Но <strong>Context Pruning</strong> - это та тактика, которая непосредственно решает <em>, что именно будет передано</em> в модель.</p>
<p>Контекстная обрезка, говоря простым языком, - это процесс автоматического удаления нерелевантной, малозначимой или противоречивой информации до того, как она попадет в контекстное окно модели. По сути, это фильтр, который сохраняет только те фрагменты текста, которые, скорее всего, имеют значение для текущей задачи.</p>
<p>Другие стратегии могут реорганизовать контекст, сжать его или отложить некоторые части на потом. Обрезка более прямолинейна: <strong>она отвечает на вопрос: "Должна ли эта часть информации вообще попасть в подсказку?".</strong></p>
<p>Вот почему обрезка оказывается особенно важной в системах RAG. Векторный поиск - это здорово, но он не идеален. Он часто возвращает большой мешок кандидатов - некоторые полезные, некоторые слабо связанные, некоторые совершенно не относящиеся к делу. Если вы просто вывалите их все в подсказку, вы столкнетесь с теми режимами отказа, о которых мы говорили ранее. Обрезка занимает место между поиском и моделью, выступая в роли привратника, который решает, какие фрагменты оставить.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/RAG_Pipeline_with_Context_Pruning_01a0d40819.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Когда обрезка работает хорошо, преимущества проявляются сразу: чистый контекст, более последовательные ответы, меньшее использование маркеров и меньшее количество странных побочных эффектов от попадания нерелевантного текста. Даже если вы ничего не меняете в настройках поиска, добавление надежного шага обрезки может заметно повысить общую производительность системы.</p>
<p>На практике обрезка является одной из самых эффективных оптимизаций в длинноконтекстном или RAG-конвейере - простая идея, большое влияние.</p>
<h2 id="Provence-A-Practical-Context-Pruning-Model" class="common-anchor-header">Provence: Практическая модель обрезки контекста<button data-href="#Provence-A-Practical-Context-Pruning-Model" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Изучая подходы к обрезке контекста, я наткнулся на две привлекательные модели с открытым исходным кодом, разработанные в <strong>Naver Labs Europe</strong>: <a href="https://huggingface.co/naver/provence-reranker-debertav3-v1"><strong>Provence</strong></a> и ее многоязычный вариант, <a href="https://huggingface.co/naver/xprovence-reranker-bgem3-v1"><strong>XProvence</strong></a>.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/provence1_b9d2c43276.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Provence - это метод обучения облегченной модели обрезки контекста для генерации с расширенным поиском, с особым акцентом на ответы на вопросы. Учитывая вопрос пользователя и найденный отрывок, она определяет и удаляет нерелевантные предложения, оставляя только ту информацию, которая вносит вклад в окончательный ответ.</p>
<p>Обрезая малозначимый контент перед генерацией, Provence уменьшает шум на входе модели, сокращает количество подсказок и уменьшает время ожидания вывода LLM. Кроме того, Provence является plug-and-play и работает с любой системой LLM или поисковой системой, не требуя тесной интеграции или архитектурных изменений.</p>
<p>Provence предлагает несколько практических возможностей для реальных конвейеров RAG.</p>
<p><strong>1. Понимание на уровне документов</strong></p>
<p>Provence рассматривает документы в целом, а не оценивает предложения по отдельности. Это важно, поскольку реальные документы часто содержат такие ссылки, как "это", "это" или "метод, описанный выше". В отдельности эти предложения могут быть неоднозначными или даже бессмысленными. Если рассматривать их в контексте, их значение становится очевидным. Благодаря целостному моделированию документа Provence принимает более точные и последовательные решения по обрезке.</p>
<p><strong>2. Адаптивный отбор предложений</strong></p>
<p>Provence автоматически определяет, сколько предложений следует сохранить в полученном документе. Вместо того чтобы полагаться на фиксированные правила вроде "сохранить пять лучших предложений", он адаптируется к запросу и содержанию.</p>
<p>На некоторые вопросы можно ответить одним предложением, в то время как для других требуется несколько вспомогательных утверждений. Provence динамически обрабатывает эту вариацию, используя порог релевантности, который хорошо работает в разных доменах и может быть скорректирован при необходимости - в большинстве случаев без ручной настройки.</p>
<p><strong>3. Высокая эффективность благодаря интегрированному рерайтингу</strong></p>
<p>Provence создан для того, чтобы быть эффективным. Это компактная, легкая модель, что делает ее значительно быстрее и дешевле в исполнении, чем подходы к обрезке на основе LLM.</p>
<p>Что еще более важно, Provence может объединить реранкинг и контекстную обрезку в один шаг. Поскольку рерайтинг уже является стандартным этапом в современных конвейерах RAG, интеграция обрезки на этом этапе делает дополнительные затраты на обрезку контекста близкими к нулю, при этом улучшая качество контекста, передаваемого в языковую модель.</p>
<p><strong>4. Многоязычная поддержка с помощью XProvence</strong></p>
<p>У Provence также есть вариант под названием XProvence, который использует ту же архитектуру, но обучается на многоязычных данных. Это позволяет ему оценивать запросы и документы на разных языках, таких как китайский, английский и корейский, что делает его подходящим для многоязычных и межъязыковых систем RAG.</p>
<h3 id="How-Provence-Is-Trained" class="common-anchor-header">Как происходит обучение Provence</h3><p>Provence использует чистый и эффективный дизайн обучения, основанный на архитектуре кросс-кодирования. Во время обучения запрос и каждый найденный отрывок объединяются в один вход и кодируются вместе. Это позволяет модели видеть полный контекст вопроса и отрывка одновременно и напрямую рассуждать об их релевантности.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/provence2_80523f7a9e.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Такое совместное кодирование позволяет Provence обучаться на основе тонких сигналов релевантности. Модель отлажена на <a href="https://zilliz.com/ai-faq/what-is-the-difference-between-bert-roberta-and-deberta-for-embeddings"><strong>DeBERTa</strong></a> как легком кодере и оптимизирована для выполнения двух задач одновременно:</p>
<ol>
<li><p><strong>Оценка релевантности на уровне документа (rerank score):</strong> Модель предсказывает оценку релевантности для всего документа, показывая, насколько хорошо он соответствует запросу. Например, оценка 0,8 означает высокую релевантность.</p></li>
<li><p><strong>Маркировка релевантности на уровне токенов (бинарная маска):</strong> Параллельно модель присваивает двоичную метку каждой лексеме, отмечая, релевантна (<code translate="no">1</code>) или нерелевантна (<code translate="no">0</code>) она запросу.</p></li>
</ol>
<p>В результате обученная модель может оценить общую релевантность документа и определить, какие его части следует сохранить или удалить.</p>
<p>Во время вывода Provence предсказывает метки релевантности на уровне лексем. Затем эти прогнозы агрегируются на уровне предложения: предложение сохраняется, если оно содержит больше релевантных лексем, чем нерелевантных; в противном случае оно отсекается. Поскольку модель обучается с контролем на уровне предложения, предсказания лексем в пределах одного предложения, как правило, совпадают, что делает эту стратегию агрегирования надежной на практике. Поведение обрезки также может быть настроено путем изменения порога агрегирования для достижения более консервативной или более агрессивной обрезки.</p>
<p>Очень важно, что Provence использует шаг реранжирования, который уже есть в большинстве конвейеров RAG. Это означает, что контекстная обрезка может быть добавлена практически без дополнительных накладных расходов, что делает Provence особенно практичным для реальных RAG-систем.</p>
<h2 id="Evaluating-Context-Pruning-Performance-Across-Models" class="common-anchor-header">Оценка эффективности контекстной обрезки в разных моделях<button data-href="#Evaluating-Context-Pruning-Performance-Across-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>До сих пор мы сосредоточились на разработке и обучении Provence. Следующий шаг - оценить, как он работает на практике: насколько хорошо он обрезает контекст, как он сравнивается с другими подходами и как он ведет себя в реальных условиях.</p>
<p>Чтобы ответить на эти вопросы, мы разработали ряд количественных экспериментов для сравнения качества обрезки контекста в нескольких моделях в реальных условиях оценки.</p>
<p>Эксперименты направлены на достижение двух основных целей:</p>
<ul>
<li><p><strong>Эффективность обрезки:</strong> Мы измеряем, насколько точно каждая модель сохраняет релевантный контент, удаляя нерелевантную информацию, используя стандартные метрики, такие как Precision, Recall и F1 score.</p></li>
<li><p><strong>Обобщение вне домена:</strong> Мы оцениваем, насколько хорошо каждая модель работает с распределениями данных, которые отличаются от обучающих данных, оценивая устойчивость в сценариях вне области.</p></li>
</ul>
<h3 id="Models-Compared" class="common-anchor-header">Сравниваемые модели</h3><ul>
<li><p><a href="https://huggingface.co/naver/provence-reranker-debertav3-v1"><strong>Прованс</strong></a></p></li>
<li><p><a href="https://huggingface.co/naver/xprovence-reranker-bgem3-v1"><strong>XProvence</strong></a></p></li>
<li><p><a href="https://huggingface.co/opensearch-project/opensearch-semantic-highlighter-v1"><strong>OpenSearch Semantic Highlighter</strong></a> (модель обрезки, основанная на архитектуре BERT, разработанная специально для задач семантического выделения)</p></li>
</ul>
<h3 id="Dataset" class="common-anchor-header">Набор данных</h3><p>В качестве оценочного набора данных мы используем WikiText-2. WikiText-2 получен из статей Википедии и содержит разнообразные структуры документов, в которых релевантная информация часто распределена по нескольким предложениям, а семантические связи могут быть нетривиальными.</p>
<p>Важно отметить, что WikiText-2 существенно отличается от данных, обычно используемых для обучения моделей обрезки контекста, но при этом напоминает реальный мир, содержащий большой объем знаний. Это делает его хорошо подходящим для оценки вне домена, что является основным направлением наших экспериментов.</p>
<h3 id="Query-Generation-and-Annotation" class="common-anchor-header">Генерация запросов и аннотация</h3><p>Чтобы построить задачу обрезки вне домена, мы автоматически генерируем пары "вопрос-ответ" из исходного корпуса WikiText-2 с помощью <strong>GPT-4o-mini</strong>. Каждая оценочная выборка состоит из трех компонентов:</p>
<ul>
<li><p><strong>Запрос:</strong> Вопрос на естественном языке, сгенерированный из документа.</p></li>
<li><p><strong>Контекст:</strong> Полный, немодифицированный документ.</p></li>
<li><p><strong>Истина:</strong> аннотации на уровне предложений, указывающие, какие предложения содержат ответ (должны быть сохранены), а какие не имеют отношения к делу (должны быть обрезаны).</p></li>
</ul>
<p>Такая схема естественным образом определяет задачу обрезки контекста: при наличии запроса и полного документа модель должна определить предложения, которые действительно имеют значение. Предложения, содержащие ответ, помечаются как релевантные и должны быть сохранены, в то время как все остальные предложения считаются нерелевантными и должны быть обрезаны. Такая формулировка позволяет количественно оценить качество обрезки с помощью показателей Precision, Recall и F1.</p>
<p>Важно отметить, что сгенерированные вопросы не фигурируют в обучающих данных ни одной из оцениваемых моделей. Таким образом, производительность отражает истинное обобщение, а не запоминание. В общей сложности мы сгенерировали 300 примеров, включающих простые вопросы, основанные на фактах, многоходовые задачи рассуждения и более сложные аналитические подсказки, чтобы лучше отразить реальные модели использования.</p>
<h3 id="Evaluation-Pipeline" class="common-anchor-header">Конвейер оценки</h3><p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/pipeline_77e52002fc.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Оптимизация гиперпараметров: Для каждой модели мы выполняем сеточный поиск в заданном пространстве гиперпараметров и выбираем конфигурацию, которая максимизирует результат F1.</p>
<h3 id="Results-and-Analysis" class="common-anchor-header">Результаты и анализ</h3><p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/result_0df098152a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Результаты показывают явные различия в производительности трех моделей.</p>
<p><strong>Provence</strong> достигла самой высокой общей производительности, получив <strong>оценку F1 66,76 %</strong>. Показатели Precision<strong>(69,53 %</strong>) и Recall<strong>(64,19 %</strong>) хорошо сбалансированы, что свидетельствует о надежном обобщении за пределами домена. В оптимальной конфигурации используется порог обрезки <strong>0,6</strong> и α <strong>= 0,051</strong>, что говорит о том, что оценки релевантности модели хорошо откалиброваны, а поведение обрезки интуитивно понятно и легко настраивается на практике.</p>
<p><strong>XProvence</strong> достигает <strong>F1 58,97 %</strong>, характеризуясь <strong>высоким показателем recall (75,52 %)</strong> и <strong>низким показателем precision (48,37 %)</strong>. Это отражает более консервативную стратегию обрезки, в которой приоритет отдается сохранению потенциально релевантной информации, а не агрессивному удалению шума. Такое поведение может быть желательным в областях, где ложноотрицательные результаты дорого обходятся - например, в здравоохранении или юридических приложениях, - но оно также увеличивает количество ложноположительных результатов, что снижает точность. Несмотря на этот компромисс, многоязыковые возможности XProvence делают его сильным вариантом для неанглийских или межъязыковых приложений.</p>
<p>В отличие от него, <strong>OpenSearch Semantic Highlighter</strong> работает значительно хуже, его <strong>результат F1 составляет 46,37 %</strong> (Precision <strong>62,35 %</strong>, Recall <strong>36,98 %</strong>). Отставание от Provence и XProvence указывает на недостатки как в калибровке оценок, так и в обобщении, особенно в условиях внедоменного использования.</p>
<h2 id="Semantic-Highlighting-Another-Way-to-Find-What-Actually-Matters-in-Text" class="common-anchor-header">Семантическое выделение: Еще один способ найти в тексте то, что действительно важно<button data-href="#Semantic-Highlighting-Another-Way-to-Find-What-Actually-Matters-in-Text" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Теперь, когда мы поговорили об обрезке контекста, стоит взглянуть на смежную часть головоломки: <a href="https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md"><strong>семантическое выделение</strong></a>. Технически обе функции выполняют практически одну и ту же основную работу - они оценивают фрагменты текста на основе того, насколько они релевантны запросу. Разница заключается в том, как результат используется в конвейере.</p>
<p>Большинство людей, услышав слово "выделение", думают о классических выделителях ключевых слов, которые можно увидеть в Elasticsearch или Solr. Эти инструменты в основном ищут дословные совпадения ключевых слов и оборачивают их в нечто вроде <code translate="no">&lt;em&gt;</code>. Они дешевы и предсказуемы, но работают только тогда, когда в тексте используются те <em>же</em> слова, что и в запросе. Если документ перефразируется, использует синонимы или формулирует идею по-другому, традиционные выделители полностью пропускают ее.</p>
<p><strong>Семантическое выделение идет другим путем.</strong> Вместо того чтобы проверять точное совпадение строк, он использует модель для оценки семантического сходства между запросом и различными участками текста. Это позволяет выделять релевантный контент, даже если формулировки совершенно разные. Для конвейеров RAG, агентских рабочих процессов или любых систем поиска с искусственным интеллектом, где смысл имеет большее значение, чем лексемы, семантическое выделение дает гораздо более четкое представление о том <em>, почему</em> был получен документ.</p>
<p>Проблема в том, что большинство существующих решений для выделения семантики не предназначены для производственных нагрузок ИИ. Мы протестировали все доступные решения, и ни одно из них не обеспечило того уровня точности, задержки и многоязычной надежности, который был необходим нам для реальных систем RAG и агентов. В итоге мы обучили и выложили в открытый доступ собственную модель: <a href="https://huggingface.co/zilliz/semantic-highlight-bilingual-v1">zilliz/semantic-highlight-bilingual-v1</a></p>
<p>На высоком уровне <strong>контекстная обрезка и семантическое выделение решают одну и ту же основную задачу</strong>: при наличии запроса и фрагмента текста выяснить, какие части действительно имеют значение. Разница лишь в том, что происходит дальше.</p>
<ul>
<li><p><strong>Контекстная обрезка</strong> отбрасывает нерелевантные части перед генерацией.</p></li>
<li><p><strong>Семантическое выделение</strong> сохраняет полный текст, но визуально выделяет важные фрагменты.</p></li>
</ul>
<p>Поскольку базовые операции настолько похожи, одна и та же модель часто может использовать обе функции. Это облегчает повторное использование компонентов в стеке и делает вашу систему RAG проще и эффективнее в целом.</p>
<h3 id="Semantic-Highlighting-in-Milvus-and-Zilliz-Cloud" class="common-anchor-header">Семантическое выделение в Milvus и Zilliz Cloud</h3><p>Семантическое выделение теперь полностью поддерживается в <a href="https://milvus.io">Milvus</a> и <a href="https://zilliz.com/cloud"><strong>Zilliz Cloud</strong></a> (полностью управляемый сервис Milvus), и оно уже оказалось полезным для всех, кто работает с RAG или поиском на основе искусственного интеллекта. Функция решает очень простую, но болезненную проблему: когда векторный поиск возвращает тонну фрагментов, как быстро понять <em>, какие предложения в этих фрагментах действительно важны</em>?</p>
<p>Без подсветки пользователи вынуждены читать целые документы, чтобы понять, почему что-то было найдено. Благодаря встроенной семантической подсветке Milvus и Zilliz Cloud автоматически выделяют фрагменты, которые семантически связаны с вашим запросом - даже если формулировки отличаются. Больше не нужно искать совпадения ключевых слов или гадать, почему появился тот или иной фрагмент.</p>
<p>Это делает поиск гораздо более прозрачным. Вместо того чтобы просто возвращать "релевантные документы", Milvus показывает <em>, где</em> находится релевантность. Для конвейеров RAG это особенно полезно, поскольку вы можете сразу увидеть, на что должна обратить внимание модель, что значительно упрощает отладку и построение подсказок.</p>
<p>Мы встроили эту поддержку непосредственно в Milvus и Zilliz Cloud, поэтому вам не придется подключать внешние модели или запускать другой сервис, чтобы получить полезную атрибуцию. Все работает внутри пути поиска: векторный поиск → скоринг релевантности → выделенные диапазоны. Он работает в масштабе "из коробки" и поддерживает многоязычные рабочие нагрузки с помощью нашей модели <a href="https://huggingface.co/zilliz/semantic-highlight-bilingual-v1">zilliz/semantic-highlight-bilingual-v1</a>.</p>
<h2 id="Looking-Ahead" class="common-anchor-header">Перспективы<button data-href="#Looking-Ahead" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Контекстная инженерия все еще довольно нова, и нам еще многое предстоит выяснить. Даже если обрезка и семантическое выделение хорошо работают в <a href="https://milvus.io">Milvus</a> и <a href="https://zilliz.com/cloud"><strong>Zilliz Cloud</strong></a><strong>,</strong> мы еще не приблизились к концу истории. Есть куча областей, которые все еще нуждаются в настоящей инженерной работе: повышение точности моделей обрезки без замедления работы, улучшение обработки странных или внедоменных запросов и соединение всех частей вместе, чтобы поиск → ранжирование → обрезка → выделение ощущались как один чистый конвейер, а не как набор хаков, склеенных вместе.</p>
<p>По мере роста контекстных окон эти решения становятся все более важными. Хорошее управление контекстом больше не является "приятным бонусом", оно становится основной частью надежной работы систем с длинным контекстом и RAG.</p>
<p>Мы собираемся продолжать эксперименты, бенчмарки и поставлять те части, которые действительно имеют значение для разработчиков. Цель проста: упростить создание систем, которые не ломаются под воздействием беспорядочных данных, непредсказуемых запросов и больших рабочих нагрузок.</p>
<p>Если вы хотите обсудить что-то из этого - или вам просто нужна помощь в отладке - вы можете зайти на наш <a href="https://discord.com/invite/8uyFbECzPX">канал Discord</a> или заказать 20-минутную индивидуальную сессию, чтобы получить знания, рекомендации и ответы на свои вопросы в<a href="https://milvus.io/blog/join-milvus-office-hours-to-get-support-from-vectordb-experts.md"> Milvus Office Hours</a>.</p>
<p>Всегда рады пообщаться и обменяться замечаниями с другими строителями.</p>
