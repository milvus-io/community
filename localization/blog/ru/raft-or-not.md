---
id: raft-or-not.md
title: >-
  Плот или не плот? Лучшее решение для обеспечения согласованности данных в
  облачных базах данных
author: Xiaofan Luan
date: 2022-05-16T00:00:00.000Z
desc: >-
  Почему алгоритм репликации на основе консенсуса не является серебряной пулей
  для достижения согласованности данных в распределенных базах данных?
cover: assets.zilliz.com/Tech_Modify_5_e18025ffbc.png
tag: Engineering
tags: 'Data science, Database, Tech, Artificial Intelligence, Vector Management'
canonicalUrl: 'https://milvus.io/blog/raft-or-not.md'
---
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/Tech_Modify_5_e18025ffbc.png" alt="Cover image" class="doc-image" id="cover-image" />
   </span> <span class="img-wrapper"> <span>Изображение на обложке</span> </span></p>
<blockquote>
<p>Эта статья написана <a href="https://github.com/xiaofan-luan">Сяофаном Луаном</a> и переработана <a href="https://www.linkedin.com/in/yiyun-n-2aa713163/">Анжелой Ни</a>.</p>
</blockquote>
<p>Репликация на основе консенсуса - широко распространенная стратегия во многих облачных распределенных базах данных. Однако она имеет определенные недостатки и определенно не является серебряной пулей.</p>
<p>Цель этого поста - сначала объяснить понятия репликации, согласованности и консенсуса в облачных распределенных базах данных, затем объяснить, почему алгоритмы на основе консенсуса, такие как Paxos и Raft, не являются серебряной пулей, и, наконец, предложить <a href="#a-log-replication-strategy-for-cloud-native-and-distributed-database">решение проблемы репликации на основе консенсуса</a>.</p>
<p><strong>Перейти к:</strong></p>
<ul>
<li><a href="#Understanding-replication-consistency-and-consensus">Понимание репликации, согласованности и консенсуса</a></li>
<li><a href="#Consensus-based-replication">Репликация на основе консенсуса</a></li>
<li><a href="#A-log-replication-strategy-for-cloud-native-and-distributed-database">Стратегия репликации журналов для облачных и распределенных баз данных</a></li>
<li><a href="#Summary">Резюме</a></li>
</ul>
<h2 id="Understanding-replication-consistency-and-consensus" class="common-anchor-header">Понимание репликации, согласованности и консенсуса<button data-href="#Understanding-replication-consistency-and-consensus" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Прежде чем углубляться в плюсы и минусы Paxos и Raft, а также предлагать наиболее подходящую стратегию репликации журналов, нам нужно сначала разобраться с понятиями репликации, согласованности и консенсуса.</p>
<p>Обратите внимание, что эта статья в основном посвящена синхронизации инкрементных данных/журнала. Поэтому, говоря о репликации данных/журнала, мы имеем в виду только репликацию инкрементных данных, а не исторических.</p>
<h3 id="Replication" class="common-anchor-header">Репликация</h3><p>Репликация - это процесс создания нескольких копий данных и их хранения на разных дисках, процессах, машинах, кластерах и т. д. с целью повышения надежности данных и ускорения запросов к ним. Поскольку при репликации данные копируются и хранятся в нескольких местах, они становятся более надежными при восстановлении после сбоев дисков, отказов физических машин или ошибок в кластере. Кроме того, множественные копии данных могут повысить производительность распределенной базы данных, значительно ускорив выполнение запросов.</p>
<p>Существуют различные режимы репликации, такие как синхронная/асинхронная репликация, репликация с сильной/эвентуальной согласованностью, лидер-последовательная/децентрализованная репликация. Выбор режима репликации влияет на доступность и согласованность системы. Поэтому, как говорится в знаменитой <a href="https://medium.com/analytics-vidhya/cap-theorem-in-distributed-system-and-its-tradeoffs-d8d981ecf37e">теореме CAP</a>, при неизбежном разделении сети архитектору системы необходимо найти компромисс между согласованностью и доступностью.</p>
<h3 id="Consistency" class="common-anchor-header">Согласованность</h3><p>В двух словах, согласованность в распределенной базе данных - это свойство, которое обеспечивает одинаковое представление данных на каждом узле или реплике при записи или чтении данных в определенный момент времени. Полный список уровней согласованности можно найти в документе <a href="https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels">здесь</a>.</p>
<p>Чтобы прояснить, здесь мы говорим о согласованности по теореме CAP, а не об ACID (atomicity, consistency, isolation, durability). Согласованность по теореме CAP означает, что каждый узел в системе имеет одни и те же данные, в то время как согласованность по ACID означает, что один узел применяет одни и те же правила для каждой потенциальной фиксации.</p>
<p>Как правило, базы данных OLTP (обработка транзакций в режиме онлайн) требуют сильной согласованности или линеаризуемости, чтобы гарантировать, что:</p>
<ul>
<li>При каждом чтении можно получить доступ к последним вставленным данным.</li>
<li>Если после чтения возвращается новое значение, то все последующие чтения, независимо от того, на одном или разных клиентах, должны возвращать это новое значение.</li>
</ul>
<p>Суть линеаризуемости заключается в том, чтобы гарантировать повторяемость нескольких реплик данных - как только новое значение записано или прочитано, все последующие чтения могут просматривать это новое значение, пока оно не будет перезаписано. Распределенная система, обеспечивающая линеаризуемость, избавляет пользователей от необходимости следить за несколькими репликами, а также гарантирует атомарность и порядок каждой операции.</p>
<h3 id="Consensus" class="common-anchor-header">Консенсус</h3><p>Понятие консенсуса вводится в распределенные системы, поскольку пользователи хотят, чтобы распределенные системы работали так же, как и автономные.</p>
<p>Проще говоря, консенсус - это общее согласие по поводу ценности. Например, Стив и Фрэнк захотели перекусить. Стив предложил съесть сэндвичи. Фрэнк согласился с предложением Стива, и они оба съели сэндвичи. Они достигли консенсуса. Точнее, ценность (сэндвичи), предложенная одним из них, была согласована с обоими, и они оба предприняли действия, основанные на этой ценности. Аналогично, консенсус в распределенной системе означает, что когда один процесс предлагает значение, все остальные процессы в системе соглашаются с этим значением и действуют на его основе.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/2bb46e57_9eb5_456e_be7e_e7762aa9eb7e_68dd2e8e65.png" alt="Consensus" class="doc-image" id="consensus" />
   </span> <span class="img-wrapper"> <span>Консенсус</span> </span></p>
<h2 id="Consensus-based-replication" class="common-anchor-header">Репликация на основе консенсуса<button data-href="#Consensus-based-replication" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Самые ранние алгоритмы, основанные на консенсусе, были предложены вместе с <a href="https://pmg.csail.mit.edu/papers/vr.pdf">репликацией с метками просмотра</a> в 1988 году. В 1989 году Лесли Лэмпорт предложил <a href="https://lamport.azurewebsites.net/pubs/paxos-simple.pdf">Paxos</a>, алгоритм, основанный на консенсусе.</p>
<p>В последние годы в индустрии появился еще один распространенный алгоритм на основе консенсуса - <a href="https://raft.github.io/">Raft</a>. Он был принят многими основными базами данных NewSQL, такими как CockroachDB, TiDB, OceanBase и т. д.</p>
<p>Примечательно, что распределенная система не обязательно поддерживает линеаризуемость, даже если в ней используется репликация на основе консенсуса. Тем не менее, линеаризуемость является необходимым условием для построения распределенной базы данных ACID.</p>
<p>При проектировании системы баз данных необходимо учитывать порядок фиксации журналов и машин состояний. Также необходимо соблюдать особую осторожность, чтобы поддерживать лидирующую аренду Paxos или Raft и предотвратить разделение мозга при разделении сети.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://user-images.githubusercontent.com/1500781/165926429-69b5144c-f3ba-4819-87c3-ab7e04a7e22e.png" alt="Raft replication state machine" class="doc-image" id="raft-replication-state-machine" />
   </span> <span class="img-wrapper"> <span>Машина состояний репликации Raft</span> </span></p>
<h3 id="Pros-and-cons" class="common-anchor-header">Плюсы и минусы</h3><p>Действительно, Raft, ZAB и <a href="https://aws.amazon.com/blogs/database/amazon-aurora-under-the-hood-quorum-and-correlated-failure/">основанный на кворуме протокол протоколирования</a> в Aurora - все это вариации Paxos. Репликация на основе консенсуса имеет следующие преимущества:</p>
<ol>
<li>Хотя репликация на основе консенсуса больше сосредоточена на согласованности и разделении сети по теореме CAP, она обеспечивает относительно лучшую доступность по сравнению с традиционной репликацией "лидер-последователь".</li>
<li>Raft - это прорыв, который значительно упростил алгоритмы, основанные на консенсусе. На GitHub существует множество библиотек Raft с открытым исходным кодом (например, <a href="https://github.com/sofastack/sofa-jraft">sofa-jraft</a>).</li>
<li>Производительность репликации на основе консенсуса может удовлетворить большинство приложений и предприятий. С появлением высокопроизводительных SSD и гигабайтных NIC (сетевых интерфейсных карт) бремя синхронизации нескольких реплик уменьшается, что делает алгоритмы Paxos и Raft основными в отрасли.</li>
</ol>
<p>Одно из заблуждений заключается в том, что репликация на основе консенсуса - это серебряная пуля для достижения согласованности данных в распределенной базе данных. Однако это не так. Проблемы доступности, сложности и производительности, с которыми сталкивается алгоритм на основе консенсуса, не позволяют ему стать идеальным решением.</p>
<ol>
<li><p>Снижение доступности Оптимизированный алгоритм Paxos или Raft имеет сильную зависимость от реплики-лидера, что приводит к слабой способности бороться с серыми отказами. При репликации на основе консенсуса новые выборы реплики-лидера не происходят до тех пор, пока узел-лидер не отвечает в течение длительного времени. Поэтому репликация на основе консенсуса не способна справиться с ситуациями, когда ведущий узел работает медленно или происходит треш.</p></li>
<li><p>Высокая сложность Хотя уже существует множество расширенных алгоритмов, основанных на Paxos и Raft, появление <a href="http://www.vldb.org/pvldb/vol13/p3072-huang.pdf">Multi-Raft</a> и <a href="https://www.vldb.org/pvldb/vol11/p1849-cao.pdf">Parallel Raft</a> требует дополнительных размышлений и тестов на синхронизацию между журналами и машинами состояний.</p></li>
<li><p>Снижение производительности В эпоху облачных технологий локальные хранилища заменяются общими хранилищами, такими как EBS и S3, чтобы обеспечить надежность и согласованность данных. В результате репликация на основе консенсуса больше не является обязательным условием для распределенных систем. Более того, репликация на основе консенсуса сопряжена с проблемой избыточности данных, поскольку и решение, и EBS имеют несколько копий.</p></li>
</ol>
<p>При многоцентровой и многооблачной репликации стремление к согласованности ставит под угрозу не только доступность, но и <a href="https://en.wikipedia.org/wiki/PACELC_theorem">задержку</a>, что приводит к снижению производительности. Поэтому в большинстве приложений линеаризуемость не является обязательным условием катастрофоустойчивости для нескольких ЦОД.</p>
<h2 id="A-log-replication-strategy-for-cloud-native-and-distributed-database" class="common-anchor-header">Стратегия репликации журналов для облачных и распределенных баз данных<button data-href="#A-log-replication-strategy-for-cloud-native-and-distributed-database" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Безусловно, алгоритмы, основанные на консенсусе, такие как Raft и Paxos, по-прежнему являются основными алгоритмами, принятыми во многих базах данных OLTP. Однако, наблюдая за примерами протокола <a href="https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/">PacificA</a>, <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/05/socrates.pdf">Socrates</a> и <a href="https://rockset.com/">Rockset</a>, мы видим, что тенденция меняется.</p>
<p>Существует два основных принципа для решения, которое может наилучшим образом служить облачной распределенной базе данных.</p>
<h3 id="1-Replication-as-a-service" class="common-anchor-header">1. Репликация как сервис</h3><p>Необходим отдельный микросервис, предназначенный для синхронизации данных. Модуль синхронизации и модуль хранения больше не должны быть тесно связаны в рамках одного процесса.</p>
<p>Например, в Socrates разделены хранилище, журнал и вычисления. Имеется один выделенный лог-сервис (сервис XLog в центре рисунка ниже).</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/1_0d7822a781.png" alt="Socrates architecture" class="doc-image" id="socrates-architecture" />
   </span> <span class="img-wrapper"> <span>Архитектура Сократа</span> </span></p>
<p>Сервис XLog - это отдельный сервис. Сохранение данных достигается с помощью хранилища с низкой задержкой. Посадочная зона в Сократе отвечает за ускоренное хранение трех реплик.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/2_6d1182b6f1.png" alt="Socrates XLog service" class="doc-image" id="socrates-xlog-service" />
   </span> <span class="img-wrapper"> <span>Сервис XLog в Сократе</span> </span></p>
<p>Лидерский узел асинхронно рассылает логи лог-брокеру и сбрасывает данные в Xstore. Локальный SSD-кэш может ускорить чтение данных. После успешной пересылки данных буферы в посадочной зоне могут быть очищены. Очевидно, что все данные журнала делятся на три уровня - посадочная зона, локальный SSD и XStore.</p>
<h3 id="2-Russian-doll-principle" class="common-anchor-header">2. Принцип русской матрешки</h3><p>Один из способов проектирования системы - следовать принципу русской матрешки: каждый слой завершен и идеально подходит для того, что он делает, так что другие слои могут быть построены на его вершине или вокруг него.</p>
<p>При проектировании облачной базы данных нам необходимо грамотно использовать другие сторонние сервисы, чтобы снизить сложность архитектуры системы.</p>
<p>Похоже, нам не обойтись без Paxos, чтобы избежать единой точки отказа. Однако мы все же можем значительно упростить репликацию журналов, передав выборы лидера сервисам Raft или Paxos, основанным на <a href="https://research.google.com/archive/chubby-osdi06.pdf">Chubby</a>, <a href="https://github.com/bloomreach/zk-replicator">Zk</a> и <a href="https://etcd.io/">etcd</a>.</p>
<p>Например, архитектура <a href="https://rockset.com/">Rockset</a> следует принципу русской матрешки и использует Kafka/Kineses для распределенных журналов, S3 для хранения и локальный SSD-кэш для повышения производительности запросов.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://user-images.githubusercontent.com/1500781/165926697-c8b380dc-d71a-41a9-a76d-a261b77f0b5d.png" alt="Rockset architecture" class="doc-image" id="rockset-architecture" />
   </span> <span class="img-wrapper"> <span>Архитектура Rockset</span> </span></p>
<h3 id="The-Milvus-approach" class="common-anchor-header">Подход Milvus</h3><p>Настраиваемая согласованность в Milvus фактически аналогична последовательным чтениям в репликации на основе консенсуса. Функция последовательного чтения подразумевает использование реплик-последователей для выполнения задач чтения данных в предположении сильной согласованности. Цель - повысить пропускную способность кластера и снизить нагрузку на лидера. Механизм, лежащий в основе функции чтения последователей, заключается в запросе индекса фиксации последнего журнала и предоставлении сервиса запросов до тех пор, пока все данные в индексе фиксации не будут применены к машинам состояний.</p>
<p>Однако при разработке Milvus не была принята стратегия следования. Другими словами, Milvus не запрашивает индекс фиксации каждый раз, когда получает запрос. Вместо этого Milvus использует механизм, подобный водяному знаку во <a href="https://flink.apache.org/">Flink</a>, который уведомляет узел запроса о местонахождении индекса фиксации через регулярный интервал времени. Причина такого механизма в том, что пользователи Milvus обычно не предъявляют высоких требований к согласованности данных, и они могут принять компромисс в видимости данных для повышения производительности системы.</p>
<p>Кроме того, Milvus использует множество микросервисов и отделяет хранение данных от вычислений. В <a href="https://milvus.io/blog/deep-dive-1-milvus-architecture-overview.md#A-bare-bones-skeleton-of-the-Milvus-architecture">архитектуре Milvus</a> для хранения данных используются S3, MinIo и Azure Blob.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/Milvus_architecture_b7743a4a7f.png" alt="Milvus architecture" class="doc-image" id="milvus-architecture" />
   </span> <span class="img-wrapper"> <span>Архитектура Milvus</span> </span></p>
<h2 id="Summary" class="common-anchor-header">Резюме<button data-href="#Summary" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В настоящее время все большее количество облачных баз данных делают репликацию журналов отдельным сервисом. Это позволяет снизить затраты на добавление реплик только для чтения и гетерогенную репликацию. Использование нескольких микросервисов позволяет быстро задействовать зрелую облачную инфраструктуру, что невозможно для традиционных баз данных. Отдельный лог-сервис может полагаться на репликацию на основе консенсуса, но также может следовать стратегии "русской матрешки" и использовать различные протоколы согласованности вместе с Paxos или Raft для достижения линеаризуемости.</p>
<h2 id="References" class="common-anchor-header">Ссылки<button data-href="#References" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li>Lamport L. Paxos made simple [J]. ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001), 2001: 51-58.</li>
<li>Ongaro D, Ousterhout J. In search of an understandable consensus algorithm[C]//2014 USENIX Annual Technical Conference (Usenix ATC 14). 2014: 305-319.</li>
<li>Oki B M, Liskov B H. Viewstamped replication: A new primary copy method to support highly-available distributed systems[C]//Proceedings of the seventh annual ACM Symposium on Principles of distributed computing. 1988: 8-17.</li>
<li>Lin W, Yang M, Zhang L, et al. PacificA: Replication in log-based distributed storage systems[J]. 2008.</li>
<li>Вербицкий А., Гупта А., Саха Д. и др. Amazon aurora: On avoiding distributed consensus for i/os, commits, and membership changes[C]//Proceedings of the 2018 International Conference on Management of Data. 2018: 789-796.</li>
<li>Antonopoulos P, Budovski A, Diaconu C, et al. Socrates: Новый sql-сервер в облаке[C]//Proceedings of the 2019 International Conference on Management of Data. 2019: 1743-1756.</li>
</ul>
