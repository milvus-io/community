---
id: how-to-choose-the-right-embedding-model-for-rag.md
title: 'От Word2Vec к LLM2Vec: Как выбрать правильную модель встраивания для RAG'
author: Rachel Liu
date: 2025-10-03T00:00:00.000Z
desc: >-
  В этом блоге мы расскажем вам о том, как оценивать встраиваемые системы на
  практике, чтобы вы могли выбрать наиболее подходящую для вашей системы RAG.
cover: assets.zilliz.com/Chat_GPT_Image_Oct_3_2025_05_07_11_PM_36b1ba77eb.png
tag: Tutorials
recommend: false
publishToMedium: true
tags: 'Milvus, vector database, vector search, embedding models'
meta_keywords: 'Milvus, AI Agent, embedding model vector database'
meta_title: |
  How to Choose the Right Embedding Model for RAG
origin: 'https://milvus.io/blog/how-to-choose-the-right-embedding-model-for-rag.md'
---
<p>Большие языковые модели очень мощные, но у них есть известная слабость: галлюцинации. <a href="https://zilliz.com/learn/Retrieval-Augmented-Generation">Retrieval-Augmented Generation (RAG)</a> - один из наиболее эффективных способов решения этой проблемы. Вместо того чтобы полагаться исключительно на память модели, RAG извлекает релевантные знания из внешнего источника и включает их в подсказку, гарантируя, что ответы будут основаны на реальных данных.</p>
<p>Система RAG обычно состоит из трех основных компонентов: собственно LLM, <a href="https://zilliz.com/learn/what-is-vector-database">векторной базы данных</a>, такой как <a href="https://milvus.io/">Milvus</a>, для хранения и поиска информации, и модели встраивания. Модель встраивания - это то, что преобразует человеческий язык в машиночитаемые векторы. Считайте ее переводчиком между естественным языком и базой данных. Качество этого переводчика определяет релевантность найденного контекста. Если он работает правильно, пользователи видят точные и полезные ответы. Если он работает неправильно, то даже самая лучшая инфраструктура создает шум, ошибки и напрасные вычисления.</p>
<p>Вот почему так важно понимать модели встраивания. Их существует множество - от ранних методов, таких как Word2Vec, до современных моделей на основе LLM, таких как семейство моделей встраивания текста OpenAI. У каждой из них есть свои преимущества и недостатки. Это руководство поможет вам прояснить ситуацию и покажет, как оценить эмбеддинги на практике, чтобы вы могли выбрать наиболее подходящий для вашей системы RAG.</p>
<h2 id="What-Are-Embeddings-and-Why-Do-They-Matter" class="common-anchor-header">Что такое эмбеддинги и почему они важны?<button data-href="#What-Are-Embeddings-and-Why-Do-They-Matter" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>На самом простом уровне эмбеддинги превращают человеческий язык в числа, которые могут понять машины. Каждое слово, предложение или документ отображается в высокоразмерном векторном пространстве, где расстояние между векторами отражает взаимосвязь между ними. Тексты с похожим смыслом имеют тенденцию группироваться вместе, в то время как несвязанный контент имеет тенденцию отдаляться друг от друга. Именно это делает возможным семантический поиск - поиск смысла, а не просто совпадение ключевых слов.</p>
<p>Модели встраивания не все работают одинаково. В целом они делятся на три категории, каждая из которых имеет свои достоинства и недостатки:</p>
<ul>
<li><p><a href="https://zilliz.com/learn/sparse-and-dense-embeddings"><strong>Разреженные векторы</strong></a> (например, BM25) ориентированы на частоту ключевых слов и длину документа. Они отлично подходят для явных совпадений, но слепы к синонимам и контексту - "искусственный интеллект" и "искусственный интеллект" будут выглядеть несовместимыми.</p></li>
<li><p><a href="https://zilliz.com/learn/sparse-and-dense-embeddings"><strong>Плотные векторы</strong></a> (например, созданные BERT) улавливают более глубокую семантику. Они могут увидеть, что "Apple выпускает новый телефон" связан с "запуском продукта iPhone", даже без общих ключевых слов. Недостатком является более высокая стоимость вычислений и меньшая интерпретируемость.</p></li>
<li><p><strong>Гибридные модели</strong> (например, BGE-M3) сочетают в себе оба этих фактора. Они могут одновременно генерировать разреженные, плотные или многовекторные представления, сохраняя точность поиска по ключевым словам и одновременно улавливая семантические нюансы.</p></li>
</ul>
<p>На практике выбор зависит от конкретного случая использования: разреженные векторы - для скорости и прозрачности, плотные - для более богатого смысла, а гибридные - когда вы хотите получить лучшее из обоих миров.</p>
<h2 id="Eight-Key-Factors-for-Evaluating-Embedding-Models" class="common-anchor-header">Восемь ключевых факторов для оценки моделей встраивания<button data-href="#Eight-Key-Factors-for-Evaluating-Embedding-Models" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><h3 id="1-Context-Window" class="common-anchor-header"><strong>#1 Контекстное окно</strong></h3><p><a href="https://zilliz.com/glossary/context-window"><strong>Контекстное окно</strong></a> определяет объем текста, который модель может обработать за один раз. Поскольку один токен равен примерно 0,75 слова, это число напрямую ограничивает длину отрывка, который модель может "увидеть" при создании вкраплений. Большое окно позволяет модели уловить весь смысл длинных документов; маленькое окно заставляет вас разбивать текст на мелкие кусочки, рискуя потерять значимый контекст.</p>
<p>Например, модель OpenAI <a href="https://zilliz.com/ai-models/text-embedding-ada-002"><em>text-embedding-ada-002</em></a> поддерживает до 8 192 лексем - достаточно, чтобы охватить всю научную статью, включая аннотацию, методы и заключение. В отличие от этого, модели с окнами всего в 512 токенов (например, <em>m3e-base</em>) требуют частого усечения, что может привести к потере ключевых деталей.</p>
<p>Вывод: если в вашем случае речь идет о длинных документах, таких как юридические документы или научные статьи, выбирайте модель с окном 8K+ токенов. Для более коротких текстов, таких как чаты службы поддержки, может быть достаточно окна в 2 тыс. токенов.</p>
<h3 id="2-Tokenization-Unit" class="common-anchor-header">Блок токенизации<strong>#2</strong> </h3><p>Перед генерацией вкраплений текст должен быть разбит на более мелкие фрагменты, называемые <strong>лексемами</strong>. То, как происходит токенизация, влияет на то, насколько хорошо модель справляется с редкими словами, профессиональными терминами и специализированными доменами.</p>
<ul>
<li><p><strong>Токенизация подслова (BPE):</strong> Разделяет слова на более мелкие части (например, "несчастье" → "un" + "happiness"). Эта функция используется по умолчанию в современных LLM, таких как GPT и LLaMA, и хорошо работает для слов, не входящих в словарный запас.</p></li>
<li><p><strong>WordPiece:</strong> Уточнение BPE, используемое в BERT, разработанное для лучшего баланса между охватом словарного запаса и эффективностью.</p></li>
<li><p><strong>Токенизация на уровне слов:</strong> Разделение только по целым словам. Она проста, но не справляется с редкой или сложной терминологией, что делает ее непригодной для технических областей.</p></li>
</ul>
<p>Для специализированных областей, таких как медицина или юриспруденция, лучше всего подходят модели, основанные на подсловах - они могут правильно обрабатывать такие термины, как <em>инфаркт миокарда</em> или <em>суброгация</em>. Некоторые современные модели, такие как <strong>NV-Embed</strong>, идут дальше, добавляя такие усовершенствования, как слои скрытого внимания, которые улучшают то, как токенизация захватывает сложную, специфическую для домена лексику.</p>
<h3 id="3-Dimensionality" class="common-anchor-header">#3 Размерность</h3><p><a href="https://zilliz.com/glossary/dimensionality-reduction"><strong>Размерность вектора</strong></a> - это длина вектора встраивания, которая определяет, насколько подробно модель может отразить семантику. Более высокая размерность (например, 1 536 или более) позволяет более тонко различать понятия, но за это приходится платить увеличением объема памяти, замедлением запросов и более высокими требованиями к вычислениям. Более низкие размерности (например, 768) быстрее и дешевле, но есть риск потерять тонкий смысл.</p>
<p>Ключевым моментом является баланс. Для большинства приложений общего назначения размеры 768-1 536 являются оптимальным сочетанием эффективности и точности. Для задач, требующих высокой точности - например, для академического или научного поиска, - выход за пределы 2 000 размеров может быть оправдан. С другой стороны, в системах с ограниченными ресурсами (например, при развертывании на периферии) можно эффективно использовать 512 измерений при условии, что качество поиска будет подтверждено. В некоторых легких рекомендательных системах или системах персонализации может быть достаточно и меньших размеров.</p>
<h3 id="4-Vocabulary-Size" class="common-anchor-header">Размер словаря #4</h3><p><strong>Объем словарного запаса</strong> модели - это количество уникальных лексем, которые может распознать ее токенизатор. Это напрямую влияет на ее способность работать с различными языками и специфической терминологией. Если слова или символа нет в словаре, он помечается как <code translate="no">[UNK]</code>, что может привести к потере смысла.</p>
<p>Требования зависят от конкретного случая использования. Многоязычные сценарии обычно требуют больших словарей - порядка 50 тысяч с лишним лексем, как в случае с <a href="https://zilliz.com/ai-models/bge-m3"><em>BGE-M3</em></a>. Для приложений, ориентированных на конкретную область, наиболее важен охват специализированных терминов. Например, юридическая модель должна изначально поддерживать такие термины, как <em>&quot;срок давности&quot;</em> или <em>&quot;добросовестное приобретение</em>&quot;, в то время как китайская модель должна учитывать тысячи иероглифов и уникальную пунктуацию. Без достаточного охвата словарного запаса точность встраивания быстро снижается.</p>
<h3 id="-5-Training-Data" class="common-anchor-header"># 5 Данные для обучения</h3><p><strong>Обучающие данные</strong> определяют границы того, что "знает" модель встраивания. Модели, обученные на широких, универсальных данных, таких как <em>text-embedding-ada-002</em>, в которых используется смесь веб-страниц, книг и Википедии, как правило, показывают хорошие результаты в различных областях. Но когда вам нужна точность в специализированных областях, модели, обученные для конкретной области, часто выигрывают. Например, <em>LegalBERT</em> и <em>BioBERT</em> превосходят общие модели на юридических и биомедицинских текстах, хотя и теряют некоторую способность к обобщению.</p>
<p>Правило:</p>
<ul>
<li><p><strong>Общие сценарии</strong> → используйте модели, обученные на широких наборах данных, но убедитесь, что они охватывают ваш целевой язык (языки). Например, для китайских приложений нужны модели, обученные на богатых китайских корпорациях.</p></li>
<li><p><strong>Вертикальные домены</strong> → выбирайте модели, ориентированные на конкретный домен, для достижения наилучшей точности.</p></li>
<li><p><strong>Лучшее из двух миров</strong> → новые модели, такие как <strong>NV-Embed</strong>, обучаемые в два этапа на общих и специфических для домена данных, показывают многообещающий прирост обобщения <em>и</em> точности для домена.</p></li>
</ul>
<h3 id="-6-Cost" class="common-anchor-header"># 6 Стоимость</h3><p>Стоимость - это не только цена API, но и <strong>экономические</strong> и <strong>вычислительные</strong> затраты. Хостинговые модели API, например, от OpenAI, основаны на использовании: вы платите за вызов, но не заботитесь об инфраструктуре. Это делает их идеальными для быстрого создания прототипов, пилотных проектов или малых и средних рабочих нагрузок.</p>
<p>Варианты с открытым исходным кодом, такие как <em>BGE</em> или <em>Sentence-BERT</em>, бесплатны в использовании, но требуют самостоятельного управления инфраструктурой, как правило, кластерами GPU или TPU. Они лучше подходят для крупномасштабного производства, где долгосрочная экономия и гибкость компенсируют единовременные затраты на настройку и обслуживание.</p>
<p>Практический вывод: <strong>Модели API идеально подходят для быстрых итераций</strong>, в то время как <strong>модели с открытым исходным кодом часто выигрывают в крупномасштабном производстве</strong>, если учесть совокупную стоимость владения (TCO). Выбор правильного пути зависит от того, что вам важнее - скорость выхода на рынок или долгосрочный контроль.</p>
<h3 id="-7-MTEB-Score" class="common-anchor-header"># 7 Оценка MTEB</h3><p><a href="https://zilliz.com/glossary/massive-text-embedding-benchmark-(mteb)"><strong>Massive Text Embedding Benchmark (MTEB)</strong></a> - наиболее широко используемый стандарт для сравнения моделей встраивания. Он оценивает производительность в различных задачах, включая семантический поиск, классификацию, кластеризацию и другие. Более высокий балл обычно означает, что модель обладает большей обобщающей способностью для различных типов задач.</p>
<p>При этом MTEB не является серебряной пулей. Модель, получившая высокий общий балл, может оказаться неэффективной в вашем конкретном случае. Например, модель, обученная в основном на английском языке, может показать хорошие результаты в эталонных тестах MTEB, но испытывать трудности при работе со специализированными медицинскими текстами или данными на неанглийском языке. Безопасный подход - использовать MTEB в качестве отправной точки, а затем проверить его на <strong>собственных наборах данных</strong>, прежде чем принимать решение.</p>
<h3 id="-8-Domain-Specificity" class="common-anchor-header"># 8 Специфика домена</h3><p>Некоторые модели специально разработаны для конкретных сценариев, и они отлично работают там, где общие модели не справляются:</p>
<ul>
<li><p><strong>Юридические:</strong> <em>LegalBERT</em> может различать тонкие юридические термины, такие как <em>защита</em> и <em>юрисдикция</em>.</p></li>
<li><p><strong>Биомедицина:</strong> <em>BioBERT</em> точно обрабатывает такие технические фразы, как <em>мРНК</em> или <em>целевая терапия</em>.</p></li>
<li><p><strong>Многоязычность:</strong> <em>BGE-M3</em> поддерживает более 100 языков, что делает его хорошо подходящим для глобальных приложений, требующих соединения английского, китайского и других языков.</p></li>
<li><p><strong>Поиск кода:</strong> <em>Qwen3-Embedding</em> достигает высших оценок (81.0+) по <em>MTEB-Code</em>, оптимизированному для запросов, связанных с программированием.</p></li>
</ul>
<p>Если ваш случай использования относится к одной из этих областей, оптимизированные для этой области модели могут значительно повысить точность поиска. Но для более широкого применения следует использовать модели общего назначения, если только ваши тесты не покажут обратное.</p>
<h2 id="Additional-Perspectives-for-Evaluating-Embeddings" class="common-anchor-header">Дополнительные перспективы для оценки встраиваемых моделей<button data-href="#Additional-Perspectives-for-Evaluating-Embeddings" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Помимо основных восьми факторов, есть еще несколько аспектов, которые стоит рассмотреть, если вы хотите получить более глубокую оценку:</p>
<ul>
<li><p><strong>Многоязычное выравнивание</strong>: Для многоязычных моделей недостаточно просто поддерживать много языков. Настоящий тест заключается в том, выровнены ли векторные пространства. Другими словами, совпадают ли семантически идентичные слова, например "cat" на английском и "gato" на испанском, в векторном пространстве? Сильное выравнивание обеспечивает согласованный межъязыковой поиск.</p></li>
<li><p><strong>Состязательное тестирование</strong>: Хорошая модель встраивания должна быть стабильной при небольших изменениях входных данных. Подавая на вход почти одинаковые предложения (например, "Кошка сидела на коврике" против "Кошка сидела на коврике"), вы можете проверить, изменяются ли результирующие векторы в разумных пределах или колеблются в широких пределах. Большие колебания часто указывают на слабую устойчивость.</p></li>
<li><p><strong>Локальная семантическая связность</strong> - это феномен проверки того, насколько плотно семантически схожие слова группируются в локальных окрестностях. Например, при наличии такого слова, как "банк", модель должна соответствующим образом группировать родственные термины (такие как "берег реки" и "финансовое учреждение"), а несвязанные термины держать на расстоянии. Измерение того, как часто "навязчивые" или нерелевантные слова проникают в эти районы, помогает сравнить качество модели.</p></li>
</ul>
<p>Эти перспективы не всегда требуются для повседневной работы, но они полезны для стресс-тестирования встраивания в производственные системы, где важна многоязычная, высокоточная или состязательная стабильность.</p>
<h2 id="Common-Embedding-Models-A-Brief-History" class="common-anchor-header">Распространенные модели встраивания: Краткая история<button data-href="#Common-Embedding-Models-A-Brief-History" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>История моделей встраивания - это история того, как машины с течением времени научились глубже понимать язык. Каждое поколение расширяло границы предыдущего, переходя от статичных представлений слов к современным большим моделям встраивания языка (LLM), способным улавливать нюансы контекста.</p>
<h3 id="Word2Vec-The-Starting-Point-2013" class="common-anchor-header">Word2Vec: Отправная точка (2013)</h3><p><a href="https://zilliz.com/glossary/word2vec">Word2Vec от Google</a> стал первым прорывом, который сделал вкрапления широко практичными. В его основе лежит <em>гипотеза распределения</em> в лингвистике - идея о том, что слова, встречающиеся в похожих контекстах, часто имеют общее значение. Анализируя огромные объемы текстов, Word2Vec отображал слова в векторное пространство, где родственные термины располагались близко друг к другу. Например, "пума" и "леопард" сгруппировались рядом благодаря общей среде обитания и охотничьим признакам.</p>
<p>Word2Vec выпускается в двух вариантах:</p>
<ul>
<li><p><strong>CBOW (Continuous Bag of Words):</strong> предсказывает пропущенное слово по окружающему контексту.</p></li>
<li><p><strong>Skip-Gram:</strong> делает обратное - предсказывает окружающие слова по целевому слову.</p></li>
</ul>
<p>Этот простой, но мощный подход позволял проводить элегантные аналогии, например:</p>
<pre><code translate="no">king - man + woman = queen
<button class="copy-code-btn"></button></code></pre>
<p>Для своего времени Word2Vec был революционным. Но у него было два существенных ограничения. Во-первых, он был <strong>статичен</strong>: каждое слово имело только один вектор, поэтому слово "банк" означало одно и то же, независимо от того, находилось ли оно рядом с "деньгами" или "рекой". Во-вторых, он работал только на <strong>уровне слов</strong>, оставляя предложения и документы вне зоны его действия.</p>
<h3 id="BERT-The-Transformer-Revolution-2018" class="common-anchor-header">BERT: Революция трансформеров (2018)</h3><p>Если Word2Vec дал нам первую карту смысла, то <a href="https://zilliz.com/learn/what-is-bert"><strong>BERT (Bidirectional Encoder Representations from Transformers)</strong></a> перерисовал ее с гораздо большей детализацией. Выпущенный Google в 2018 году, BERT ознаменовал начало эры <em>глубокого семантического понимания</em>, внедрив архитектуру Transformer в эмбеддинги. В отличие от более ранних LSTM, трансформеры могут исследовать все слова в последовательности одновременно и в обоих направлениях, что позволяет получить гораздо более богатый контекст.</p>
<p>Волшебство BERT было достигнуто благодаря двум умным задачам предварительного обучения:</p>
<ul>
<li><p><strong>Маскированное моделирование языка (MLM):</strong> Случайным образом скрывает слова в предложении и заставляет модель предсказывать их, обучая ее угадывать значение из контекста.</p></li>
<li><p><strong>Предсказание следующего предложения (Next Sentence Prediction, NSP):</strong> тренирует модель определять, следуют ли два предложения друг за другом, помогая ей изучать взаимосвязи между предложениями.</p></li>
</ul>
<p>Входные векторы BERT объединяли три элемента: вкрапления лексем (само слово), вкрапления сегментов (к какому предложению оно относится) и вкрапления позиций (где оно находится в последовательности). Все это позволило BERT улавливать сложные семантические связи как на уровне <strong>предложений</strong>, так и на уровне <strong>документов</strong>. Этот скачок сделал BERT передовым для таких задач, как ответы на вопросы и семантический поиск.</p>
<p>Конечно, BERT не был совершенен. Его ранние версии были ограничены <strong>окном в 512 слов</strong>, что означало необходимость измельчения длинных документов, а иногда и потерю смысла. Плотным векторам также не хватало интерпретируемости - вы могли увидеть, что два текста совпадают, но не всегда могли объяснить, почему. Более поздние варианты, такие как <strong>RoBERTa</strong>, отказались от задачи NSP после того, как исследования показали, что от нее мало пользы, сохранив при этом мощное обучение MLM.</p>
<h3 id="BGE-M3-Fusing-Sparse-and-Dense-2023" class="common-anchor-header">BGE-M3: Слияние разреженности и плотности (2023 год)</h3><p>К 2023 году область стала достаточно зрелой, чтобы понять, что ни один метод встраивания не может решить все задачи. Появилась <a href="https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings">BGE-M3</a> (BAAI General Embedding-M3), гибридная модель, специально разработанная для задач поиска. Ее ключевое новшество заключается в том, что она генерирует не один тип вектора, а плотные векторы, разреженные векторы и мультивекторы одновременно, объединяя их сильные стороны.</p>
<ul>
<li><p><strong>Плотные векторы</strong> отражают глубокую семантику, обрабатывая синонимы и парафразы (например, "iPhone launch", ≈ "Apple выпускает новый телефон").</p></li>
<li><p><strong>Разреженные векторы</strong> присваивают явные веса терминам. Даже если ключевое слово не встречается, модель может сделать вывод о его релевантности - например, связать "новый продукт iPhone" с "Apple Inc." и "смартфон".</p></li>
<li><p><strong>Мультивекторы</strong> еще больше улучшают плотные вкрапления, позволяя каждой лексеме вносить свой собственный балл взаимодействия, что полезно для тонкого поиска.</p></li>
</ul>
<p>Конвейер обучения BGE-M3 отражает эту сложность:</p>
<ol>
<li><p><strong>Предварительное обучение</strong> на массивных неразмеченных данных с помощью <em>RetroMAE</em> (кодер с маской + декодер с реконструкцией) для создания общего семантического понимания.</p></li>
<li><p><strong>Общая тонкая настройка</strong> с помощью контрастного обучения на 100 миллионах пар текстов, что повышает эффективность поиска.</p></li>
<li><p><strong>Тонкая настройка задачи</strong> с помощью настройки инструкций и сложной отрицательной выборки для оптимизации под конкретный сценарий.</p></li>
</ol>
<p>Результаты впечатляют: BGE-M3 обрабатывает множество грануляций (от уровня слов до уровня документов), обеспечивает высокую многоязычную производительность - особенно на китайском языке - и балансирует между точностью и эффективностью лучше, чем большинство аналогов. На практике он представляет собой большой шаг вперед в создании моделей встраивания, которые являются одновременно мощными и практичными для крупномасштабного поиска.</p>
<h3 id="LLMs-as-Embedding-Models-2023–Present" class="common-anchor-header">LLM как модели встраивания (2023 год - настоящее время)</h3><p>В течение многих лет преобладало мнение, что декодерные модели языка (LLM), такие как GPT, не подходят для встраивания. Считалось, что их каузальное внимание, которое обращается только к предыдущим лексемам, ограничивает глубокое семантическое понимание. Но недавние исследования перевернули это мнение. При правильной настройке LLM могут генерировать вкрапления, которые конкурируют, а иногда и превосходят специально созданные модели. Два ярких примера - LLM2Vec и NV-Embed.</p>
<p><strong>LLM2Vec</strong> адаптирует LLM, работающие только с декодером, с тремя ключевыми изменениями:</p>
<ul>
<li><p><strong>Двунаправленное преобразование внимания</strong>: замена причинно-следственных масок, чтобы каждый лексем мог воспринимать всю последовательность.</p></li>
<li><p><strong>Предсказание следующего лексем с маской (MNTP):</strong> новая цель обучения, которая поощряет двунаправленное понимание.</p></li>
<li><p><strong>Неконтролируемое контрастное обучение:</strong> вдохновленное SimCSE, оно сближает семантически похожие предложения в векторном пространстве.</p></li>
</ul>
<p><strong>NV-Embed</strong>, тем временем, использует более рациональный подход:</p>
<ul>
<li><p><strong>Слои латентного внимания:</strong> добавление обучаемых "латентных массивов" для улучшения объединения последовательностей.</p></li>
<li><p><strong>Прямое двунаправленное обучение:</strong> просто удаляем причинные маски и настраиваем с помощью контрастного обучения.</p></li>
<li><p><strong>Оптимизация среднего пула:</strong> использование средневзвешенных значений по лексемам, чтобы избежать "предвзятости последнего лексемы".</p></li>
</ul>
<p>В результате современные вкрапления на основе LLM сочетают в себе <strong>глубокое семантическое понимание</strong> и <strong>масштабируемость</strong>. Они могут работать с <strong>очень длинными контекстными окнами (8К-32К лексем)</strong>, что делает их особенно сильными для задач с большим количеством документов в научных исследованиях, юриспруденции или корпоративном поиске. А поскольку они используют ту же основу LLM, то иногда могут обеспечивать высокое качество встраивания даже в более ограниченных средах.</p>
<h2 id="Conclusion-Turning-Theory-into-Practice" class="common-anchor-header">Заключение: Превращение теории в практику<button data-href="#Conclusion-Turning-Theory-into-Practice" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Когда дело доходит до выбора модели встраивания, теория помогает только в теории. Реальная проверка заключается в том, насколько хорошо она работает в <em>вашей</em> системе с <em>вашими</em> данными. Несколько практических шагов могут сделать разницу между моделью, которая хорошо выглядит на бумаге, и моделью, которая действительно работает в производстве:</p>
<ul>
<li><p><strong>Проводите скрининг с подмножествами MTEB.</strong> Используйте контрольные показатели, особенно задачи поиска, чтобы составить первоначальный список кандидатов.</p></li>
<li><p><strong>Протестируйте на реальных бизнес-данных.</strong> Создайте оценочные наборы из собственных документов, чтобы измерить recall, precision и latency в реальных условиях.</p></li>
<li><p><strong>Проверьте совместимость с базами данных.</strong> Разреженные векторы требуют поддержки инвертированных индексов, в то время как плотные векторы высокой размерности требуют большего объема хранения и вычислений. Убедитесь, что ваша база данных векторов может удовлетворить ваш выбор.</p></li>
<li><p><strong>Умело обращайтесь с длинными документами.</strong> Используйте стратегии сегментации, такие как скользящие окна, для повышения эффективности и сочетайте их с моделями больших контекстных окон для сохранения смысла.</p></li>
</ul>
<p>От простых статических векторов Word2Vec до вкраплений на базе LLM с 32 тыс. контекстов - мы стали свидетелями огромного прогресса в том, как машины понимают язык. Но вот урок, который в конце концов усваивает каждый разработчик: модель <em>с наивысшим баллом</em> не всегда является <em>лучшей</em> для вашего случая использования.</p>
<p>В конце концов, пользователей не волнуют таблицы лидеров MTEB или эталонные графики - они просто хотят быстро найти нужную информацию. Выберите модель, которая обеспечивает баланс между точностью, стоимостью и совместимостью с вашей системой, и вы создадите то, что не просто впечатляет в теории, но и действительно работает в реальном мире.</p>
