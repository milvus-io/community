---
id: optimize-vector-databases-enhance-rag-driven-generative-ai.md
title: 'Оптимизация векторных баз данных, улучшение генеративного ИИ на основе RAG'
author: 'Cathy Zhang, Dr. Malini Bhandaru'
date: 2024-05-13T00:00:00.000Z
desc: >-
  В этой статье вы узнаете больше о векторных базах данных и их бенчмарках,
  наборах данных для изучения различных аспектов и инструментах, используемых
  для анализа производительности - все, что нужно для начала оптимизации
  векторных баз данных.
cover: >-
  assets.zilliz.com/Optimize_Vector_Databases_Enhance_RAG_Driven_Generative_AI_6e3b370f25.png
tag: Engineering
tags: >-
  Milvus, Vector Database, Open Source, Data science, Artificial Intelligence,
  Vector Management, RAG, Generative AI
recommend: true
canonicalUrl: >-
  https://medium.com/intel-tech/optimize-vector-databases-enhance-rag-driven-generative-ai-90c10416cb9c
---
<p><em>Это сообщение было первоначально опубликовано на <a href="https://medium.com/intel-tech/optimize-vector-databases-enhance-rag-driven-generative-ai-90c10416cb9c">канале Intel в Medium</a> и публикуется здесь с разрешения.</em></p>
<p><br></p>
<p>Два метода оптимизации векторной базы данных при использовании RAG</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:1400/1*FRWBVwOHPYFDIVTp_ylZNQ.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Фотография <a href="https://unsplash.com/@ilyapavlov?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Ильи Павлова</a> на <a href="https://unsplash.com/photos/monitor-showing-java-programming-OqtafYT5kTw?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a></p>
<p>Кэти Чжан и доктор Малини Бхандару Соавторы: Линь Ян и Чангян Лю</p>
<p>Генеративные модели искусственного интеллекта (GenAI), которые находят все большее применение в нашей повседневной жизни, совершенствуются с помощью технологии <a href="https://www.techtarget.com/searchenterpriseai/definition/retrieval-augmented-generation">генерации с расширением поиска (RAG)</a>, которая позволяет повысить точность и надежность ответа за счет получения фактов из внешних источников. RAG помогает обычной <a href="https://www.techtarget.com/whatis/definition/large-language-model-LLM">модели большого языка (LLM)</a> понять контекст и уменьшить количество <a href="https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)">галлюцинаций</a> за счет использования гигантской базы неструктурированных данных, хранящихся в виде векторов - математического представления, которое помогает передать контекст и взаимосвязи между данными.</p>
<p>RAG помогает извлекать больше контекстной информации и, следовательно, генерировать более качественные ответы, но векторные базы данных, на которые они опираются, становятся все больше и больше, чтобы обеспечить богатый контент для использования. Как на горизонте появляются LLM с триллионами параметров, так и векторные базы данных с миллиардами векторов не за горами. Как инженерам по оптимизации, нам было интересно узнать, сможем ли мы сделать векторные базы данных более производительными, быстрее загружать данные и быстрее создавать индексы, чтобы обеспечить скорость поиска даже при добавлении новых данных. Это не только сократит время ожидания пользователей, но и сделает ИИ-решения на базе RAG более устойчивыми.</p>
<p>В этой статье вы узнаете больше о векторных базах данных и их бенчмарках, наборах данных для изучения различных аспектов и инструментах, используемых для анализа производительности, - обо всем, что необходимо для начала оптимизации векторных баз данных. Мы также поделимся нашими достижениями в оптимизации двух популярных решений для векторных баз данных, чтобы вдохновить вас на путь оптимизации производительности и воздействия на устойчивость.</p>
<h2 id="Understanding-Vector-Databases" class="common-anchor-header">Понимание векторных баз данных<button data-href="#Understanding-Vector-Databases" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В отличие от традиционных реляционных или нереляционных баз данных, где данные хранятся в структурированном виде, векторная база данных содержит математическое представление отдельных элементов данных, называемое вектором, построенное с помощью функции встраивания или преобразования. Вектор обычно представляет особенности или семантические значения и может быть коротким или длинным. Векторные базы данных выполняют поиск векторов по сходству с помощью метрики расстояния (где "ближе" означает, что результаты более похожи), такой как <a href="https://www.pinecone.io/learn/vector-similarity/">евклидово, точечное или косинусное сходство</a>.</p>
<p>Чтобы ускорить процесс поиска, векторные данные организуются с помощью механизма индексирования. Примерами таких методов организации являются плоские структуры, <a href="https://arxiv.org/abs/2002.09094">инвертированный файл (IVF),</a> <a href="https://arxiv.org/abs/1603.09320">иерархические перемещаемые малые миры (HNSW)</a>, <a href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">хеширование с учетом локальности (LSH)</a> и другие. Каждый из этих методов способствует повышению эффективности и результативности поиска похожих векторов в случае необходимости.</p>
<p>Давайте рассмотрим, как можно использовать базу данных векторов в системе GenAI. Рисунок 1 иллюстрирует как загрузку данных в векторную базу, так и ее использование в контексте приложения GenAI. Когда вы вводите подсказку, она подвергается процессу преобразования, идентичному тому, который используется для создания векторов в базе данных. Затем эта преобразованная векторная подсказка используется для извлечения похожих векторов из базы данных векторов. Эти извлеченные элементы, по сути, служат разговорной памятью, предоставляя контекстную историю для подсказок, подобно тому, как работают LLM. Эта функция особенно полезна в обработке естественного языка, компьютерном зрении, рекомендательных системах и других областях, требующих семантического понимания и сопоставления данных. Ваша первоначальная подсказка впоследствии "объединяется" с извлеченными элементами, обеспечивая контекст и помогая LLM формулировать ответы на основе предоставленного контекста, а не только полагаться на свои исходные обучающие данные.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:1400/1*zQj_YJdWc2xKB6Vv89lzDQ.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Рисунок 1. Архитектура приложения RAG.</p>
<p>Векторы хранятся и индексируются для быстрого поиска. Векторные базы данных бывают двух видов: традиционные базы данных, которые были расширены для хранения векторов, и специально созданные векторные базы данных. Примерами традиционных баз данных, поддерживающих векторы, являются <a href="https://redis.io/">Redis</a>, <a href="https://github.com/pgvector/pgvector">pgvector</a>, <a href="https://www.elastic.co/elasticsearch">Elasticsearch</a> и <a href="https://opensearch.org/">OpenSearch</a>. Примерами специально созданных векторных баз данных являются проприетарные решения <a href="https://zilliz.com/">Zilliz</a> и <a href="https://www.pinecone.io/">Pinecone</a>, а также проекты с открытым исходным кодом <a href="https://milvus.io/">Milvus</a>, <a href="https://weaviate.io/">Weaviate</a>, <a href="https://qdrant.tech/">Qdrant</a>, <a href="https://github.com/facebookresearch/faiss">Faiss</a> и <a href="https://www.trychroma.com/">Chroma</a>. Вы можете узнать больше о векторных базах данных на GitHub через <a href="https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/vectorstores">LangChain </a>и <a href="https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases">OpenAI Cookbook</a>.</p>
<p>Мы рассмотрим по одной базе данных из каждой категории - Milvus и Redis.</p>
<h2 id="Improving-Performance" class="common-anchor-header">Повышение производительности<button data-href="#Improving-Performance" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Прежде чем перейти к оптимизации, давайте рассмотрим, как оцениваются векторные базы данных, некоторые механизмы оценки и доступные инструменты анализа производительности.</p>
<h3 id="Performance-Metrics" class="common-anchor-header">Метрики производительности</h3><p>Давайте рассмотрим ключевые метрики, которые помогут вам оценить производительность векторных баз данных.</p>
<ul>
<li><strong>Задержка загрузки</strong> измеряет время, необходимое для загрузки данных в память векторной базы данных и создания индекса. Индекс - это структура данных, используемая для эффективной организации и извлечения векторных данных на основе их сходства или расстояния. Типы <a href="https://milvus.io/docs/index.md#In-memory-Index">индексов in-memory</a> включают <a href="https://thedataquarry.com/posts/vector-db-3/#flat-indexes">плоский индекс</a>, <a href="https://supabase.com/docs/guides/ai/vector-indexes/ivf-indexes">IVF_FLAT</a>, <a href="https://towardsdatascience.com/ivfpq-hnsw-for-billion-scale-similarity-search-89ff2f89d90e">IVF_PQ, HNSW</a>, <a href="https://github.com/google-research/google-research/tree/master/scann">масштабируемые ближайшие соседи (ScaNN)</a>и <a href="https://milvus.io/docs/disk_index.md">DiskANN</a>.</li>
<li><strong>Recall</strong> - это доля истинных совпадений, или релевантных элементов, найденных в <a href="https://redis.io/docs/data-types/probabilistic/top-k/">K лучших</a> результатах, полученных поисковым алгоритмом. Более высокие значения показателя Recall указывают на лучший поиск релевантных элементов.</li>
<li><strong>Количество запросов в секунду (QPS)</strong> - это скорость, с которой векторная база данных может обрабатывать входящие запросы. Более высокие значения QPS означают лучшую способность обработки запросов и пропускную способность системы.</li>
</ul>
<h3 id="Benchmarking-Frameworks" class="common-anchor-header">Бенчмаркинг фреймворков</h3><p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:920/1*mssEjZAuXg6nf-pad67rHA.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Рисунок 2. Схема бенчмаркинга векторных баз данных.</p>
<p>Для проведения бенчмаркинга векторной базы данных требуется сервер векторной базы данных и клиенты. В наших тестах производительности мы использовали два популярных инструмента с открытым исходным кодом.</p>
<ul>
<li><a href="https://github.com/zilliztech/VectorDBBench/tree/main"><strong>VectorDBBench</strong></a><strong>:</strong> Разработанный и открытый компанией Zilliz, VectorDBBench помогает тестировать различные векторные базы данных с разными типами индексов и предоставляет удобный веб-интерфейс.</li>
<li><a href="https://github.com/qdrant/vector-db-benchmark/tree/master"><strong>vector-db-benchmark</strong></a><strong>:</strong> Разработанный и открытый компанией Qdrant, vector-db-benchmark помогает протестировать несколько типичных векторных баз данных для типа индекса <a href="https://www.datastax.com/guides/hierarchical-navigable-small-worlds">HNSW</a>. Он запускает тесты через командную строку и предоставляет __файл <a href="https://docs.docker.com/compose/">Docker Compose</a> для упрощения запуска серверных компонентов.</li>
</ul>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:1400/1*NpHHEFV0TxRMse83hK6H1A.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Рисунок 3. Пример команды vector-db-benchmark, используемой для запуска эталонного теста.</p>
<p>Но эталонный фреймворк - это только часть уравнения. Нам нужны данные, которые позволят проверить различные аспекты самого решения векторной базы данных, такие как способность обрабатывать большие объемы данных, различные размеры векторов и скорость поиска.</p>
<h3 id="Open-Datasets-to-Exercise-Vector-Databases" class="common-anchor-header">Открытые наборы данных для тестирования векторных баз данных</h3><p>Большие наборы данных - хорошие кандидаты для тестирования задержки нагрузки и распределения ресурсов. Некоторые наборы данных имеют высокую размерность и хорошо подходят для тестирования скорости вычисления сходства.</p>
<p>Размерность наборов данных варьируется от 25 до 2048. Набор данных <a href="https://laion.ai/">LAION</a>, открытая коллекция изображений, использовался для обучения очень больших визуальных и языковых глубоко нейронных моделей, таких как стабильные диффузионные генеративные модели. Набор данных OpenAI из 5 миллионов векторов, каждый из которых имеет размерность 1536, был создан VectorDBBench путем запуска OpenAI на <a href="https://huggingface.co/datasets/allenai/c4">необработанных данных</a>. Учитывая, что каждый элемент вектора имеет тип FLOAT, для сохранения одних только векторов требуется около 29 ГБ (5M * 1536 * 4) памяти, плюс еще столько же для хранения индексов и других метаданных - итого 58 ГБ памяти для тестирования. При использовании инструмента vector-db-benchmark обеспечьте достаточное дисковое пространство для сохранения результатов.</p>
<p>Для тестирования задержки загрузки нам нужна большая коллекция векторов, которую предоставляет <a href="https://docs.hippo.transwarp.io/docs/performance-dataset">deep-image-96-angular</a>. Для проверки производительности генерации индексов и вычисления сходства векторы высокой размерности дают большую нагрузку. Для этого мы выбрали набор данных 500K с векторами размерности 1536.</p>
<h3 id="Performance-Tools" class="common-anchor-header">Инструменты для проверки производительности</h3><p>Мы рассмотрели способы нагрузить систему, чтобы выявить интересующие нас метрики, но давайте изучим, что происходит на более низком уровне: насколько загружен вычислительный блок, потребление памяти, ожидание блокировок и т. д.? Эти данные дают представление о поведении базы данных, что особенно полезно для выявления проблемных областей.</p>
<p>Утилита <a href="https://www.redhat.com/sysadmin/interpret-top-output">top</a> в Linux предоставляет информацию о производительности системы. Однако инструмент <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a> в Linux позволяет получить более глубокие сведения. Чтобы узнать больше, мы также рекомендуем ознакомиться с <a href="https://www.brendangregg.com/perf.html">примерами Linux perf</a> и <a href="https://www.intel.com/content/www/us/en/docs/vtune-profiler/cookbook/2023-0/top-down-microarchitecture-analysis-method.html">методом анализа микроархитектуры Intel "сверху вниз"</a>. Еще один инструмент - <a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html">Intel® vTune™ Profiler</a>, который полезен при оптимизации производительности и конфигурации не только приложений, но и системы для различных рабочих нагрузок, включая HPC, облачные вычисления, IoT, медиа, системы хранения данных и многое другое.</p>
<h2 id="Milvus-Vector-Database-Optimizations" class="common-anchor-header">Оптимизации базы данных Milvus Vector<button data-href="#Milvus-Vector-Database-Optimizations" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Давайте рассмотрим несколько примеров того, как мы пытались повысить производительность векторной базы данных Milvus.</p>
<h3 id="Reducing-Memory-Movement-Overhead-in-Datanode-Buffer-Write" class="common-anchor-header">Сокращение накладных расходов на перемещение памяти при записи в буфер датанода</h3><p>Прокси-серверы Milvus записывают данные в лог-брокер через <em>MsgStream</em>. Затем узлы данных потребляют эти данные, преобразуя и сохраняя их в сегменты. Сегменты объединяют вновь вставленные данные. Логика слияния выделяет новый буфер для хранения/перемещения старых и новых вставляемых данных, а затем возвращает новый буфер в качестве старых данных для следующего слияния данных. В результате старые данные становятся все больше и больше, что, в свою очередь, замедляет перемещение данных. Профили производительности показали высокие накладные расходы на эту логику.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:1400/1*Az4dMVBcGmdeyKNrwpR19g.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Рисунок 4. Слияние и перемещение данных в векторной базе данных приводит к высокой производительности.</p>
<p>Мы изменили логику работы <em>буфера слияния</em>, чтобы напрямую добавлять новые данные к старым, избегая выделения нового буфера и перемещения больших старых данных. Перфопрофили подтверждают отсутствие накладных расходов на эту логику. Метрики микрокода <em>metric_CPU operating frequency</em> и <em>metric_CPU utilization</em> указывают на улучшение, которое согласуется с тем, что системе больше не нужно ждать долгого перемещения памяти. Задержка загрузки улучшилась более чем на 60 %. Улучшение зафиксировано на <a href="https://github.com/milvus-io/milvus/pull/26839">GitHub</a>.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://miro.medium.com/v2/resize:fit:1400/1*MmaUtBTdqmMvC5MlQ8V0wQ.jpeg" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Рисунок 5. При меньшем копировании мы видим улучшение производительности более чем на 50 % в задержке загрузки.</p>
<h3 id="Inverted-Index-Building-with-Reduced-Memory-Allocation-Overhead" class="common-anchor-header">Инвертированное построение индексов с уменьшенной нагрузкой на выделение памяти</h3><p>Поисковая система <a href="https://milvus.io/docs/knowhere.md">Knowhere</a> компании Milvus использует <a href="https://www.vlfeat.org/api/kmeans-fundamentals.html#kmeans-elkan">алгоритм Elkan k-means</a> для обучения кластерных данных для создания <a href="https://milvus.io/docs/v1.1.1/index.md">инвертированных файловых (IVF) индексов</a>. Каждый раунд обучения данных определяет количество итераций. Чем больше счетчик, тем лучше результаты обучения. Однако это также означает, что алгоритм Элкана будет вызываться чаще.</p>
<p>Алгоритм Elkan при каждом выполнении обрабатывает выделение и удаление памяти. В частности, он выделяет память для хранения половины размера данных симметричной матрицы, исключая диагональные элементы. В Knowhere размерность симметричной матрицы, используемой алгоритмом Elkan, установлена на 1024, что приводит к размеру памяти около 2 МБ. Это означает, что для каждого раунда обучения Elkan многократно выделяет и деаллоцирует 2 МБ памяти.</p>
<p>Данные профилирования указывали на частую активность выделения большого объема памяти. Фактически, это вызывало выделение <a href="https://www.oreilly.com/library/view/linux-device-drivers/9781785280009/4759692f-43fb-4066-86b2-76a90f0707a2.xhtml">виртуальной области памяти (VMA)</a>, выделение физических страниц, настройку карты страниц и обновление статистики cgroup памяти в ядре. Такая активность выделения/деаллокации большого объема памяти в некоторых ситуациях может также усугубить фрагментацию памяти. Это существенный налог.</p>
<p>Структура <em>IndexFlatElkan</em> специально разработана и создана для поддержки алгоритма Элкана. В каждом процессе обучения данных инициализируется экземпляр <em>IndexFlatElkan</em>. Чтобы уменьшить влияние на производительность, возникающее из-за частого выделения и удаления памяти в алгоритме Elkan, мы рефакторизовали логику кода, перенеся управление памятью за пределы функции алгоритма Elkan в процесс построения <em>IndexFlatElkan</em>. Это позволяет выделять память только один раз на этапе инициализации, обслуживая все последующие вызовы функции алгоритма Elkan из текущего процесса обучения данным, и помогает улучшить задержку загрузки примерно на 3 %. Найти <a href="https://github.com/zilliztech/knowhere/pull/280">патч Knowhere можно здесь</a>.</p>
<h2 id="Redis-Vector-Search-Acceleration-through-Software-Prefetch" class="common-anchor-header">Ускорение векторного поиска в Redis с помощью программной предвыборки<button data-href="#Redis-Vector-Search-Acceleration-through-Software-Prefetch" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Redis, популярное традиционное хранилище данных в памяти с ключами-значениями, недавно начало поддерживать векторный поиск. Чтобы выйти за рамки обычного хранилища ключевых значений, он предлагает модули расширения; модуль <a href="https://github.com/RediSearch/RediSearch">RediSearch</a> облегчает хранение и поиск векторов непосредственно в Redis.</p>
<p>Для поиска векторного сходства Redis поддерживает два алгоритма, а именно грубую силу и HNSW. Алгоритм HNSW специально разработан для эффективного поиска приблизительных ближайших соседей в высокоразмерных пространствах. Он использует приоритетную очередь с именем <em>candidate_set</em> для управления всеми векторами-кандидатами для вычисления расстояния.</p>
<p>Каждый вектор-кандидат содержит значительные метаданные в дополнение к данным вектора. В результате при загрузке кандидата из памяти он может вызывать пропуски кэша данных, что приводит к задержкам в обработке. Наша оптимизация вводит программную предварительную выборку, чтобы проактивно загружать следующего кандидата во время обработки текущего. Это усовершенствование позволило повысить производительность на 2-3 % при векторном поиске сходства в одном экземпляре Redis. Патч находится в процессе апстриминга.</p>
<h2 id="GCC-Default-Behavior-Change-to-Prevent-Mixed-Assembly-Code-Penalties" class="common-anchor-header">Изменение поведения GCC по умолчанию для предотвращения штрафов за смешанный ассемблерный код<button data-href="#GCC-Default-Behavior-Change-to-Prevent-Mixed-Assembly-Code-Penalties" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Для достижения максимальной производительности часто используемые участки кода часто пишутся на ассемблере вручную. Однако, когда разные сегменты кода написаны разными людьми или в разное время, используемые инструкции могут быть из несовместимых наборов инструкций ассемблера, таких как <a href="https://www.intel.com/content/www/us/en/architecture-and-technology/avx-512-overview.html">Intel® Advanced Vector Extensions 512 (Intel® AVX-512)</a> и <a href="https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions">Streaming SIMD Extensions (SSE)</a>. Если смешанный код не скомпилировать должным образом, он приведет к снижению производительности. <a href="https://www.intel.com/content/dam/develop/external/us/en/documents/11mc12-avoiding-2bavx-sse-2btransition-2bpenalties-2brh-2bfinal-809104.pdf">Подробнее о смешивании инструкций Intel AVX и SSE можно узнать здесь</a>.</p>
<p>Вы можете легко определить, используете ли вы смешанный режим ассемблера и не скомпилировали ли вы код с <em>VZEROUPPER</em>, что приводит к снижению производительности. Это можно заметить с помощью команды perf, например <em>sudo perf stat -e 'assists.sse_avx_mix/event/event=0xc1,umask=0x10/' &lt;workload&gt;.</em> Если в вашей ОС нет поддержки этого события, используйте <em>cpu/event=0xc1,umask=0x10,name=assists_sse_avx_mix/</em>.</p>
<p>Компилятор Clang по умолчанию вставляет <em>VZEROUPPER</em>, что позволяет избежать штрафа за смешанный режим. Но компилятор GCC вставляет <em>VZEROUPPER</em> только в том случае, если указаны флаги компилятора -O2 или -O3. Мы связались с командой GCC и объяснили проблему, и теперь по умолчанию они корректно обрабатывают ассемблерный код в смешанном режиме.</p>
<h2 id="Start-Optimizing-Your-Vector-Databases" class="common-anchor-header">Начните оптимизировать свои векторные базы данных<button data-href="#Start-Optimizing-Your-Vector-Databases" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Векторные базы данных играют неотъемлемую роль в GenAI, и они становятся все больше и больше, чтобы генерировать более качественные ответы. Что касается оптимизации, то приложения ИИ ничем не отличаются от других программных приложений, поскольку они раскрывают свои секреты при использовании стандартных инструментов анализа производительности, а также бенчмарков и стресс-факторов.</p>
<p>Используя эти инструменты, мы обнаружили ловушки производительности, связанные с излишним выделением памяти, неспособностью выполнять предварительную выборку инструкций и использованием неправильных опций компилятора. Основываясь на полученных результатах, мы внесли улучшения в Milvus, Knowhere, Redis и компилятор GCC, чтобы сделать ИИ немного более производительным и устойчивым. Векторные базы данных - важный класс приложений, достойный ваших усилий по оптимизации. Мы надеемся, что эта статья поможет вам начать работу.</p>
