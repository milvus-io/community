{"codeList":["from txtai.pipeline import HFOnnx\n\npath = \"sentence-transformers/all-MiniLM-L6-v2\"\nonnx_model = HFOnnx()\nmodel = onnx_model(path, \"pooling\", \"model.onnx\", True)\n","from onnxruntime_extensions import gen_processing_models\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer('all-MiniLM-L6-v2')\ntok_encode, _ = gen_processing_models(embedding_model.tokenizer, pre_kwargs={})\n\nonnx_tokenizer_path = \"tokenizer.onnx\"\nwith open(tokenizer_path, \"wb\") as f:\n  f.write(tok_encode.SerializeToString())\n","// Imports required for Java/ONNX integration\nimport ai.onnxruntime.*;\nimport ai.onnxruntime.extensions.*;\n\n…\n\n// Set up inference sessions for tokenizer and model\nvar env = OrtEnvironment.getEnvironment();\n\nvar sess_opt = new OrtSession.SessionOptions();\nsess_opt.registerCustomOpLibrary(OrtxPackage.getLibraryPath());\n\nvar tokenizer = env.createSession(\"app/tokenizer.onnx\", sess_opt);\nvar model = env.createSession(\"app/model.onnx\", sess_opt);\n\n…\n\n// Perform inference and extract text embeddings into native Java\nvar results = session.run(inputs).get(\"embeddings\");\nfloat[][] embeddings = (float[][]) results.get().getValue();\n"],"headingContent":"","anchorList":[{"label":"Что такое ONNX (Open Neural Network Exchange)?","href":"What-is-ONNX-Open-Neural-Network-Exchange","type":2,"isActive":false},{"label":"Рабочий процесс ONNX","href":"ONNX-Workflow","type":2,"isActive":false},{"label":"Экспорт NN-моделей из Python","href":"Exporting-NN-Models-from-Python","type":2,"isActive":false},{"label":"Вывод модели в Java","href":"Model-Inference-in-Java","type":2,"isActive":false},{"label":"Резюме","href":"Summary","type":2,"isActive":false},{"label":"Ресурсы","href":"Resources","type":2,"isActive":false}]}