---
id: >-
  embedding-first-chunking-second-smarter-rag-retrieval-with-max-min-semantic-chunking.md
title: >-
  Сначала встраивание, потом расчленение: Более разумный поиск по RAG с помощью
  Max-Min семантического расчленения
author: Rachel Liu
date: 2025-12-24T00:00:00.000Z
cover: assets.zilliz.com/maxmin_cover_8be0b87409.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'Max–Min Semantic Chunking, Milvus, RAG, chunking strategies'
meta_title: |
  Max–Min Semantic Chunking: Top Chunking Strategy to Improve RAG Performance
desc: >-
  Узнайте, как Max-Min Semantic Chunking повышает точность RAG с помощью
  подхода, основанного на встраивании, который создает более интеллектуальные
  фрагменты, улучшает качество контекста и обеспечивает более высокую
  производительность поиска.
origin: >-
  https://milvus.io/blog/embedding-first-chunking-second-smarter-rag-retrieval-with-max-min-semantic-chunking.md
---
<p>Технология<a href="https://zilliz.com/learn/Retrieval-Augmented-Generation">Retrieval Augmented Generation (RAG)</a> стала стандартным подходом к обеспечению контекста и памяти для приложений ИИ - на нее полагаются агенты ИИ, ассистенты поддержки клиентов, базы знаний и поисковые системы.</p>
<p>Почти в каждом конвейере RAG стандартный процесс одинаков: берем документы, разбиваем их на фрагменты, а затем встраиваем эти фрагменты для поиска по сходству в векторную базу данных, например <a href="https://milvus.io/">Milvus</a>. Поскольку <strong>разбивка</strong> происходит заранее, качество этих фрагментов напрямую влияет на то, насколько хорошо система извлекает информацию и насколько точными получаются конечные ответы.</p>
<p>Проблема заключается в том, что традиционные стратегии разбиения на куски обычно разбивают текст без какого-либо семантического понимания. При разбиении текста на куски фиксированной длины учитывается количество лексем, а при рекурсивном разбиении используется структура поверхностного уровня, но в обоих случаях не учитывается реальный смысл текста. В результате связанные идеи часто оказываются разделенными, несвязанные строки - сгруппированными, а важный контекст - разрозненным.</p>
<p>В этом блоге я хочу рассказать о другой стратегии разбиения текста на части: <a href="https://link.springer.com/article/10.1007/s10791-025-09638-7"><strong>Max-Min Semantic Chunking</strong></a>. Вместо того чтобы сначала разбивать текст на куски, она встраивает его в текст и использует семантическое сходство, чтобы решить, где должны формироваться границы. Благодаря встраиванию перед разрезанием, конвейер может отслеживать естественные изменения смысла, а не полагаться на произвольные ограничения по длине.</p>
<h2 id="How-a-Typical-RAG-Pipeline-Works" class="common-anchor-header">Принцип работы типичного конвейера RAG<button data-href="#How-a-Typical-RAG-Pipeline-Works" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Большинство конвейеров RAG, независимо от фреймворка, следуют одному и тому же четырехэтапному конвейеру. Вероятно, вы и сами написали какую-то версию этого конвейера:</p>
<h3 id="1-Data-Cleaning-and-Chunking" class="common-anchor-header">1. Очистка и разбивка данных</h3><p>Конвейер начинается с очистки исходных документов: удаляются заголовки, нижние колонтитулы, навигационный текст и все, что не является реальным содержимым. Как только шум удален, текст разбивается на более мелкие фрагменты. Большинство команд используют фрагменты фиксированного размера - обычно 300-800 лексем, - поскольку это позволяет сделать модель встраивания управляемой. Недостатком является то, что разбиение происходит по длине, а не по смыслу, поэтому границы могут быть произвольными.</p>
<h3 id="2-Embedding-and-Storage" class="common-anchor-header">2. Встраивание и хранение</h3><p>Затем каждый чанк встраивается с помощью модели встраивания, например, OpenAI <a href="https://zilliz.com/ai-models/text-embedding-3-small"><code translate="no">text-embedding-3-small</code></a> или кодировщика BAAI. Полученные векторы хранятся в базе данных векторов, например <a href="https://milvus.io/">Milvus</a> или <a href="https://zilliz.com/cloud">Zilliz Cloud</a>. База данных обрабатывает индексацию и поиск сходства, чтобы вы могли быстро сравнивать новые запросы со всеми сохраненными чанками.</p>
<h3 id="3-Querying" class="common-anchor-header">3. Запрос</h3><p>Когда пользователь задает вопрос - например, <em>"Как RAG уменьшает галлюцинации?"</em> - система встраивает запрос и отправляет его в базу данных. База данных возвращает топ-K фрагментов, чьи векторы наиболее близки к запросу. Именно на эти фрагменты текста будет опираться модель при ответе на вопрос.</p>
<h3 id="4-Answer-Generation" class="common-anchor-header">4. Генерация ответа</h3><p>Полученные фрагменты объединяются с запросом пользователя и поступают в LLM. Модель генерирует ответ, используя предоставленный контекст в качестве основы.</p>
<p><strong>Чанкинг находится в самом начале всего этого конвейера, но он имеет огромное влияние</strong>. Если фрагменты соответствуют естественному смыслу текста, поиск будет точным и последовательным. Если же фрагменты были вырезаны в неудобных местах, системе будет сложнее найти нужную информацию, даже при наличии сильных вкраплений и быстрой векторной базы данных.</p>
<h3 id="The-Challenges-of-Getting-Chunking-Right" class="common-anchor-header">Трудности правильного использования чанков</h3><p>Большинство систем RAG сегодня используют один из двух основных методов разбиения на куски, оба из которых имеют свои ограничения.</p>
<p><strong>1. Куски фиксированного размера</strong></p>
<p>Это самый простой подход: разбиение текста по фиксированному количеству лексем или символов. Он быстр и предсказуем, но совершенно не учитывает грамматику, темы и переходы. Предложения могут сокращаться вдвое. Иногда даже слова. Вложения, которые вы получаете из этих кусков, как правило, шумные, потому что границы не отражают реальную структуру текста.</p>
<p><strong>2. Рекурсивное разбиение символов</strong></p>
<p>Этот метод немного умнее. Он делит текст на иерархические части, основываясь на таких признаках, как абзацы, переносы строк или предложения. Если раздел слишком длинный, он рекурсивно делит его дальше. В целом результат получается более последовательным, но все равно непоследовательным. Некоторые документы не имеют четкой структуры или имеют неравномерную длину разделов, что снижает точность поиска. А в некоторых случаях этот подход все равно выдает фрагменты, которые превышают контекстное окно модели.</p>
<p>Оба метода сталкиваются с одним и тем же компромиссом: точность против контекста. Маленькие фрагменты повышают точность поиска, но теряют окружающий контекст; большие фрагменты сохраняют смысл, но рискуют добавить нерелевантный шум. Нахождение правильного баланса - это то, что делает куски одновременно основополагающими и разочаровывающими при разработке систем RAG.</p>
<h2 id="Max–Min-Semantic-Chunking-Embed-First-Chunk-Later" class="common-anchor-header">Максимально-минимальный семантический чанкинг: Сначала встроить, потом разбить<button data-href="#Max–Min-Semantic-Chunking-Embed-First-Chunk-Later" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В 2025 году С.Р. Бхат и др. опубликовали статью <a href="https://arxiv.org/abs/2505.21700"><em>Rethinking Chunk Size for Long-Document Retrieval: A Multi-Dataset Analysis</em></a>. Один из ключевых выводов, к которому они пришли, заключается в том, что не существует единого <strong>"оптимального"</strong> размера куска для RAG. Маленькие куски (64-128 лексем), как правило, лучше работают с фактологическими вопросами или вопросами типа поиска, в то время как большие куски (512-1024 лексем) помогают при решении задач повествования или высокоуровневых рассуждений. Другими словами, разбивка на части фиксированного размера - это всегда компромисс.</p>
<p>В связи с этим возникает естественный вопрос: вместо того чтобы выбирать одну длину и надеяться на лучшее, можем ли мы разбивать фрагменты по смыслу, а не по размеру? <a href="https://link.springer.com/article/10.1007/s10791-025-09638-7"><strong>Max-Min Semantic Chunking</strong></a> - один из найденных мною подходов, который пытается сделать именно это.</p>
<p>Идея проста: <strong>сначала встраиваем, потом разбиваем</strong>. Вместо того чтобы разбивать текст на части, а затем встраивать все выпавшие куски, алгоритм встраивает <em>все предложения</em> вперед. Затем он использует семантические связи между этими вложениями предложений, чтобы решить, где должны проходить границы.</p>
<p>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/embed_first_chunk_second_94f69c664c.png" alt="Diagram showing embed-first chunk-second workflow in Max-Min Semantic Chunking" class="doc-image" id="diagram-showing-embed-first-chunk-second-workflow-in-max-min-semantic-chunking" />
   </span> <span class="img-wrapper"> <span>Диаграмма, показывающая рабочий процесс embed-first chunk-second в Max-Min Semantic Chunking</span> </span></p>
<p>С концептуальной точки зрения, метод рассматривает разбиение на части как ограниченную задачу кластеризации в пространстве вкраплений. Вы проходите документ по порядку, по одному предложению за раз. Для каждого предложения алгоритм сравнивает его вложения с вложениями в текущем куске. Если новое предложение достаточно близко по семантике, оно присоединяется к куску. Если оно слишком далеко, алгоритм начинает новый чанк. Ключевым ограничением является то, что чанки должны следовать оригинальному порядку предложений - никакого переупорядочивания, никакой глобальной кластеризации.</p>
<p>В результате получается набор фрагментов переменной длины, которые отражают те места, где смысл документа действительно меняется, а не те, где счетчик символов случайно достиг нуля.</p>
<h2 id="How-the-Max–Min-Semantic-Chunking-Strategy-Works" class="common-anchor-header">Принцип работы стратегии Max-Min Semantic Chunking<button data-href="#How-the-Max–Min-Semantic-Chunking-Strategy-Works" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Max-Min Semantic Chunking определяет границы фрагментов, сравнивая, как предложения соотносятся друг с другом в высокоразмерном векторном пространстве. Вместо того чтобы полагаться на фиксированную длину, он смотрит на то, как меняется смысл в документе. Процесс можно разбить на шесть этапов:</p>
<h3 id="1-Embed-all-sentences-and-start-a-chunk" class="common-anchor-header">1. Встраивание всех предложений и создание фрагмента</h3><p>Модель встраивания преобразует каждое предложение в документе в векторное вложение. Она обрабатывает предложения по порядку. Если первые <em>n-k</em> предложений образуют текущий чанк C, то следующее предложение (sₙ₋ₖ₊₁) должно быть оценено: должно ли оно присоединиться к C или начать новый чанк?</p>
<h3 id="2-Measure-how-consistent-the-current-chunk-is" class="common-anchor-header">2. Измерьте, насколько последовательным является текущий чанк.</h3><p>Внутри чанка C рассчитайте минимальное парное косинусное сходство между всеми вкраплениями предложений. Это значение отражает, насколько тесно связаны между собой предложения в этом блоке. Более низкое минимальное сходство указывает на то, что предложения менее связаны между собой, что говорит о необходимости разделения блока.</p>
<h3 id="3-Compare-the-new-sentence-to-the-chunk" class="common-anchor-header">3. Сравните новое предложение с фрагментом.</h3><p>Далее рассчитайте максимальное косинусное сходство между новым предложением и любым предложением, уже содержащимся в C. Это отражает, насколько хорошо новое предложение семантически соответствует существующему блоку.</p>
<h3 id="4-Decide-whether-to-extend-the-chunk-or-start-a-new-one" class="common-anchor-header">4. Решите, стоит ли расширять чанк или начинать новый.</h3><p>Это основное правило:</p>
<ul>
<li><p>Если <strong>максимальное сходство нового предложения</strong> с чанком <strong>C</strong> <strong>больше или равно</strong> <strong>минимальному сходству внутри C</strong>, → новое предложение вписывается в чанк и остается в нем.</p></li>
<li><p>В противном случае → начинаем новый чанк.</p></li>
</ul>
<p>Это гарантирует, что каждый чанк сохраняет внутреннюю семантическую согласованность.</p>
<h3 id="5-Adjust-thresholds-as-the-document-changes" class="common-anchor-header">5. Регулировка пороговых значений по мере изменения документа</h3><p>Для оптимизации качества чанков такие параметры, как размер чанка и пороги сходства, можно динамически корректировать. Это позволяет алгоритму адаптироваться к изменяющимся структурам документов и семантической плотности.</p>
<h3 id="6-Handle-the-first-few-sentences" class="common-anchor-header">6. Обработка первых нескольких предложений</h3><p>Если чанк содержит только одно предложение, алгоритм обрабатывает первое сравнение, используя фиксированный порог сходства. Если сходство между предложением 1 и предложением 2 выше этого порога, они образуют чанк. Если нет, они сразу же разделяются.</p>
<h2 id="Strengths-and-Limitations-of-Max–Min-Semantic-Chunking" class="common-anchor-header">Сильные и слабые стороны семантического разбиения по методу Max-Min<button data-href="#Strengths-and-Limitations-of-Max–Min-Semantic-Chunking" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Max-Min Semantic Chunking улучшает работу систем RAG по разбиению текста, используя смысл вместо длины, но это не серебряная пуля. Вот практический взгляд на то, что он делает хорошо, и на то, где он все еще не работает.</p>
<h3 id="What-It-Does-Well" class="common-anchor-header">Что он делает хорошо</h3><p>Max-Min Semantic Chunking улучшает традиционную разбивку на части тремя важными способами:</p>
<h4 id="1-Dynamic-meaning-driven-chunk-boundaries" class="common-anchor-header"><strong>1. Динамические, управляемые смыслом границы чанков</strong></h4><p>В отличие от подходов, основанных на фиксированном размере или структуре, этот метод опирается на семантическое сходство для определения границ кусков. Он сравнивает минимальное сходство внутри текущего куска (насколько он связный) с максимальным сходством между новым предложением и этим куском (насколько хорошо оно вписывается). Если последний показатель выше, предложение присоединяется к куску, в противном случае начинается новый кусок.</p>
<h4 id="2-Simple-practical-parameter-tuning" class="common-anchor-header"><strong>2. Простая и практичная настройка параметров</strong></h4><p>Алгоритм зависит всего от трех основных гиперпараметров:</p>
<ul>
<li><p><strong>максимальный размер куска</strong>,</p></li>
<li><p><strong>минимальное сходство</strong> между первыми двумя предложениями и</p></li>
<li><p><strong>порог сходства</strong> для добавления новых предложений.</p></li>
</ul>
<p>Эти параметры настраиваются автоматически в зависимости от контекста - для сохранения связности большие куски требуют более жестких порогов сходства.</p>
<h4 id="3-Low-processing-overhead" class="common-anchor-header"><strong>3. Низкие накладные расходы на обработку</strong></h4><p>Поскольку конвейер RAG уже вычисляет вкрапления предложений, Max-Min Semantic Chunking не требует больших вычислений. Все, что ему нужно, - это набор проверок косинусного сходства при сканировании предложений. Это делает его дешевле многих методов семантического расщепления, требующих дополнительных моделей или многоступенчатой кластеризации.</p>
<h3 id="What-It-Still-Can’t-Solve" class="common-anchor-header">Что она все еще не может решить</h3><p>Max-Min Semantic Chunking улучшает границы фрагментов, но не устраняет все проблемы сегментации документов. Поскольку алгоритм обрабатывает предложения по порядку и кластеризует только локально, он все равно может упустить дальние связи в длинных или более сложных документах.</p>
<p>Одна из распространенных проблем - <strong>фрагментация контекста</strong>. Когда важная информация распределена по разным частям документа, алгоритм может поместить эти части в отдельные куски. При этом каждый кусок несет в себе лишь часть смысла.</p>
<p>Например, в примечаниях к выпуску Milvus 2.4.13, как показано ниже, один фрагмент может содержать идентификатор версии, а другой - список функций. Запрос типа <em>"Какие новые функции были представлены в Milvus 2.4.13?"</em> зависит от обоих. Если эти сведения разделены по разным фрагментам, модель встраивания может не связать их, что приведет к ухудшению поиска.</p>
<ul>
<li>
  
   <span class="img-wrapper"> <img translate="no" src="https://assets.zilliz.com/v2413_a98e1b1f99.png" alt="Example showing context fragmentation in Milvus 2.4.13 Release Notes with version identifier and feature list in separate chunks" class="doc-image" id="example-showing-context-fragmentation-in-milvus-2.4.13-release-notes-with-version-identifier-and-feature-list-in-separate-chunks" />
   </span> <span class="img-wrapper"> <span>Пример фрагментации контекста в Milvus 2.4.13 Release Notes с идентификатором версии и списком функций в отдельных фрагментах</span> </span></li>
</ul>
<p>Эта фрагментация также влияет на этап генерации LLM. Если ссылка на версию находится в одном чанке, а описание характеристик - в другом, модель получает неполный контекст и не может точно определить взаимосвязь между ними.</p>
<p>Для смягчения последствий таких случаев системы часто используют такие техники, как скользящие окна, перекрывающиеся границы чанков или многопроходное сканирование. Эти подходы позволяют восстановить часть недостающего контекста, уменьшить фрагментацию и помочь этапу поиска сохранить связанную информацию.</p>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Max-Min Semantic Chunking не является волшебным решением всех проблем RAG, но он дает нам более разумный способ думать о границах фрагментов. Вместо того чтобы позволять ограничениям на лексемы решать, где идеи будут измельчаться, он использует вкрапления, чтобы определить, где смысл действительно меняется. Для многих реальных документов - API, спецификаций, журналов, заметок о выпуске, руководств по устранению неполадок - только это может заметно повысить качество поиска.</p>
<p>Что мне нравится в этом подходе, так это то, что он естественно вписывается в существующие конвейеры RAG. Если вы уже внедряете предложения или абзацы, дополнительные затраты сводятся к нескольким проверкам косинусного сходства. Вам не нужны дополнительные модели, сложная кластеризация или тяжелая предварительная обработка. И когда этот метод работает, создаваемые им фрагменты кажутся более "человеческими" - более близкими к тому, как мы мысленно группируем информацию при чтении.</p>
<p>Но у метода все еще есть "слепые пятна". Он видит смысл только локально и не может соединить информацию, которая намеренно разнесена в разные стороны. Перекрывающиеся окна, многопроходное сканирование и другие трюки, сохраняющие контекст, все еще необходимы, особенно для документов, где ссылки и объяснения находятся далеко друг от друга.</p>
<p>Тем не менее, Max-Min Semantic Chunking двигает нас в правильном направлении: от произвольной нарезки текста к поисковым конвейерам, которые действительно уважают семантику. Если вы изучаете способы сделать RAG более надежным, с этим стоит поэкспериментировать.</p>
<p>У вас есть вопросы или вы хотите глубже изучить вопрос повышения производительности RAG? Присоединяйтесь к нашему <a href="https://discord.com/invite/8uyFbECzPX">Discord</a> и общайтесь с инженерами, которые каждый день создают и настраивают реальные поисковые системы.</p>
