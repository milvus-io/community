---
id: Building-an-AI-Powered-Writing-Assistant-with-WPS-Office.md
title: Создание письменного помощника с искусственным интеллектом для WPS Office
author: milvus
date: 2020-07-28T03:35:40.105Z
desc: >-
  Узнайте, как компания Kingsoft использовала Milvus, систему поиска сходств с
  открытым исходным кодом, для создания системы рекомендаций для помощника по
  написанию текстов WPS Office с поддержкой искусственного интеллекта.
cover: assets.zilliz.com/wps_thumbnail_6cb7876963.jpg
tag: Scenarios
canonicalUrl: >-
  https://zilliz.com/blog/Building-an-AI-Powered-Writing-Assistant-with-WPS-Office
---
<custom-h1>Создание помощника по написанию текстов на основе искусственного интеллекта для WPS Office</custom-h1><p>WPS Office - это инструмент для повышения производительности, разработанный компанией Kingsoft и имеющий более 150 млн пользователей по всему миру. Отдел искусственного интеллекта (ИИ) компании с нуля создал умного помощника для письма, используя алгоритмы семантического соответствия, такие как распознавание намерений и кластеризация текста. Инструмент существует как в виде веб-приложения, так и в виде <a href="https://walkthechat.com/wechat-mini-programs-simple-introduction/">мини-программы для WeChat</a>. Он помогает пользователям быстро создавать конспекты, отдельные абзацы и целые документы, просто введя заголовок и выбрав до пяти ключевых слов.</p>
<p>Рекомендательный механизм помощника по написанию текстов использует Milvus, поисковую систему с открытым исходным кодом для поиска сходств, в качестве основного модуля обработки векторов. Ниже мы рассмотрим процесс создания интеллектуального помощника WPS Offices, в том числе процесс извлечения характеристик из неструктурированных данных, а также роль Milvus в хранении данных и работе рекомендательного механизма инструмента.</p>
<p>Перейти к:</p>
<ul>
<li><a href="#building-an-ai-powered-writing-assistant-for-wps-office">Создание помощника по написанию текстов на базе искусственного интеллекта для WPS Office</a><ul>
<li><a href="#making-sense-of-unstructured-textual-data">Осмысление неструктурированных текстовых данных</a></li>
<li><a href="#using-the-tfidf-model-to-maximize-feature-extraction">Использование модели TFIDF для максимального извлечения признаков</a></li>
<li><a href="#extracting-features-with-the-bi-directional-lstm-cnns-crf-deep-learning-model">Извлечение признаков с помощью двунаправленной модели глубокого обучения LSTM-CNNs-CRF</a></li>
<li><a href="#creating-sentence-embeddings-using-infersent">Создание вкраплений предложений с помощью Infersent</a></li>
<li><a href="#storing-and-querying-vectors-with-milvus">Хранение и запрос векторов с помощью Milvus</a></li>
<li><a href="#ai-isnt-replacing-writers-its-helping-them-write">ИИ не заменяет писателей, а помогает им писать</a></li>
</ul></li>
</ul>
<h3 id="Making-sense-of-unstructured-textual-data" class="common-anchor-header">Осмысление неструктурированных текстовых данных</h3><p>Как и любая другая современная задача, требующая решения, создание помощника по написанию текстов WPS началось с неструктурированных данных. Точнее, с десятков миллионов плотных текстовых документов, из которых необходимо извлечь значимые характеристики. Чтобы понять сложность этой задачи, рассмотрим, как два журналиста из разных новостных изданий могут писать репортажи на одну и ту же тему.</p>
<p>Хотя оба они будут придерживаться правил, принципов и процессов, регулирующих структуру предложений, они будут по-разному выбирать слова, создавать предложения разной длины и использовать собственные структуры статей, чтобы рассказать похожие (а может, и разные) истории. В отличие от структурированных наборов данных с фиксированным числом измерений, тела текстов по своей сути не имеют структуры, поскольку синтаксис, управляющий ими, очень податлив. Чтобы найти смысл, из неструктурированного корпуса документов необходимо извлечь машиночитаемые признаки. Но сначала данные необходимо очистить.</p>
<p>Существует множество способов очистки текстовых данных, ни один из которых не будет подробно рассмотрен в этой статье. Тем не менее, это важный этап, который предшествует обработке данных и может включать удаление тегов, удаление символов ударения, расширение сокращений, удаление специальных символов, удаление стоп-слов и многое другое. Подробное описание методов предварительной обработки и очистки текстовых данных можно найти <a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">здесь</a>.</p>
<h3 id="Using-the-TFIDF-model-to-maximize-feature-extraction" class="common-anchor-header">Использование модели TFIDF для максимального извлечения признаков</h3><p>Чтобы начать осмысление неструктурированных текстовых данных, к корпусу, из которого черпает информацию помощник по написанию WPS, была применена модель "частота терминов - обратная частота документов" (TFIDF). Эта модель использует комбинацию двух метрик, частоты терминов и обратной частоты документов, чтобы присвоить каждому слову в документе значение TFIDF. Частота терминов (TF) - это количество терминов в документе, деленное на общее количество терминов в документе, а обратная частота документов (IDF) - это количество документов в корпусе, деленное на количество документов, в которых встречается данный термин.</p>
<p>Произведение TF и IDF дает оценку того, насколько часто термин встречается в документе, умноженную на уникальность слова в корпусе. В конечном итоге, значения TFIDF являются мерой того, насколько релевантно слово документу в коллекции документов. Термины сортируются по значениям TFIDF, и те из них, которые имеют низкие значения (т. е. распространенные слова), могут иметь меньший вес при использовании глубокого обучения для извлечения признаков из корпуса.</p>
<h3 id="Extracting-features-with-the-bi-directional-LSTM-CNNs-CRF-deep-learning-model" class="common-anchor-header">Извлечение признаков с помощью двунаправленной модели глубокого обучения LSTM-CNNs-CRF</h3><p>Используя комбинацию двунаправленной долговременной памяти (BLSTM), сверточных нейронных сетей (CNN) и условных случайных полей (CRF), можно извлекать из корпуса представления на уровне слов и символов. <a href="https://arxiv.org/pdf/1603.01354.pdf">Модель BLSTM-CNNs-CRF</a>, используемая для создания помощника по письму WPS Office, работает следующим образом:</p>
<ol>
<li><strong>CNN:</strong> В качестве входных данных для CNN используются вкрапления символов, затем извлекаются семантически значимые структуры слов (например, префикс или суффикс) и кодируются в векторы представлений на уровне символов.</li>
<li><strong>BLSTM:</strong> векторы на уровне символов конкатенируются с векторами вкраплений слов, а затем поступают в сеть BLSTM. Каждая последовательность представляется в прямом и обратном направлении в два отдельных скрытых состояния, чтобы уловить прошлую и будущую информацию.</li>
<li><strong>CRF:</strong> выходные векторы из BLSTM подаются на слой CRF для совместного декодирования наилучшей последовательности меток.</li>
</ol>
<p>Теперь нейронная сеть способна извлекать и классифицировать именованные сущности из неструктурированного текста. Этот процесс называется <a href="https://en.wikipedia.org/wiki/Named-entity_recognition">распознаванием именованных сущностей (NER)</a> и включает в себя нахождение и классификацию таких категорий, как имена людей, учреждения, географические объекты и т. д. Эти сущности играют важную роль в сортировке и запоминании данных. Отсюда можно извлекать из корпуса ключевые предложения, абзацы и резюме.</p>
<h3 id="Creating-sentence-embeddings-using-Infersent" class="common-anchor-header">Создание вкраплений предложений с помощью Infersent</h3><p><a href="https://github.com/facebookresearch/InferSent">Infersent</a>, метод встраивания предложений под наблюдением, разработанный Facebook, который встраивает полные предложения в векторное пространство, используется для создания векторов, которые будут введены в базу данных Milvus. Для обучения Infersent использовался корпус Stanford Natural Language Inference (SNLI), содержащий 570 тысяч пар предложений, которые были написаны и помечены людьми. Дополнительную информацию о работе Infersent можно найти <a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">здесь</a>.</p>
<h3 id="Storing-and-querying-vectors-with-Milvus" class="common-anchor-header">Хранение и запрос векторов с помощью Milvus</h3><p><a href="https://www.milvus.io/">Milvus</a> - это поисковая система с открытым исходным кодом, которая поддерживает добавление, удаление, обновление и поиск вкраплений в масштабе триллиона байт практически в режиме реального времени. Для повышения производительности запросов Milvus позволяет указывать тип индекса для каждого поля вектора. Интеллектуальный помощник WPS Office использует индекс IVF_FLAT, самый базовый тип индекса Inverted File (IVF), где "flat" означает, что векторы хранятся без сжатия или квантования. Кластеризация основана на IndexFlat2, который использует точный поиск для расстояния L2.</p>
<p>Несмотря на то что IVF_FLAT имеет 100-процентный коэффициент отзыва запросов, отсутствие сжатия приводит к сравнительно низкой скорости запросов. <a href="https://milvus.io/docs/manage-partitions.md">Функция разбиения</a> Milvus используется для разделения данных на несколько частей физического хранилища на основе заданных правил, что делает запросы более быстрыми и точными. Когда векторы добавляются в Milvus, теги указывают, в какой раздел следует добавить данные. Запросы к векторным данным используют теги, чтобы указать, в каком разделе должен выполняться запрос. Данные могут быть дополнительно разбиты на сегменты внутри каждого раздела для дальнейшего повышения скорости.</p>
<p>Интеллектуальный помощник в написании текстов также использует кластеры Kubernetes, позволяющие запускать контейнеры приложений на нескольких машинах и в разных средах, а также MySQL для управления метаданными.</p>
<h3 id="AI-isn’t-replacing-writers-it’s-helping-them-write" class="common-anchor-header">ИИ не заменяет писателей, а помогает им писать</h3><p>Помощник написания текстов Kingsoft для WPS Office использует Milvus для управления базой данных, содержащей более 2 миллионов документов. Система обладает высокой гибкостью и способна осуществлять поиск в триллионных массивах данных практически в режиме реального времени. Запросы выполняются в среднем за 0,2 секунды, что означает, что целые документы могут быть созданы практически мгновенно, используя только название или несколько ключевых слов. Хотя ИИ не заменит профессиональных писателей, существующие сегодня технологии способны дополнить процесс написания текстов новыми и интересными способами. Будущее неизвестно, но, по крайней мере, писатели могут рассчитывать на более продуктивные, а для некоторых и менее сложные методы "прикладывания пера к бумаге".</p>
<p>При написании этой статьи были использованы следующие источники:</p>
<ul>
<li>"<a href="https://arxiv.org/pdf/1603.01354.pdf">End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF</a>", Xuezhe Ma и Eduard Hovy.</li>
<li>"<a href="https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41">Традиционные методы для текстовых данных</a>", Дипанджан (DJ) Саркар.</li>
<li>"<a href="https://ieeexplore.ieee.org/document/8780663">Извлечение текстовых признаков на основе ассоциативной семантики TF-IDF</a>", Цин Лю, Цзин Ван, Дехай Чжан, Юнь Ян, НайЯо Ван.</li>
<li>"<a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001">Понимание вкраплений предложений с помощью Infersent от Facebook</a>", Рехан Ахмад.</li>
<li>"<a href="https://arxiv.org/pdf/1705.02364.pdf">Контролируемое обучение универсальным представлениям предложений на основе данных естественного языка</a>", Алексис Конно, Дуве Кила, Хольгер Швенк, Лопк Барро, Антуан Бордес.V1</li>
</ul>
<p>Читайте другие <a href="https://zilliz.com/user-stories">истории пользователей</a>, чтобы узнать больше о создании вещей с помощью Milvus.</p>
