---
id: >-
  we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md
title: >-
  Мы провели бенчмаркинг 20+ API для встраивания с помощью Milvus: 7 фактов,
  которые вас удивят
author: Jeremy Zhu
date: 2025-05-23T00:00:00.000Z
desc: >-
  Самые популярные API для встраивания - не самые быстрые. География имеет
  большее значение, чем архитектура модели. И иногда процессор за 20 долларов в
  месяц выигрывает у API за 200 долларов в месяц.
cover: >-
  assets.zilliz.com/We_Benchmarked_20_Embedding_AP_Is_with_Milvus_7_Insights_That_Will_Surprise_You_12268622f0.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database, vector search'
meta_keywords: 'Milvus, Embedding API, RAG, latency, vector search'
meta_title: >
  We Benchmarked 20+ Embedding APIs with Milvus: 7 Insights That Will Surprise
  You
origin: >-
  https://milvus.io/blog/we-benchmarked-20-embedding-apis-with-milvus-7-insights-that-will-surprise-you.md
---
<p><strong>Наверное, каждый разработчик ИИ создал систему RAG, которая работает идеально... в его локальной среде.</strong></p>
<p>Вы добились точности поиска, оптимизировали базу данных векторов, и ваша демонстрация работает как по маслу. Затем вы развертываете систему на производстве, и вдруг:</p>
<ul>
<li><p>Ваши локальные запросы, выполняемые за 200 мс, занимают у реальных пользователей 3 секунды.</p></li>
<li><p>Коллеги в разных регионах сообщают о совершенно разной производительности</p></li>
<li><p>Провайдер встраивания, который вы выбрали для "лучшей точности", становится вашим самым узким местом.</p></li>
</ul>
<p>Что случилось? Вот убийца производительности, который никто не оценивает: <strong>задержка API встраивания</strong>.</p>
<p>В то время как рейтинги MTEB зацикливаются на количестве отзывов и размерах моделей, они игнорируют метрику, которую на самом деле чувствуют ваши пользователи - сколько времени они ждут, прежде чем увидеть какой-либо ответ. Мы протестировали каждого крупного поставщика услуг встраивания в реальных условиях и обнаружили настолько сильные различия в задержках, что они заставят вас усомниться во всей стратегии выбора поставщика.</p>
<p><strong><em>Спойлер: Самые популярные API для встраивания - не самые быстрые. География имеет большее значение, чем архитектура модели. И иногда <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><annotation encoding="application/x-tex">процессор</annotation><mrow><mn>за</mn><mi>20/месяцCPUbeatsa20/месяц</mi></mrow><annotation encoding="application/x-tex">выигрывает</annotation></semantics></math></span></span>у</em></strong><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><strong><em> <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="mord mathnormal">20/месяцCPUbeatsa200/месяц</span></span></span></span>API-вызовов.</em></strong></p>
<h2 id="Why-Embedding-API-Latency-Is-the-Hidden-Bottleneck-in-RAG" class="common-anchor-header">Почему задержка API является скрытым узким местом в RAG<button data-href="#Why-Embedding-API-Latency-Is-the-Hidden-Bottleneck-in-RAG" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>При создании систем RAG, поиска в электронной коммерции или рекомендательных движков модели встраивания служат основным компонентом, преобразующим текст в векторы, что позволяет машинам понимать семантику и выполнять эффективный поиск по сходству. Хотя мы обычно предварительно вычисляем модели встраивания для библиотек документов, пользовательские запросы все равно требуют обращения к API для преобразования вопросов в векторы в реальном времени, и эта задержка в реальном времени часто становится узким местом во всей цепочке приложений.</p>
<p>Популярные бенчмарки встраивания, такие как MTEB, фокусируются на точности запоминания или размере модели, часто упуская из виду важнейшую метрику производительности - задержку API. Используя функцию Milvus <code translate="no">TextEmbedding</code> Function, мы провели комплексные реальные тесты на крупнейших поставщиках услуг встраивания в Северной Америке и Азии.</p>
<p>Задержка при внедрении проявляется на двух критических этапах:</p>
<h3 id="Query-Time-Impact" class="common-anchor-header">Влияние на время выполнения запроса</h3><p>В типичном рабочем процессе RAG, когда пользователь задает вопрос, система должна:</p>
<ul>
<li><p>преобразовать запрос в вектор с помощью вызова API для встраивания</p></li>
<li><p>Поиск похожих векторов в Milvus</p></li>
<li><p>Передать результаты и исходный вопрос в LLM</p></li>
<li><p>Сгенерировать и вернуть ответ.</p></li>
</ul>
<p>Многие разработчики полагают, что генерация ответа в LLM - это самая медленная часть. Однако возможность потокового вывода многих LLM создает иллюзию скорости - вы быстро видите первый токен. В действительности, если вызов API для встраивания занимает сотни миллисекунд или даже секунды, он становится первым и наиболее заметным узким местом в цепочке ответов.</p>
<h3 id="Data-Ingestion-Impact" class="common-anchor-header">Влияние загрузки данных</h3><p>Будь то создание индекса с нуля или выполнение регулярных обновлений, массовое встраивание требует векторизации тысяч или миллионов текстовых фрагментов. Если при каждом вызове встраивания возникает высокая задержка, весь конвейер данных резко замедляется, что приводит к задержке выпуска продуктов и обновления базы знаний.</p>
<p>Оба сценария делают задержку API для встраивания необязательным показателем производительности для производственных систем RAG.</p>
<h2 id="Measuring-Real-World-Embedding-API-Latency-with-Milvus" class="common-anchor-header">Измерение задержки встроенного API в реальном мире с помощью Milvus<button data-href="#Measuring-Real-World-Embedding-API-Latency-with-Milvus" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Milvus - это высокопроизводительная векторная база данных с открытым исходным кодом, которая предлагает новый интерфейс <code translate="no">TextEmbedding</code> Function. Эта функция интегрирует популярные модели встраивания от OpenAI, Cohere, AWS Bedrock, Google Vertex AI, Voyage AI и многих других поставщиков непосредственно в ваш конвейер данных, оптимизируя ваш конвейер векторного поиска с помощью одного вызова.</p>
<p>Используя этот новый функциональный интерфейс, мы протестировали и провели бенчмаркинг различных популярных API для встраивания от американских поставщиков моделей, таких как OpenAI и Cohere, а также от азиатских поставщиков, таких как AliCloud и SiliconFlow, измеряя их сквозную задержку в реалистичных сценариях развертывания.</p>
<p>Наш комплексный набор тестов охватывал различные конфигурации моделей:</p>
<table>
<thead>
<tr><th><strong>Провайдер</strong></th><th><strong>Модель</strong></th><th><strong>Размеры</strong></th></tr>
</thead>
<tbody>
<tr><td>OpenAI</td><td>текст-вставка-ада-002</td><td>1536</td></tr>
<tr><td>OpenAI</td><td>text-embedding-3-small</td><td>1536</td></tr>
<tr><td>OpenAI</td><td>текст-вставка-3-большой</td><td>3072</td></tr>
<tr><td>AWS Bedrock</td><td>amazon.titan-embed-text-v2:0</td><td>1024</td></tr>
<tr><td>Google Vertex AI</td><td>вставка текста-005</td><td>768</td></tr>
<tr><td>Google Vertex AI</td><td>текст-многоязычный-вставка-002</td><td>768</td></tr>
<tr><td>VoyageAI</td><td>voyage-3-large</td><td>1024</td></tr>
<tr><td>VoyageAI</td><td>вояж-3</td><td>1024</td></tr>
<tr><td>VoyageAI</td><td>voyage-3-lite</td><td>512</td></tr>
<tr><td>VoyageAI</td><td>voyage-code-3</td><td>1024</td></tr>
<tr><td>Cohere</td><td>embed-english-v3.0</td><td>1024</td></tr>
<tr><td>Cohere</td><td>embed-multilingual-v3.0</td><td>1024</td></tr>
<tr><td>Cohere</td><td>embed-english-light-v3.0</td><td>384</td></tr>
<tr><td>Cohere</td><td>embed-multilingual-light-v3.0</td><td>384</td></tr>
<tr><td>Aliyun Dashscope</td><td>text-embedding-v1</td><td>1536</td></tr>
<tr><td>Aliyun Dashscope</td><td>text-embedding-v2</td><td>1536</td></tr>
<tr><td>Aliyun Dashscope</td><td>text-embedding-v3</td><td>1024</td></tr>
<tr><td>Siliconflow</td><td>BAAI/bge-large-zh-v1.5</td><td>1024</td></tr>
<tr><td>Siliconflow</td><td>BAAI/bge-large-en-v1.5</td><td>1024</td></tr>
<tr><td>Siliconflow</td><td>netease-youdao/bce-embedding-base_v1</td><td>768</td></tr>
<tr><td>Siliconflow</td><td>BAAI/bge-m3</td><td>1024</td></tr>
<tr><td>Siliconflow</td><td>Pro/BAAI/bge-m3</td><td>1024</td></tr>
<tr><td>TEI</td><td>BAAI/bge-base-en-v1.5</td><td>768</td></tr>
</tbody>
</table>
<h2 id="7-Key-Findings-from-Our-Benchmarking-Results" class="common-anchor-header">7 Основные выводы по результатам бенчмаркинга<button data-href="#7-Key-Findings-from-Our-Benchmarking-Results" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Мы протестировали известные модели встраивания из Северной Америки и Азии при различных размерах пакетов, длине токенов и сетевых условиях, измеряя среднюю задержку во всех сценариях. Полученные нами результаты позволяют сделать критические выводы, которые изменят ваше представление о выборе и оптимизации API для встраивания. Давайте посмотрим.</p>
<h3 id="1-Global-Network-Effects-Are-More-Significant-Than-You-Think" class="common-anchor-header">1. Глобальные сетевые эффекты более значительны, чем вы думаете</h3><p>Сетевое окружение - это, пожалуй, самый важный фактор, влияющий на производительность API для встраивания. Один и тот же поставщик услуг встроенного API может работать совершенно по-разному в разных сетевых средах.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/latency_in_Asia_vs_in_US_cb4b5a425a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Если ваше приложение развернуто в Азии и обращается к таким сервисам, как OpenAI, Cohere или VoyageAI, развернутым в Северной Америке, задержки в сети значительно возрастают. Наши реальные тесты показали, что задержка вызовов API повсеместно увеличивается в <strong>3-4 раза</strong>!</p>
<p>И наоборот, если ваше приложение развернуто в Северной Америке и обращается к азиатским сервисам, таким как AliCloud Dashscope или SiliconFlow, снижение производительности будет еще более значительным. SiliconFlow, в частности, показал увеличение задержки <strong>почти в 100 раз</strong> в межрегиональных сценариях!</p>
<p>Это означает, что вы всегда должны выбирать провайдеров встраивания с учетом места развертывания и географии пользователей - заявления о производительности без сетевого контекста бессмысленны.</p>
<h3 id="2-Model-Performance-Rankings-Reveal-Surprising-Results" class="common-anchor-header">2. Рейтинг производительности моделей показывает удивительные результаты</h3><p>Наше комплексное тестирование задержек выявило четкую иерархию производительности:</p>
<ul>
<li><p><strong>Североамериканские модели (медианная задержка)</strong>: Cohere &gt; Google Vertex AI &gt; VoyageAI &gt; OpenAI &gt; AWS Bedrock.</p></li>
<li><p><strong>Азиатские модели (медианная задержка)</strong>: SiliconFlow &gt; AliCloud Dashscope</p></li>
</ul>
<p>Эти рейтинги опровергают общепринятые представления о выборе провайдера.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/median_latency_with_batch_size_1_ef83bec9c8.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/median_latency_with_batch_size_10_0d4e52566f.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/all_model_latency_vs_token_length_when_batch_size_is_10_537516cc1c.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/all_model_latency_vstoken_lengthwhen_batch_size_is_10_4dcf0d549a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Примечание: Учитывая значительное влияние сетевого окружения и географических регионов серверов на задержку API для встраивания в реальном времени, мы сравнивали задержки североамериканских и азиатских моделей отдельно.</p>
<h3 id="3-Model-Size-Impact-Varies-Dramatically-by-Provider" class="common-anchor-header">3. Влияние размера модели сильно зависит от провайдера</h3><p>Мы наблюдали общую тенденцию, когда большие модели имеют более высокую задержку, чем стандартные модели, которые имеют более высокую задержку, чем маленькие/легкие модели. Однако эта закономерность не была универсальной и позволила выявить важные моменты в архитектуре бэкенда. Например:</p>
<ul>
<li><p><strong>Cohere и OpenAI</strong> показали минимальный разрыв в производительности между размерами моделей.</p></li>
<li><p><strong>VoyageAI</strong> продемонстрировал четкие различия в производительности в зависимости от размера модели.</p></li>
</ul>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_1_f9eaf2be26.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_2_cf4d72d1ad.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Model_Size_Impact_Varies_Dramatically_by_Provider_3_5e0c8d890b.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Это говорит о том, что время отклика API зависит не только от архитектуры модели, но и от множества факторов, включая стратегии пакетной обработки запросов, оптимизацию обработки запросов и инфраструктуру конкретного провайдера. Урок очевиден: <em>не доверяйте размеру модели или дате выпуска как надежным показателям производительности - всегда тестируйте в своей собственной среде развертывания.</em></p>
<h3 id="4-Token-Length-and-Batch-Size-Create-Complex-Trade-offs" class="common-anchor-header">4. Длина токена и размер пакета создают сложные компромиссы</h3><p>В зависимости от реализации бэкенда, в частности стратегии пакетной обработки, длина токена может не оказывать существенного влияния на задержку до тех пор, пока размер партии не увеличится. Наше тестирование выявило определенные закономерности:</p>
<ul>
<li><p><strong>Задержка OpenAI</strong> оставалась довольно стабильной между маленькими и большими партиями, что говорит о широких возможностях бэкенда по пакетной обработке.</p></li>
<li><p><strong>VoyageAI</strong> продемонстрировал явный эффект длины токена, что говорит о минимальной оптимизации пакетной обработки.</p></li>
</ul>
<p>Большие размеры партий увеличивают абсолютную задержку, но повышают общую пропускную способность. В наших тестах переход от batch=1 к batch=10 увеличил задержку на 2×-5×, при этом значительно повысив общую пропускную способность. Это представляет собой критически важную возможность оптимизации для рабочих процессов массовой обработки, когда вы можете обменять задержку отдельных запросов на значительное повышение общей пропускной способности системы.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Going_from_batch_1_to_10_latency_increased_2_5_9811536a3c.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>При переходе от партии = 1 к 10 задержка увеличилась на 2×-5×</p>
<h3 id="5-API-Reliability-Introduces-Production-Risk" class="common-anchor-header">5. Надежность API создает производственный риск</h3><p>Мы наблюдали значительную вариативность задержек, особенно в OpenAI и VoyageAI, что вносит непредсказуемость в производственные системы.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Latency_variance_when_batch_1_d9cd88fb73.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Разброс задержек при партии=1</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Latency_variance_when_batch_10_5efc33bf4e.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Разброс задержек при batch=10</p>
<p>Хотя наше тестирование было сосредоточено в основном на задержках, использование любого внешнего API влечет за собой риски сбоев, включая колебания сети, ограничение скорости провайдера и перебои в обслуживании. В отсутствие четких SLA от провайдеров разработчикам следует применять надежные стратегии обработки ошибок, включая повторные попытки, тайм-ауты и автоматические выключатели для поддержания надежности системы в производственных средах.</p>
<h3 id="6-Local-Inference-Can-Be-Surprisingly-Competitive" class="common-anchor-header">6. Локальные выводы могут быть удивительно конкурентоспособными</h3><p>Наши тесты также показали, что развертывание локальных моделей встраивания среднего размера может обеспечить производительность, сравнимую с облачными API, что очень важно для приложений с ограниченным бюджетом или чувствительных к задержкам.</p>
<p>Например, развертывание открытого исходного кода <code translate="no">bge-base-en-v1.5</code> с помощью TEI (Text Embeddings Inference) на скромном процессоре 4c8g соответствовало производительности SiliconFlow по задержкам, обеспечивая доступную альтернативу локальному внедрению. Этот вывод особенно важен для индивидуальных разработчиков и небольших команд, которые могут не иметь ресурсов GPU корпоративного уровня, но при этом нуждаются в высокопроизводительных возможностях встраивания.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/TEI_Latency_2f09be1ef0.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Латентность TEI</p>
<h3 id="7-Milvus-Overhead-Is-Negligible" class="common-anchor-header">7. Накладные расходы Milvus незначительны</h3><p>Поскольку мы использовали Milvus для тестирования задержки API встраивания, мы убедились, что дополнительные накладные расходы, вносимые функцией TextEmbedding Function от Milvus, крайне малы и практически пренебрежимо малы. Наши измерения показали, что операции Milvus добавляют всего 20-40 мс, в то время как вызовы API для встраивания занимают от сотен миллисекунд до нескольких секунд - это означает, что <strong>Milvus добавляет менее 5 % накладных расходов</strong> к общему времени работы. Узким местом в производительности является, прежде всего, передача данных по сети и собственные вычислительные возможности поставщиков услуг встроенного API, а не серверный уровень Milvus.</p>
<h2 id="Tips-How-to-Optimize-Your-RAG-Embedding-Performance" class="common-anchor-header">Советы: Как оптимизировать производительность встраивания RAG<button data-href="#Tips-How-to-Optimize-Your-RAG-Embedding-Performance" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Основываясь на результатах наших всесторонних сравнительных исследований, мы рекомендуем следующие стратегии для оптимизации производительности встраивания в систему RAG:</p>
<h3 id="1-Always-Localize-Your-Testing" class="common-anchor-header">1. Всегда локализуйте тестирование</h3><p>Не стоит слепо доверять любым общим отчетам бенчмарков (включая этот!). Вы всегда должны тестировать модели в реальной среде развертывания, а не полагаться только на опубликованные эталоны. Условия сети, географическая близость и различия в инфраструктуре могут существенно повлиять на реальную производительность.</p>
<h3 id="2-Geo-Match-Your-Providers-Strategically" class="common-anchor-header">2. Стратегически правильно подбирайте провайдеров</h3><ul>
<li><p><strong>Для развертывания в Северной Америке</strong>: Рассмотрите Cohere, VoyageAI, OpenAI/Azure или GCP Vertex AI - и всегда проводите собственную проверку производительности.</p></li>
<li><p><strong>Для азиатских развертываний</strong>: Серьезно рассмотрите поставщиков азиатских моделей, таких как AliCloud Dashscope или SiliconFlow, которые предлагают лучшую региональную производительность.</p></li>
<li><p><strong>Для глобальной аудитории</strong>: Внедрите мультирегиональную маршрутизацию или выбирайте провайдеров с глобально распределенной инфраструктурой, чтобы минимизировать штрафы за межрегиональную задержку.</p></li>
</ul>
<h3 id="3-Question-Default-Provider-Choices" class="common-anchor-header">3. Ставьте под сомнение выбор провайдера по умолчанию</h3><p>Модели встраивания OpenAI настолько популярны, что многие предприятия и разработчики выбирают их в качестве опции по умолчанию. Однако наши тесты показали, что задержки и стабильность OpenAI в лучшем случае средние, несмотря на его популярность на рынке. Проверьте предположения о "лучших" провайдерах с помощью собственных строгих бенчмарков - популярность не всегда коррелирует с оптимальной производительностью для конкретного случая использования.</p>
<h3 id="4-Optimize-Batch-and-Chunk-Configurations" class="common-anchor-header">4. Оптимизируйте конфигурации пакетных и групповых соединений</h3><p>Одна конфигурация не подходит для всех моделей и случаев использования. Оптимальный размер пакета и длина чанка значительно отличаются у разных провайдеров из-за различий в архитектуре бэкенда и стратегиях пакетной обработки. Систематически экспериментируйте с различными конфигурациями, чтобы найти оптимальную точку производительности, учитывая компромисс между пропускной способностью и задержкой для конкретных требований вашего приложения.</p>
<h3 id="5-Implement-Strategic-Caching" class="common-anchor-header">5. Внедрите стратегическое кэширование</h3><p>Для высокочастотных запросов кэшируйте как текст запроса, так и его сгенерированные вкрапления (используя такие решения, как Redis). Последующие идентичные запросы могут напрямую обращаться к кэшу, сокращая задержку до миллисекунд. Это один из самых экономичных и эффективных методов оптимизации задержки запросов.</p>
<h3 id="6-Consider-Local-Inference-Deployment" class="common-anchor-header">6. Рассмотрите возможность развертывания локальных выводов</h3><p>Если у вас очень высокие требования к задержке ввода данных, задержке запросов и конфиденциальности данных, или если стоимость вызовов API непомерно высока, рассмотрите возможность локального развертывания моделей встраивания для выводов. Стандартные планы API часто имеют ограничения по QPS, нестабильную задержку и отсутствие гарантий SLA - ограничения, которые могут быть проблематичными для производственных сред.</p>
<p>Для многих индивидуальных разработчиков или небольших команд отсутствие графических процессоров корпоративного класса может показаться препятствием для локального развертывания высокопроизводительных моделей встраивания. Однако это не означает полный отказ от локального вывода. В сочетании с высокопроизводительными механизмами вывода, такими как <a href="https://github.com/huggingface/text-embeddings-inference">Hugging Face's text-embeddings-inference</a>, даже запуск небольших и средних моделей встраивания на CPU может обеспечить достойную производительность, которая может превзойти вызовы API с высокой задержкой, особенно для крупномасштабной автономной генерации встраивания.</p>
<p>Этот подход требует тщательного рассмотрения компромиссов между стоимостью, производительностью и сложностью обслуживания.</p>
<h2 id="How-Milvus-Simplifies-Your-Embedding-Workflow" class="common-anchor-header">Как Milvus упрощает рабочий процесс встраивания<button data-href="#How-Milvus-Simplifies-Your-Embedding-Workflow" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Как уже упоминалось, Milvus - это не просто высокопроизводительная векторная база данных, она также предлагает удобный интерфейс функций встраивания, который легко интегрируется с популярными моделями встраивания от различных поставщиков, таких как OpenAI, Cohere, AWS Bedrock, Google Vertex AI, Voyage AI и других по всему миру, в ваш конвейер векторного поиска.</p>
<p>Milvus выходит за рамки хранения и поиска векторов благодаря функциям, которые упрощают интеграцию встраивания:</p>
<ul>
<li><p><strong>Эффективное управление векторами</strong>: Будучи высокопроизводительной базой данных, созданной для работы с огромными коллекциями векторов, Milvus предлагает надежное хранение, гибкие варианты индексирования (HNSW, IVF, RaBitQ, DiskANN и другие) и возможности быстрого и точного поиска.</p></li>
<li><p><strong>Оптимизированное переключение провайдеров</strong>: Milvus предлагает интерфейс <code translate="no">TextEmbedding</code> Function, позволяющий настраивать функцию с помощью ключей API, мгновенно переключать провайдеров или модели и измерять реальную производительность без сложной интеграции SDK.</p></li>
<li><p><strong>Конечные конвейеры данных</strong>: Вызовите <code translate="no">insert()</code> с необработанным текстом, и Milvus автоматически встроит и сохранит векторы за одну операцию, значительно упростив код конвейера данных.</p></li>
<li><p><strong>Переход от текста к результатам за один вызов</strong>: Позвоните на <code translate="no">search()</code> с текстовыми запросами, и Milvus выполнит встраивание, поиск и возврат результатов - все это за один вызов API.</p></li>
<li><p><strong>Интеграция, не зависящая от провайдера</strong>: Milvus абстрагируется от деталей реализации провайдера; достаточно один раз настроить свою функцию и ключ API, и вы готовы к работе.</p></li>
<li><p><strong>Совместимость с экосистемами с открытым исходным кодом</strong>: Независимо от того, генерируете ли вы вкрапления с помощью нашей встроенной функции <code translate="no">TextEmbedding</code> Function, локального вывода или другого метода, Milvus обеспечивает единые возможности хранения и извлечения.</p></li>
</ul>
<p>Это позволяет оптимизировать работу по принципу "Data-In, Insight-Out", когда Milvus управляет генерацией векторов внутри системы, что делает код вашего приложения более простым и удобным для обслуживания.</p>
<h2 id="Conclusion-The-Performance-Truth-Your-RAG-System-Needs" class="common-anchor-header">Заключение: Правда о производительности, в которой нуждается ваша система RAG<button data-href="#Conclusion-The-Performance-Truth-Your-RAG-System-Needs" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Тихий убийца производительности RAG находится не там, где ищет большинство разработчиков. В то время как команды тратят ресурсы на оперативное проектирование и оптимизацию LLM, встроенная задержка API незаметно подрывает пользовательский опыт задержками, которые могут быть в 100 раз хуже, чем ожидалось. Наши комплексные бенчмарки обнажают суровую реальность: популярность не означает производительность, география во многих случаях имеет большее значение, чем выбор алгоритма, а локальные выводы иногда выигрывают у дорогих облачных API.</p>
<p>Эти результаты указывают на важнейшее "слепое пятно" в оптимизации RAG. Межрегиональные штрафы за задержку, неожиданные рейтинги производительности провайдеров и удивительная конкурентоспособность локальных выводов - это не крайние случаи, это производственные реалии, влияющие на реальные приложения. Понимание и измерение производительности API для встраивания очень важно для обеспечения отзывчивого пользовательского опыта.</p>
<p>Выбор поставщика встраивания - один из важнейших элементов головоломки производительности RAG. Тестирование в реальной среде развертывания, выбор географически подходящих провайдеров и рассмотрение альтернатив, таких как локальный вывод, позволит вам устранить основной источник задержек при работе с пользователями и создать действительно отзывчивые приложения ИИ.</p>
