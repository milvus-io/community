---
id: power-high-performance-rag-for-genai-with-hpe-alletra-storag-mp-milvus.md
title: Высокопроизводительный RAG для GenAI с HPE Alletra Storage MP + Milvus
author: Denise Ochoa-Mendoza
date: 2025-11-10T00:00:00.000Z
cover: assets.zilliz.com/hpe_cover_45b4796ef3.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'Milvus, HPE, Alletra Storage MP X10000, vector database, RAG'
meta_title: Optimized RAG with HPE Alletra Storage MP X10000 + Milvus
desc: >-
  Усиление GenAI с помощью HPE Alletra Storage MP X10000 и Milvus. Получите
  масштабируемый векторный поиск с низкой задержкой и хранилище корпоративного
  класса для быстрого и безопасного RAG.
origin: >-
  https://community.hpe.com/t5/around-the-storage-block/power-high-performance-rag-for-genai-with-hpe-alletra-storage-mp/ba-p/7257369
---
<p><em>Это сообщение было первоначально опубликовано на <a href="https://community.hpe.com/t5/around-the-storage-block/power-high-performance-rag-for-genai-with-hpe-alletra-storage-mp/ba-p/7257369">HPE Community</a> и публикуется здесь с разрешения.</em></p>
<p>HPE Alletra Storage MP X10000 и Milvus обеспечивают масштабируемость и низкую задержку RAG, позволяя LLM предоставлять точные, богатые контекстом ответы с высокопроизводительным векторным поиском для рабочих нагрузок GenAI.</p>
<h2 id="In-generative-AI-RAG-needs-more-than-just-an-LLM" class="common-anchor-header">В генеративном ИИ RAG нужен не только LLM.<button data-href="#In-generative-AI-RAG-needs-more-than-just-an-LLM" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Контекст раскрывает истинную мощь генеративного ИИ (GenAI) и больших языковых моделей (LLM). Когда LLM получает правильные сигналы для ориентации своих ответов, он может давать точные, релевантные и достоверные ответы.</p>
<p>Подумайте об этом так: если бы вас забросили в густые джунгли с устройством GPS, но без спутникового сигнала. На экране отображается карта, но без вашего текущего местоположения она бесполезна для навигации. Напротив, GPS с сильным спутниковым сигналом не просто показывает карту - он дает вам пошаговые инструкции.</p>
<p>Именно это и делает генерация с дополненным поиском (RAG) для LLM. У модели уже есть карта (ее предварительно обученные знания), но нет направления (ваши данные о конкретной области). LLM без RAG похожи на GPS-устройства, которые полны знаний, но не могут сориентироваться в реальном времени. RAG обеспечивает сигнал, который говорит модели, где она находится и куда идти.</p>
<p>RAG основывает ответы модели на надежных, актуальных знаниях, полученных из вашего собственного контента, специфичного для данной области, - политик, документации по продуктам, билетов, PDF-файлов, кода, аудио-транскриптов, изображений и многого другого. Заставить RAG работать в масштабе очень сложно. Процесс поиска должен быть достаточно быстрым, чтобы обеспечить бесперебойную работу пользователей, достаточно точным, чтобы возвращать наиболее релевантную информацию, и предсказуемым, даже когда система испытывает большую нагрузку. Это означает, что необходимо справляться с большими объемами запросов, постоянным вводом данных и фоновыми задачами, такими как создание индексов, без снижения производительности. Развернуть конвейер RAG с несколькими PDF-файлами относительно просто. Однако при масштабировании до сотен PDF-файлов это становится значительно сложнее. Вы не можете хранить все в памяти, поэтому надежная и эффективная стратегия хранения становится необходимой для управления вкраплениями, индексами и производительностью поиска. Для RAG требуется векторная база данных и слой хранения, способный поддерживать темпы роста параллелизма и объемов данных.</p>
<h2 id="Vector-databases-power-RAG" class="common-anchor-header">Векторные базы данных - основа RAG<button data-href="#Vector-databases-power-RAG" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В основе RAG лежит семантический поиск - поиск информации по смыслу, а не по точным ключевым словам. Именно здесь на помощь приходят векторные базы данных. В них хранятся высокоразмерные вкрапления текста, изображений и других неструктурированных данных, что позволяет осуществлять поиск по сходству и извлекать наиболее релевантный контекст для ваших запросов. Milvus - ведущий пример: облачная нативная векторная база данных с открытым исходным кодом, созданная для поиска по сходству в миллиардных масштабах. Она поддерживает гибридный поиск, сочетая векторное сходство с ключевыми словами и скалярными фильтрами для повышения точности, и предлагает независимое масштабирование вычислений и хранилищ с возможностями оптимизации с учетом GPU для ускорения. Milvus также управляет данными в рамках интеллектуального жизненного цикла сегментов, переходя от растущих к уплотненным сегментам с уплотнением и несколькими вариантами индексирования по принципу приближенных ближайших соседей (ANN), такими как HNSW и DiskANN, обеспечивая производительность и масштабируемость для рабочих нагрузок ИИ в реальном времени, таких как RAG.</p>
<h2 id="The-hidden-challenge-Storage-throughput--latency" class="common-anchor-header">Скрытая проблема: пропускная способность хранилища и задержка<button data-href="#The-hidden-challenge-Storage-throughput--latency" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Рабочие нагрузки, связанные с векторным поиском, оказывают давление на все части системы. Они требуют высококонкурентного ввода данных при сохранении низкой задержки при интерактивных запросах. В то же время фоновые операции, такие как создание индексов, уплотнение и перезагрузка данных, должны выполняться без сбоев в работе. Многие узкие места в традиционных архитектурах связаны с хранением данных. Будь то ограничения ввода-вывода (I/O), задержки при поиске метаданных или ограничения параллелизма. Чтобы обеспечить предсказуемую производительность в реальном времени в масштабе, уровень хранения должен соответствовать требованиям векторных баз данных.</p>
<h2 id="The-storage-foundation-for-high-performance-vector-search" class="common-anchor-header">Основа хранения данных для высокопроизводительного векторного поиска<button data-href="#The-storage-foundation-for-high-performance-vector-search" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p><a href="https://www.hpe.com/in/en/alletra-storage-mp-x10000.html">HPE Alletra Storage MP X10000</a> - это оптимизированная под флеш-память, полностью NVMe, S3-совместимая платформа для хранения объектов, разработанная для обеспечения производительности в реальном времени в масштабе. В отличие от традиционных объектных хранилищ, ориентированных на емкость, HPE Alletra Storage MP X10000 предназначена для таких высокопроизводительных рабочих нагрузок с низкой задержкой, как векторный поиск. Его механизм структурированных в журнал ключевых значений и метаданные на основе экстентов обеспечивают высокопараллельное чтение и запись, а GPUDirect RDMA обеспечивает пути передачи данных с нулевым копированием, что снижает нагрузку на CPU и ускоряет передачу данных на GPU. Архитектура поддерживает дезагрегированное масштабирование, позволяющее независимо наращивать емкость и производительность, и включает такие функции корпоративного уровня, как шифрование, контроль доступа на основе ролей (RBAC), неизменяемость и долговечность данных. В сочетании с облачно-нативным дизайном HPE Alletra Storage MP X10000 легко интегрируется со средами Kubernetes, что делает его идеальной основой хранения для развертывания Milvus.</p>
<h2 id="HPE-Alletra-Storage-MP-X10000-and-Milvus-A-scalable-foundation-for-RAG" class="common-anchor-header">HPE Alletra Storage MP X10000 и Milvus: масштабируемая основа для RAG<button data-href="#HPE-Alletra-Storage-MP-X10000-and-Milvus-A-scalable-foundation-for-RAG" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>HPE Alletra Storage MP X10000 и Milvus дополняют друг друга, обеспечивая быстрое, предсказуемое и легко масштабируемое RAG. На рисунке 1 показана архитектура масштабируемых сценариев использования ИИ и конвейеров RAG, демонстрирующая, как компоненты Milvus, развернутые в контейнерной среде, взаимодействуют с высокопроизводительными объектными хранилищами HPE Alletra Storage MP X10000.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Figure_1_Architecture_of_scalable_AI_use_cases_and_RAG_pipeline_using_HPE_Alletra_Storage_MP_X10000_and_Milvus_ed3a87a5ee.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Milvus четко отделяет вычисления от хранения, а HPE Alletra Storage MP X10000 обеспечивает высокопроизводительный доступ к объектам с низкой задержкой, не отставая от векторных рабочих нагрузок. Вместе они обеспечивают предсказуемую производительность при масштабировании: Milvus распределяет запросы между шардами, а дробное многомерное масштабирование HPE Alletra Storage MP X10000 позволяет поддерживать постоянную задержку по мере роста объема данных и QPS. Проще говоря, вы добавляете именно ту емкость или производительность, которая вам нужна, и тогда, когда она вам нужна. Еще одно преимущество - простота эксплуатации: HPE Alletra Storage MP X10000 поддерживает максимальную производительность из одного ведра, исключая сложное выстраивание уровней, а корпоративные функции (шифрование, RBAC, неизменяемость, надежная долговечность) поддерживают локальные или гибридные развертывания с высоким уровнем суверенитета данных и согласованными целями уровня обслуживания (SLO).</p>
<p>Когда масштабируется векторный поиск, хранилище часто обвиняют в медленном вводе, уплотнении или извлечении данных. С Milvus на HPE Alletra Storage MP X10000 эта ситуация меняется. Архитектура платформы, полностью состоящая из NVMe, структурированных журналов, и опция GPUDirect RDMA обеспечивают стабильный доступ к объектам с ультранизкой задержкой - даже в условиях интенсивного параллелизма и во время операций жизненного цикла, таких как создание и перезагрузка индексов. На практике ваши конвейеры RAG остаются привязанными к вычислениям, а не к хранилищу. По мере роста коллекций и увеличения объемов запросов Milvus остается отзывчивым, а HPE Alletra Storage MP X10000 сохраняет резерв ввода-вывода, обеспечивая предсказуемую линейную масштабируемость без перестройки архитектуры хранилища. Это становится особенно важным по мере того, как развертывание RAG выходит за рамки начальных этапов проверки концепции и переходит к полноценному производству.</p>
<h2 id="Enterprise-ready-RAG-Scalable-predictable-and-built-for-GenAI" class="common-anchor-header">RAG для предприятий: масштабируемый, предсказуемый и созданный для GenAI<button data-href="#Enterprise-ready-RAG-Scalable-predictable-and-built-for-GenAI" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Для RAG и рабочих нагрузок GenAI в режиме реального времени сочетание HPE Alletra Storage MP X10000 и Milvus обеспечивает перспективную основу, которая уверенно масштабируется. Это интегрированное решение позволяет организациям создавать интеллектуальные системы, быстрые, эластичные и безопасные, без ущерба для производительности и управляемости. Milvus обеспечивает распределенный векторный поиск с GPU-ускорением и модульное масштабирование, а HPE Alletra Storage MP X10000 обеспечивает сверхбыстрый доступ к объектам с низкой задержкой, долговечность корпоративного уровня и управление жизненным циклом. Вместе они отделяют вычисления от хранения, обеспечивая предсказуемую производительность даже при росте объемов данных и сложности запросов. Независимо от того, обслуживаете ли вы рекомендации в реальном времени, используете семантический поиск или масштабируете миллиарды векторов, эта архитектура обеспечивает быстродействие, экономичность и оптимизацию облачных конвейеров RAG. Благодаря бесшовной интеграции с Kubernetes и облаком HPE GreenLake вы получаете унифицированное управление, ценообразование на основе потребления и гибкость при развертывании в гибридных или частных облачных средах. HPE Alletra Storage MP X10000 и Milvus: масштабируемое, высокопроизводительное решение RAG, созданное с учетом требований современных GenAI.</p>
