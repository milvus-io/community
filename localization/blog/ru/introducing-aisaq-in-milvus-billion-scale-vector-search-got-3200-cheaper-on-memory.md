---
id: >-
  introducing-aisaq-in-milvus-billion-scale-vector-search-got-3200-cheaper-on-memory.md
title: >-
  Представляем AISAQ в Milvus: векторный поиск миллиардного масштаба стал на
  3,200× дешевле по памяти
author: Martin Li
date: 2025-12-10T00:00:00.000Z
cover: assets.zilliz.com/AISAQ_Cover_66b628b762.png
tag: Engineering
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'Milvus2.6, AISAQ, DISKANN, vector search'
meta_title: |
  AISAQ in Milvus Cuts Memory 3,200× for Billion-Scale Search
desc: >-
  Узнайте, как Milvus снижает затраты на память на 3200× с помощью AISAQ,
  обеспечивая масштабируемый поиск миллиардов векторов без накладных расходов на
  DRAM.
origin: >-
  https://milvus.io/blog/introducing-aisaq-in-milvus-billion-scale-vector-search-got-3200-cheaper-on-memory.md
---
<p>Векторные базы данных стали основной инфраструктурой для критически важных систем искусственного интеллекта, и объемы данных в них растут по экспоненте, часто достигая миллиардов векторов. При таких масштабах все становится сложнее: поддерживать низкую задержку, сохранять точность, обеспечивать надежность, работать с репликами и регионами. Но одна проблема, как правило, всплывает на поверхность раньше и определяет архитектурные решения - стоимость<strong>.</strong></p>
<p>Чтобы обеспечить быстрый поиск, большинство векторных баз данных хранят ключевые структуры индексирования в DRAM (Dynamic Random Access Memory), самом быстром и самом дорогом ярусе памяти. Такая конструкция эффективна с точки зрения производительности, но плохо масштабируется. Использование DRAM зависит от объема данных, а не от трафика запросов, и даже при сжатии или частичной разгрузке SSD большая часть индекса должна оставаться в памяти. По мере роста массивов данных затраты на память быстро становятся ограничивающим фактором.</p>
<p>Milvus уже поддерживает <strong>DISKANN</strong>, дисковый ANN-подход, который снижает нагрузку на память за счет переноса большей части индекса на SSD. Однако DISKANN все еще полагается на DRAM для сжатых представлений, используемых во время поиска. В <a href="https://milvus.io/docs/release_notes.md#v264">Milvus 2.6</a> эта идея получила дальнейшее развитие благодаря <a href="https://milvus.io/docs/aisaq.md">AISAQ</a>, векторному индексу на основе диска, вдохновленному <a href="https://milvus.io/docs/diskann.md">DISKANN</a>. Архитектура AiSAQ, разработанная компанией KIOXIA, построена по принципу "Zero-DRAM-Footprint Architecture", которая хранит все важные для поиска данные на диске и оптимизирует размещение данных для минимизации операций ввода-вывода. При нагрузке в миллиард векторов это позволяет сократить использование памяти с <strong>32 ГБ до 10 МБ -</strong> <strong>уменьшение на 3 200 раз при</strong>сохранении практической производительности.</p>
<p>В следующих разделах мы объясним, как работает векторный поиск на основе графов, откуда берутся затраты на память и как AISAQ меняет кривую затрат при векторном поиске в миллиардных масштабах.</p>
<h2 id="How-Conventional-Graph-Based-Vector-Search-Works" class="common-anchor-header">Принцип работы обычного векторного поиска на основе графов<button data-href="#How-Conventional-Graph-Based-Vector-Search-Works" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p><strong>Векторный поиск</strong> - это процесс нахождения точек данных, числовые представления которых наиболее близки к запросу в высокоразмерном пространстве. "Ближайшие" означает наименьшее расстояние в соответствии с функцией расстояния, такой как косинусоидальное расстояние или расстояние L2. На малых масштабах это просто: вычислите расстояние между запросом и каждым вектором, а затем верните ближайшие. Однако при больших масштабах, скажем, миллиардных, такой подход быстро становится слишком медленным, чтобы быть практичным.</p>
<p>Чтобы избежать исчерпывающих сравнений, современные системы приближенного поиска ближайших соседей (ANNS) опираются на <strong>индексы на основе графов</strong>. Вместо того чтобы сравнивать запрос с каждым вектором, индекс организует векторы в <strong>граф</strong>. Каждый узел представляет собой вектор, а ребра соединяют векторы, которые численно близки. Такая структура позволяет значительно сузить пространство поиска.</p>
<p>Граф строится заранее, основываясь исключительно на связях между векторами. Он не зависит от запросов. Когда поступает запрос, задача системы - <strong>эффективно перемещаться по графу</strong> и определять векторы с наименьшим расстоянием до запроса, не сканируя весь набор данных.</p>
<p>Поиск начинается с заранее определенной <strong>точки входа</strong> в граф. Эта начальная точка может быть далека от запроса, но алгоритм шаг за шагом улучшает ее положение, двигаясь к векторам, которые оказываются ближе к запросу. Во время этого процесса поиск поддерживает две внутренние структуры данных, которые работают вместе: <strong>список кандидатов</strong> и <strong>список результатов</strong>.</p>
<p>И два наиболее важных шага в этом процессе - расширение списка кандидатов и обновление списка результатов.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/whiteboard_exported_image_84f8324275.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="Expanding-the-Candidate-List" class="common-anchor-header">Расширение списка кандидатов</h3><p><strong>Список кандидатов</strong> представляет собой то, куда поиск может быть направлен дальше. Это приоритетный набор узлов графа, которые кажутся перспективными на основании их расстояния до запроса.</p>
<p>На каждой итерации алгоритм:</p>
<ul>
<li><p><strong>Выбирает ближайшего из найденных на данный момент кандидатов.</strong> Из списка кандидатов выбирается вектор с наименьшим расстоянием до запроса.</p></li>
<li><p><strong>Извлекает из графа соседей этого вектора.</strong> Этими соседями являются векторы, которые были определены во время построения индекса как близкие к текущему вектору.</p></li>
<li><p><strong>Оценивает непосещенных соседей и добавляет их в список кандидатов.</strong> Для каждого соседа, который еще не был исследован, алгоритм вычисляет его расстояние до запроса. Ранее посещенные соседи пропускаются, а новые добавляются в список кандидатов, если они кажутся перспективными.</p></li>
</ul>
<p>Многократно расширяя список кандидатов, поиск исследует все более релевантные области графа. Это позволяет алгоритму неуклонно двигаться к лучшим ответам, исследуя лишь небольшую часть всех векторов.</p>
<h3 id="Updating-the-Result-List" class="common-anchor-header">Обновление списка результатов</h3><p>В то же время алгоритм ведет <strong>список результатов</strong>, в который записываются лучшие кандидаты, найденные на данный момент для окончательного вывода. По мере выполнения поиска он:</p>
<ul>
<li><p><strong>Отслеживает ближайшие векторы, встреченные во время обхода.</strong> К ним относятся векторы, выбранные для расширения, а также другие, оцененные по пути.</p></li>
<li><p><strong>Сохраняет их расстояния до запроса.</strong> Это позволяет ранжировать кандидатов и поддерживать текущий топ-K ближайших соседей.</p></li>
</ul>
<p>Со временем, по мере того как оценивается все больше кандидатов и находится все меньше улучшений, список результатов стабилизируется. Если дальнейшее исследование графа вряд ли приведет к получению более близких векторов, поиск прекращается и возвращает список результатов в качестве окончательного ответа.</p>
<p>Проще говоря, <strong>список кандидатов управляет поиском</strong>, а <strong>список результатов фиксирует лучшие ответы, найденные на данный момент</strong>.</p>
<h2 id="The-Trade-Off-in-Graph-Based-Vector-Search" class="common-anchor-header">Компромисс в векторном поиске на основе графов<button data-href="#The-Trade-Off-in-Graph-Based-Vector-Search" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Именно графовый подход делает крупномасштабный векторный поиск практичным в первую очередь. Благодаря навигации по графу вместо сканирования каждого вектора система может находить высококачественные результаты, затрагивая лишь небольшую часть набора данных.</p>
<p>Однако эта эффективность не достается даром. Поиск на основе графов выявляет фундаментальный компромисс между <strong>точностью и стоимостью.</strong></p>
<ul>
<li><p>Поиск большего числа соседей повышает точность за счет охвата большей части графа и снижения вероятности пропуска истинных ближайших соседей.</p></li>
<li><p>В то же время каждое дополнительное расширение добавляет работы: больше вычислений расстояний, больше обращений к структуре графа и больше считываний векторных данных. По мере углубления или расширения поиска эти затраты накапливаются. В зависимости от того, как спроектирован индекс, они проявляются в виде более высокой загрузки процессора, увеличения объема памяти или дополнительных дисковых операций ввода-вывода.</p></li>
</ul>
<p>Баланс между этими противоположными силами - высокой отзывчивостью и эффективным использованием ресурсов - является центральным моментом при разработке поиска на основе графов.</p>
<p>И <a href="https://milvus.io/blog/diskann-explained.md"><strong>DISKANN</strong></a>, и <strong>AISAQ</strong> построены с учетом этих противоречий, но они делают разные архитектурные решения о том, как и где оплачиваются эти расходы.</p>
<h2 id="How-DISKANN-Optimizes-Disk-Based-Vector-Search" class="common-anchor-header">Как DISKANN оптимизирует векторный поиск на основе дисков<button data-href="#How-DISKANN-Optimizes-Disk-Based-Vector-Search" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/DISKANN_9c9c6a734f.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>DISKANN - самое влиятельное на сегодняшний день решение на основе дисковых ИНС, которое служит официальным базовым уровнем для конкурса NeurIPS Big ANN, глобального эталона векторного поиска миллиардного масштаба. Его значимость заключается не только в производительности, но и в том, что он доказал: <strong>для быстрого поиска ANN на основе графов не обязательно полностью жить в памяти</strong>.</p>
<p>Сочетая SSD-накопители с тщательно подобранными структурами в памяти, DISKANN продемонстрировала, что крупномасштабный векторный поиск может достигать высокой точности и низкой задержки на аппаратном обеспечении, не требуя огромных площадей DRAM. Это достигается за счет переосмысления <em>того, какие части поиска должны быть быстрыми</em>, а <em>какие могут допускать более медленный доступ</em>.</p>
<p><strong>На высоком уровне DISKANN сохраняет наиболее часто используемые данные в памяти, перемещая более крупные, менее часто используемые структуры на диск.</strong> Этот баланс достигается за счет нескольких ключевых конструктивных решений.</p>
<h3 id="1-Using-PQ-Distances-to-Expand-the-Candidate-List" class="common-anchor-header">1. Использование расстояния PQ для расширения списка кандидатов</h3><p>Расширение списка кандидатов - самая частая операция в поиске на основе графов. Каждое расширение требует оценки расстояния между вектором запроса и соседями узла-кандидата. Выполнение этих вычислений с использованием полных, высокоразмерных векторов потребовало бы частого случайного чтения с диска - дорогостоящая операция как с вычислительной точки зрения, так и с точки зрения ввода-вывода.</p>
<p>DISKANN избегает этих затрат, сжимая векторы в <strong>коды Product Quantization (PQ)</strong> и сохраняя их в памяти. PQ-коды намного меньше полных векторов, но все же сохраняют достаточно информации для приблизительной оценки расстояния.</p>
<p>Во время расширения кандидатов DISKANN вычисляет расстояния, используя эти PQ-коды в памяти, вместо того чтобы считывать полные векторы с SSD. Это значительно сокращает дисковые операции ввода-вывода во время обхода графа, позволяя быстро и эффективно расширять кандидатов, при этом большая часть трафика SSD не попадает на критический путь.</p>
<h3 id="2-Co-Locating-Full-Vectors-and-Neighbor-Lists-on-Disk" class="common-anchor-header">2. Совместное размещение полных векторов и списков соседей на диске</h3><p>Не все данные можно сжать или получить к ним приблизительный доступ. После того как перспективные кандидаты определены, для получения точных результатов поиск по-прежнему нуждается в доступе к двум типам данных:</p>
<ul>
<li><p><strong>Списки соседей</strong>, чтобы продолжить обход графа</p></li>
<li><p><strong>полные (несжатые) векторы</strong> для окончательного ранжирования.</p></li>
</ul>
<p>К этим структурам обращаются реже, чем к PQ-кодам, поэтому DISKANN хранит их на твердотельном накопителе. Чтобы минимизировать дисковые накладные расходы, DISKANN помещает список соседей каждого узла и его полный вектор в одну и ту же физическую область на диске. Это гарантирует, что при одном чтении с SSD можно получить и то, и другое.</p>
<p>Благодаря совместному размещению связанных данных DISKANN уменьшает количество случайных обращений к диску, необходимых при поиске. Эта оптимизация повышает эффективность как расширения, так и повторного ранжирования, особенно в больших масштабах.</p>
<h3 id="3-Parallel-Node-Expansion-for-Better-SSD-Utilization" class="common-anchor-header">3. Параллельное расширение узлов для лучшего использования твердотельных накопителей</h3><p>Поиск ANN на основе графов - это итерационный процесс. Если в каждой итерации расширяется только один узел-кандидат, система выдает только одно чтение с диска за раз, оставляя большую часть параллельной пропускной способности SSD неиспользованной. Чтобы избежать этой неэффективности, DISKANN расширяет несколько кандидатов в каждой итерации и отправляет параллельные запросы на чтение на твердотельный накопитель. Такой подход позволяет гораздо лучше использовать доступную полосу пропускания и сократить общее количество необходимых итераций.</p>
<p>Параметр <strong>beam_width_ratio</strong> управляет тем, сколько кандидатов расширяется параллельно: <strong>Ширина луча = количество ядер процессора × соотношение ширины луча.</strong> Более высокое соотношение расширяет поиск, потенциально повышая точность, но также увеличивает объем вычислений и дисковых операций ввода-вывода.</p>
<p>Чтобы компенсировать это, DISKANN использует <code translate="no">search_cache_budget_gb_ratio</code>, который резервирует память для кэширования часто используемых данных, сокращая повторные чтения с SSD. Вместе эти механизмы помогают DISKANN сбалансировать точность, задержку и эффективность ввода-вывода.</p>
<h3 id="Why-This-Matters--and-Where-the-Limits-Appear" class="common-anchor-header">Почему это важно - и где появляются пределы</h3><p>Конструкция DISKANN - это большой шаг вперед для векторного поиска на дисках. Благодаря тому, что PQ-коды хранятся в памяти, а более крупные структуры переносятся на SSD, она значительно сокращает занимаемую память по сравнению с индексами графов, полностью хранящимися в памяти.</p>
<p>В то же время эта архитектура по-прежнему зависит от <strong>постоянно включенной DRAM</strong> для критически важных для поиска данных. Коды PQ, кэши и управляющие структуры должны оставаться в памяти, чтобы обеспечить эффективность обхода. По мере роста массивов данных до миллиардов векторов и добавления реплик или регионов, потребность в памяти может стать ограничивающим фактором.</p>
<p>Именно этот недостаток и призван устранить <strong>AISAQ</strong>.</p>
<h2 id="How-AISAQ-Works-and-Why-It-Matters" class="common-anchor-header">Как работает AISAQ и почему это важно<button data-href="#How-AISAQ-Works-and-Why-It-Matters" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>AISAQ опирается непосредственно на основные идеи DISKANN, но вносит критический сдвиг: он устраняет <strong>необходимость хранить данные PQ в DRAM</strong>. Вместо того чтобы рассматривать сжатые векторы как критически важные для поиска структуры, всегда находящиеся в памяти, AISAQ переносит их на твердотельный накопитель и пересматривает способ размещения данных графа на диске, чтобы сохранить эффективность обхода.</p>
<p>Чтобы это работало, AISAQ реорганизует хранилище узлов таким образом, чтобы данные, необходимые для поиска графа - полные векторы, списки соседей и информация PQ - располагались на диске в шаблонах, оптимизированных для локальности доступа. Цель состоит не только в том, чтобы переместить больше данных на более экономичный диск, но и в том, чтобы сделать это <strong>без нарушения процесса поиска, описанного ранее</strong>.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/AISAQ_244e661794.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Для удовлетворения различных требований приложений в AISAQ предусмотрено два режима хранения данных на дисках: Производительность и Масштаб. С технической точки зрения эти режимы различаются главным образом тем, как хранятся сжатые PQ-данные и как к ним осуществляется доступ во время поиска. С точки зрения приложений эти режимы отвечают двум разным требованиям: требованиям низкой задержки, характерным для онлайновых систем семантического поиска и рекомендаций, и требованиям сверхвысокого масштаба, характерным для RAG.</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/aisaq_vs_diskann_35ebee3c64.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="AISAQ-performance-Optimized-for-Speed" class="common-anchor-header">Производительность AISAQ: Оптимизировано для скорости</h3><p>В режиме AISAQ-performance все данные хранятся на диске, а накладные расходы на ввод-вывод снижаются за счет размещения данных.</p>
<p>В этом режиме:</p>
<ul>
<li><p>Полный вектор каждого узла, список ребер и PQ-коды его соседей хранятся вместе на диске.</p></li>
<li><p>Посещение узла по-прежнему требует только <strong>одного чтения с SSD</strong>, поскольку все данные, необходимые для расширения и оценки кандидатов, размещены на диске.</p></li>
</ul>
<p>С точки зрения алгоритма поиска, это в точности повторяет схему доступа DISKANN. Расширение кандидатов остается эффективным, а производительность во время выполнения сопоставима, даже несмотря на то, что все критически важные для поиска данные теперь хранятся на диске.</p>
<p>Компромисс заключается в накладных расходах на хранение. Поскольку данные PQ соседа могут находиться на дисковых страницах нескольких узлов, такая схема вводит избыточность и значительно увеличивает общий размер индекса.</p>
<p>Поэтому в режиме AISAQ-Performance приоритетом является низкая задержка ввода-вывода, а не эффективность работы с диском. С точки зрения приложений, режим AiSAQ-Performance может обеспечить задержку в диапазоне 10 мс, что необходимо для онлайнового семантического поиска.</p>
<h3 id="AISAQ-scale-Optimized-for-Storage-Efficiency" class="common-anchor-header">Масштаб AISAQ: Оптимизация для повышения эффективности хранения данных</h3><p>Режим AISAQ-Scale использует противоположный подход. Он разработан для <strong>минимизации использования диска</strong>, при этом все данные хранятся на SSD.</p>
<p>В этом режиме:</p>
<ul>
<li><p>PQ-данные хранятся на диске отдельно, без избыточности.</p></li>
<li><p>Это устраняет избыточность и значительно уменьшает размер индекса.</p></li>
</ul>
<p>Компромисс заключается в том, что для доступа к PQ-кодам узла и его соседей может потребоваться <strong>несколько считываний с SSD</strong>, что увеличивает количество операций ввода-вывода при расширении кандидатов. При отсутствии оптимизации это может значительно замедлить поиск.</p>
<p>Для борьбы с этими накладными расходами в режиме AISAQ-Scale вводятся две дополнительные оптимизации:</p>
<ul>
<li><p><strong>Перегруппировка данных PQ</strong>, которая упорядочивает векторы PQ по приоритету доступа для улучшения локальности и уменьшения случайных чтений.</p></li>
<li><p><strong>Кэш PQ в DRAM</strong> (<code translate="no">pq_read_page_cache_size</code>), который хранит часто используемые данные PQ и позволяет избежать повторных чтений с диска для "горячих" записей.</p></li>
</ul>
<p>Благодаря этим оптимизациям режим AISAQ-Scale достигает гораздо большей эффективности хранения, чем AISAQ-Performance, сохраняя при этом практическую производительность поиска. Эта производительность остается ниже, чем у DISKANN, но при этом отсутствуют накладные расходы на хранение (размер индекса аналогичен DISKANN), а объем памяти значительно меньше. С точки зрения приложений, AiSAQ предоставляет средства для удовлетворения требований RAG в сверхвысоких масштабах.</p>
<h3 id="Key-Advantages-of-AISAQ" class="common-anchor-header">Ключевые преимущества AISAQ</h3><p>Перенося все критически важные для поиска данные на диск и изменяя способ доступа к ним, AISAQ кардинально меняет стоимость и масштабируемость векторного поиска на основе графов. Его конструкция обеспечивает три существенных преимущества.</p>
<p><strong>1. До 3 200× меньшее использование DRAM</strong></p>
<p>Квантование продукта значительно уменьшает размер высокоразмерных векторов, но при миллиардных масштабах объем памяти все равно остается значительным. Даже после сжатия PQ-коды должны храниться в памяти во время поиска в обычных системах.</p>
<p>Например, для <strong>SIFT1B</strong>, эталона с миллиардом 128-мерных векторов, одни только PQ-коды требуют примерно <strong>30-120 ГБ DRAM</strong>, в зависимости от конфигурации. Для хранения полных векторов без сжатия потребуется еще <strong>~480 ГБ</strong>. Хотя PQ сокращает использование памяти на 4-16×, оставшийся след все еще достаточно велик, чтобы доминировать над стоимостью инфраструктуры.</p>
<p>AISAQ полностью устраняет это требование. Благодаря хранению кодов PQ на SSD вместо DRAM память больше не расходуется на постоянные индексные данные. DRAM используется только для легких, переходных структур, таких как списки кандидатов и метаданные управления. На практике это позволяет сократить использование памяти с десятков гигабайт до <strong>примерно 10 МБ</strong>. В репрезентативной конфигурации миллиардного масштаба объем DRAM снижается с <strong>32 ГБ до 10 МБ</strong>, то есть <strong>на 3 200 раз</strong>.</p>
<p>Учитывая, что SSD-накопители стоят примерно <strong>1/30 цены за единицу емкости</strong> по сравнению с DRAM, этот сдвиг оказывает прямое и значительное влияние на общую стоимость системы.</p>
<p><strong>2. Отсутствие дополнительных накладных расходов на ввод-вывод</strong></p>
<p>Перемещение PQ-кодов из памяти на диск обычно увеличивает количество операций ввода-вывода во время поиска. AISAQ избегает этого, тщательно контролируя <strong>расположение данных и шаблоны доступа</strong>. Вместо того чтобы разбрасывать связанные данные по диску, AISAQ размещает PQ-коды, полные векторы и списки соседей так, чтобы их можно было извлекать вместе. Это гарантирует, что расширение кандидатов не приведет к дополнительным случайным чтениям.</p>
<p>Чтобы предоставить пользователям контроль над компромиссом между размером индекса и эффективностью ввода-вывода, AISAQ вводит параметр <code translate="no">inline_pq</code>, который определяет, сколько данных PQ хранится в строке на каждом узле:</p>
<ul>
<li><p><strong>Меньше inline_pq:</strong> меньший размер индекса, но может потребоваться дополнительный ввод-вывод.</p></li>
<li><p><strong>Большее значение inline_pq:</strong> больший размер индекса, но сохраняется однократный доступ.</p></li>
</ul>
<p>При конфигурации с <strong>inline_pq = max_degree</strong> AISAQ считывает полный вектор узла, список соседей и все PQ-коды за одну дисковую операцию, что соответствует шаблону ввода-вывода DISKANN, сохраняя все данные на SSD.</p>
<p><strong>3. Последовательный доступ к PQ повышает эффективность вычислений</strong></p>
<p>В DISKANN расширение узла-кандидата требует R случайных обращений к памяти для получения PQ-кодов его R соседей. AISAQ устраняет эту случайность, получая все PQ-коды за один ввод-вывод и последовательно сохраняя их на диске.</p>
<p>Последовательное расположение обеспечивает два важных преимущества:</p>
<ul>
<li><p><strong>Последовательное чтение с твердотельного накопителя происходит гораздо быстрее</strong>, чем случайное чтение.</p></li>
<li><p><strong>Непрерывные данные более удобны для кэширования</strong>, что позволяет процессорам эффективнее вычислять расстояния PQ.</p></li>
</ul>
<p>Это повышает скорость и предсказуемость вычислений расстояний PQ и помогает компенсировать затраты на производительность при хранении кодов PQ на SSD, а не в DRAM.</p>
<h2 id="AISAQ-vs-DISKANN-Performance-Evaluation" class="common-anchor-header">AISAQ против DISKANN: оценка производительности<button data-href="#AISAQ-vs-DISKANN-Performance-Evaluation" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>После понимания того, чем AISAQ архитектурно отличается от DISKANN, следующий вопрос прост: <strong>как эти конструктивные решения влияют на производительность и использование ресурсов на практике?</strong> В данной статье мы сравниваем AISAQ и DISKANN по трем параметрам, которые наиболее важны в миллиардных масштабах: <strong>производительность поиска, потребление памяти и использование диска</strong>.</p>
<p>В частности, мы изучаем, как ведет себя AISAQ при изменении объема встроенных данных PQ (<code translate="no">INLINE_PQ</code>). Этот параметр напрямую контролирует компромисс между размером индекса, дисковым вводом-выводом и эффективностью во время выполнения. Мы также оцениваем оба подхода на <strong>векторных рабочих нагрузках с низкой и высокой размерностью, поскольку размерность сильно влияет на стоимость вычисления расстояний и</strong> требования к хранению.</p>
<h3 id="Setup" class="common-anchor-header">Установка</h3><p>Все эксперименты проводились на одноузловой системе, чтобы изолировать поведение индекса и избежать влияния сетевых или распределенных системных эффектов.</p>
<p><strong>Аппаратная конфигурация:</strong></p>
<ul>
<li><p>ПРОЦЕССОР: Процессор Intel® Xeon® Platinum 8375C @ 2,90 ГГц.</p></li>
<li><p>Память: Скорость: 3200 MT/s, Тип: DDR4, объем: 32 ГБ</p></li>
<li><p>Диск: 500 ГБ NVMe SSD</p></li>
</ul>
<p><strong>Параметры построения индекса</strong></p>
<pre><code translate="no">{
  <span class="hljs-string">&quot;max_degree&quot;</span>: <span class="hljs-number">48</span>,
  <span class="hljs-string">&quot;search_list_size&quot;</span>: <span class="hljs-number">100</span>,
  <span class="hljs-string">&quot;inline_pq&quot;</span>: <span class="hljs-number">0</span>/<span class="hljs-number">12</span>/<span class="hljs-number">24</span>/<span class="hljs-number">48</span>,  <span class="hljs-comment">// AiSAQ only</span>
  <span class="hljs-string">&quot;pq_code_budget_gb_ratio&quot;</span>: <span class="hljs-number">0.125</span>,
  <span class="hljs-string">&quot;search_cache_budget_gb_ratio&quot;</span>: <span class="hljs-number">0.0</span>,
  <span class="hljs-string">&quot;build_dram_budget_gb&quot;</span>: <span class="hljs-number">32.0</span>
}
<button class="copy-code-btn"></button></code></pre>
<p><strong>Параметры запроса</strong></p>
<pre><code translate="no">{
  <span class="hljs-string">&quot;k&quot;</span>: <span class="hljs-number">100</span>,
  <span class="hljs-string">&quot;search_list_size&quot;</span>: <span class="hljs-number">100</span>,
  <span class="hljs-string">&quot;beamwidth&quot;</span>: <span class="hljs-number">8</span>
}
<button class="copy-code-btn"></button></code></pre>
<h3 id="Benchmark-Method" class="common-anchor-header">Метод бенчмарка</h3><p>DISKANN и AISAQ были протестированы с помощью <a href="https://milvus.io/docs/knowhere.md">Knowhere</a>, векторного поискового движка с открытым исходным кодом, используемого в Milvus. В этой оценке использовались два набора данных:</p>
<ul>
<li><p><strong>SIFT128D (1 млн векторов):</strong> известный 128-мерный эталон, обычно используемый для поиска дескрипторов изображений. <em>(Размер сырого набора данных ≈ 488 МБ)</em></p></li>
<li><p><strong>Cohere768D (1M векторов):</strong> 768-мерный набор вложений, типичный для семантического поиска на основе трансформаторов. <em>(Размер необработанного набора данных ≈ 2930 МБ)</em></p></li>
</ul>
<p>Эти наборы данных отражают два различных сценария реального мира: компактные зрительные характеристики и большие семантические вкрапления.</p>
<h3 id="Results" class="common-anchor-header">Результаты</h3><p><strong>Sift128D1M (полный вектор ~488 МБ)</strong></p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/aisaq_53da7b566a.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p><strong>Cohere768D1M (полный вектор ~2930 МБ)</strong></p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Cohere768_D1_M_8dfa3dffb7.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="Analysis" class="common-anchor-header">Анализ</h3><p><strong>Набор данных SIFT128D</strong></p>
<p>На наборе данных SIFT128D AISAQ может сравниться с производительностью DISKANN, когда все данные PQ выстраиваются в ряд, так что необходимые данные каждого узла полностью помещаются на одной странице SSD объемом 4 КБ (INLINE_PQ = 48). При такой конфигурации каждый фрагмент информации, необходимый для поиска, размещается в колоке:</p>
<ul>
<li><p>Полный вектор: 512B</p></li>
<li><p>Список соседей: 48 × 4 + 4 = 196B</p></li>
<li><p>PQ-коды соседей: 48 × (512B × 0.125) ≈ 3072B</p></li>
<li><p>Итого: 3780B</p></li>
</ul>
<p>Поскольку весь узел умещается на одной странице, на один доступ требуется только один ввод-вывод, и AISAQ избегает случайного чтения внешних PQ-данных.</p>
<p>Однако, когда вставляется только часть PQ-данных, оставшиеся PQ-коды должны быть извлечены из другого места на диске. Это вводит дополнительные случайные операции ввода-вывода, которые резко увеличивают потребность в IOPS и приводят к значительному падению производительности.</p>
<p><strong>Набор данных Cohere768D</strong></p>
<p>На наборе данных Cohere768D AISAQ работает хуже, чем DISKANN. Причина в том, что 768-мерный вектор просто не помещается на одну страницу SSD объемом 4 КБ:</p>
<ul>
<li><p>Полный вектор: 3072B</p></li>
<li><p>Список соседей: 48 × 4 + 4 = 196B</p></li>
<li><p>PQ-коды соседей: 48 × (3072B × 0.125) ≈ 18432B</p></li>
<li><p>Итого: 21 700 Б (≈ 6 страниц)</p></li>
</ul>
<p>В этом случае, даже если все PQ-коды вложены, каждый узел занимает несколько страниц. Хотя количество операций ввода-вывода остается неизменным, каждый ввод-вывод должен передавать гораздо больше данных, что приводит к ускоренному расходованию пропускной способности SSD. Как только пропускная способность становится ограничивающим фактором, AISAQ не может идти в ногу с DISKANN - особенно на высокоразмерных рабочих нагрузках, где объемы данных на каждом узле быстро растут.</p>
<p><strong>Примечание:</strong></p>
<p>Схема хранения AISAQ обычно увеличивает размер индекса на диске на <strong>4-6×</strong>. Это сознательный компромисс: полные векторы, списки соседей и PQ-коды размещаются на диске, чтобы обеспечить эффективный одностраничный доступ при поиске. Хотя это увеличивает использование SSD, дисковая емкость значительно дешевле DRAM и легче масштабируется при больших объемах данных.</p>
<p>На практике пользователи могут настраивать этот компромисс, регулируя коэффициенты сжатия <code translate="no">INLINE_PQ</code> и PQ. Эти параметры позволяют сбалансировать производительность поиска, занимаемую площадь на диске и общую стоимость системы в зависимости от требований рабочей нагрузки, а не ограничиваться фиксированными лимитами памяти.</p>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Экономика современного оборудования меняется. Цены на DRAM остаются высокими, в то время как производительность твердотельных накопителей стремительно растет - пропускная способность дисков PCIe 5.0 теперь превышает <strong>14 ГБ/с</strong>. В результате архитектуры, в которых критически важные для поиска данные переносятся из дорогой DRAM в более доступные SSD-накопители, становятся все более привлекательными. Учитывая, что стоимость <strong>гигабайта</strong> емкости SSD в <strong>30 раз ниже, чем</strong> DRAM, эти различия больше не являются второстепенными - они оказывают существенное влияние на дизайн системы.</p>
<p>AISAQ отражает этот сдвиг. Благодаря отсутствию необходимости постоянно выделять большой объем памяти он позволяет системам векторного поиска масштабироваться в зависимости от объема данных и требований рабочей нагрузки, а не от ограничений DRAM. Такой подход соответствует более широкой тенденции к архитектурам "все в хранилище", в которых быстрые SSD играют центральную роль не только в сохранении данных, но и в активных вычислениях и поиске. Предлагая два режима работы - Performance и Scale - AiSAQ удовлетворяет требованиям как семантического поиска (требующего минимальной задержки), так и RAG (требующего очень высокого масштаба, но умеренной задержки).</p>
<p>Этот сдвиг вряд ли ограничится векторными базами данных. Подобные модели проектирования уже появляются в обработке графов, аналитике временных рядов и даже в некоторых частях традиционных реляционных систем, поскольку разработчики переосмысливают давние представления о том, где должны располагаться данные для достижения приемлемой производительности. По мере того как будут развиваться экономичные аппаратные средства, будут развиваться и системные архитектуры.</p>
<p>Более подробную информацию об обсуждаемых здесь конструкциях см. в документации:</p>
<ul>
<li><p><a href="https://milvus.io/docs/aisaq.md">AISAQ | Milvus Documentation</a></p></li>
<li><p><a href="https://milvus.io/docs/diskann.md">DISKANN | Документация Milvus</a></p></li>
</ul>
<p>У вас есть вопросы или вы хотите получить подробную информацию о любой функции последней версии Milvus? Присоединяйтесь к нашему<a href="https://discord.com/invite/8uyFbECzPX"> каналу Discord</a> или создавайте проблемы на<a href="https://github.com/milvus-io/milvus"> GitHub</a>. Вы также можете заказать 20-минутную индивидуальную сессию, чтобы получить знания, рекомендации и ответы на свои вопросы в<a href="https://milvus.io/blog/join-milvus-office-hours-to-get-support-from-vectordb-experts.md"> Milvus Office Hours</a>.</p>
<h2 id="Learn-More-about-Milvus-26-Features" class="common-anchor-header">Подробнее о возможностях Milvus 2.6<button data-href="#Learn-More-about-Milvus-26-Features" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><p><a href="https://milvus.io/blog/introduce-milvus-2-6-built-for-scale-designed-to-reduce-costs.md">Представляем Milvus 2.6: доступный векторный поиск в миллиардных масштабах</a></p></li>
<li><p><a href="https://milvus.io/blog/data-in-and-data-out-in-milvus-2-6.md">Представляем функцию встраивания: Как Milvus 2.6 оптимизирует векторизацию и семантический поиск</a></p></li>
<li><p><a href="https://milvus.io/blog/json-shredding-in-milvus-faster-json-filtering-with-flexibility.md">Измельчение JSON в Milvus: 88,9-кратное ускорение фильтрации JSON с гибкостью</a></p></li>
<li><p><a href="https://milvus.io/blog/unlocking-true-entity-level-retrieval-new-array-of-structs-and-max-sim-capabilities-in-milvus.md">Разблокирование истинного поиска на уровне сущностей: Новые возможности Array-of-Structs и MAX_SIM в Milvus</a></p></li>
<li><p><a href="https://milvus.io/blog/minhash-lsh-in-milvus-the-secret-weapon-for-fighting-duplicates-in-llm-training-data.md">MinHash LSH в Milvus: секретное оружие для борьбы с дубликатами в обучающих данных LLM </a></p></li>
<li><p><a href="https://milvus.io/blog/bring-vector-compression-to-the-extreme-how-milvus-serves-3%C3%97-more-queries-with-rabitq.md">Доведите векторное сжатие до крайности: как Milvus обслуживает в 3 раза больше запросов с помощью RaBitQ</a></p></li>
<li><p><a href="https://milvus.io/blog/benchmarks-lie-vector-dbs-deserve-a-real-test.md">Бенчмарки лгут - векторные БД заслуживают реальной проверки </a></p></li>
<li><p><a href="https://milvus.io/blog/we-replaced-kafka-pulsar-with-a-woodpecker-for-milvus.md">Мы заменили Kafka/Pulsar на Woodpecker для Milvus </a></p></li>
<li><p><a href="https://milvus.io/blog/how-to-filter-efficiently-without-killing-recall.md">Векторный поиск в реальном мире: как эффективно фильтровать, не убивая отзыв</a></p></li>
</ul>
