---
id: understanding-ivf-vector-index-how-It-works-and-when-to-choose-it-over-hnsw.md
title: >-
  Понимание векторного индекса ЭКО: Как он работает и когда его следует выбирать
  вместо HNSW
author: Jack Li
date: 2025-10-27T00:00:00.000Z
cover: assets.zilliz.com/ivf_cover_157df122bc.png
tag: Tutorials
recommend: false
publishToMedium: true
tags: 'Milvus, vector database'
meta_keywords: 'IVF, ANN, HNSW, vector index, vector database'
meta_title: How to Choose Between IVF and HNSW for ANN Vector Search
desc: >-
  Узнайте, как работает векторный индекс IVF, как он ускоряет поиск ANN и когда
  он превосходит HNSW по скорости, памяти и эффективности фильтрации.
origin: >-
  https://milvus.io/blog/understanding-ivf-vector-index-how-It-works-and-when-to-choose-it-over-hnsw.md
---
<p>В векторных базах данных часто требуется быстро найти наиболее похожие результаты среди огромных коллекций высокоразмерных векторов, таких как характеристики изображений, текстовые вкрапления или аудиопредставления. Без индекса единственным вариантом является сравнение вектора запроса с каждым вектором в наборе данных. Такой <strong>грубый поиск</strong> может сработать, если у вас несколько тысяч векторов, но когда вы имеете дело с десятками или сотнями миллионов, он становится невыносимо медленным и требует больших вычислительных затрат.</p>
<p>Вот тут-то и приходит на помощь <strong>приближенный</strong> поиск <strong>ближайших соседей (ANN)</strong>. Представьте, что вы ищете конкретную книгу в огромной библиотеке. Вместо того чтобы проверять каждую книгу по очереди, вы начинаете просматривать разделы, в которых она, скорее всего, содержится. Возможно, вы не получите <em>точно</em> таких же результатов, как при полном поиске, но вы будете очень близки к ним - и за долю времени. Короче говоря, ANN обменивает небольшую потерю точности на значительное увеличение скорости и масштабируемости.</p>
<p>Среди множества способов реализации ANN-поиска наиболее широко используются <strong>IVF (Inverted File)</strong> и <strong>HNSW (Hierarchical Navigable Small World)</strong>. Но IVF выделяется своей эффективностью и приспособленностью к крупномасштабному векторному поиску. В этой статье мы расскажем вам о том, как работает IVF и как он сравнивается с HNSW, чтобы вы могли понять их компромиссы и выбрать тот, который лучше всего подходит для вашей рабочей нагрузки.</p>
<h2 id="What-is-an-IVF-Vector-Index" class="common-anchor-header">Что такое векторный индекс ЭКО?<button data-href="#What-is-an-IVF-Vector-Index" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p><strong>IVF (Inverted File)</strong> - это один из наиболее широко используемых алгоритмов для ANN. Он заимствует свою основную идею из "перевернутого индекса", используемого в системах поиска текстов, только на этот раз вместо слов и документов мы имеем дело с векторами в высокоразмерном пространстве.</p>
<p>Подумайте об этом, как об организации огромной библиотеки. Если свалить каждую книгу (вектор) в одну гигантскую кучу, поиск нужной займет целую вечность. ЭКО решает эту проблему, сначала <strong>кластеризуя</strong> все векторы в группы, или <em>ведра</em>. Каждое ведро представляет собой "категорию" похожих векторов, определяемую <strong>центроидом -</strong>своего рода резюме или "ярлыком" для всего, что находится в этом кластере.</p>
<p>Когда поступает запрос, поиск происходит в два этапа:</p>
<p><strong>1. Поиск ближайших кластеров.</strong> Система ищет несколько кластеров, чьи центроиды ближе всего к вектору запроса - это все равно что направиться в два или три отдела библиотеки, где с наибольшей вероятностью есть ваша книга.</p>
<p><strong>2. Выполните поиск в этих кластерах.</strong> Как только вы окажетесь в нужных секциях, вам нужно будет просмотреть лишь небольшой набор книг, а не всю библиотеку.</p>
<p>Такой подход сокращает объем вычислений на порядки. Вы по-прежнему получаете высокоточные результаты, но гораздо быстрее.</p>
<h2 id="How-to-Build-an-IVF-Vector-Index" class="common-anchor-header">Как построить векторный индекс ЭКО<button data-href="#How-to-Build-an-IVF-Vector-Index" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Процесс построения векторного индекса ЭКО включает в себя три основных этапа: Кластеризация K-средних, назначение векторов и кодирование сжатия (опционально). Полный процесс выглядит следующим образом:</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/ivf_building_process_90c2966975.webp" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="Step-1-K-means-Clustering" class="common-anchor-header">Шаг 1: Кластеризация K-средних</h3><p>Сначала запустите кластеризацию k-средних на наборе данных X, чтобы разделить высокоразмерное векторное пространство на n-лист кластеров. Каждый кластер представлен центроидом, который хранится в таблице центроидов C. Количество центроидов, nlist, является ключевым гиперпараметром, определяющим, насколько тонкой будет кластеризация.</p>
<p>Вот как работает k-means под капотом:</p>
<ul>
<li><p><strong>Инициализация:</strong> Случайным образом выбираем <em>nlist</em> векторов в качестве начальных центроидов.</p></li>
<li><p><strong>Назначение:</strong> Для каждого вектора вычисляется расстояние до всех центроидов и он присваивается ближайшему из них.</p></li>
<li><p><strong>Обновление:</strong> Для каждого кластера вычислите среднее значение его векторов и установите его в качестве нового центроида.</p></li>
<li><p><strong>Итерация и сходимость:</strong> Повторяйте назначение и обновление до тех пор, пока центроиды не перестанут существенно меняться или не будет достигнуто максимальное количество итераций.</p></li>
</ul>
<p>Когда k-средние сходятся, полученные центроиды из n-листа образуют "индексный каталог" ЭКО. Они определяют, как грубо разбит набор данных, позволяя впоследствии быстро сузить пространство поиска.</p>
<p>Вспомните аналогию с библиотекой: обучение центроидов похоже на решение о том, как сгруппировать книги по темам:</p>
<ul>
<li><p>Большой nlist означает больше разделов, каждый из которых содержит меньше, более конкретных книг.</p></li>
<li><p>Меньший nlist означает меньшее количество разделов, каждый из которых охватывает более широкий, более смешанный спектр тем.</p></li>
</ul>
<h3 id="Step-2-Vector-Assignment" class="common-anchor-header">Шаг 2: Назначение векторов</h3><p>Далее каждый вектор присваивается кластеру, к центроиду которого он ближе всего, формируя инвертированные списки (List_i). В каждом инвертированном списке хранятся идентификаторы и информация о хранении всех векторов, принадлежащих к данному кластеру.</p>
<p>Этот шаг можно сравнить с расстановкой книг по соответствующим разделам. Когда вы потом будете искать нужное название, вам нужно будет проверить только те несколько разделов, в которых оно, скорее всего, есть, вместо того чтобы бродить по всей библиотеке.</p>
<h3 id="Step-3-Compression-Encoding-Optional" class="common-anchor-header">Шаг 3: Кодирование со сжатием (необязательно)</h3><p>Чтобы сэкономить память и ускорить вычисления, векторы в каждом кластере могут пройти через кодирование со сжатием. Существует два распространенных подхода:</p>
<ul>
<li><p><strong>SQ8 (скалярное квантование):</strong> Этот метод квантует каждое измерение вектора в 8 бит. Для стандартного вектора <code translate="no">float32</code> каждое измерение обычно занимает 4 байта. При использовании SQ8 оно сокращается до 1 байта - достигается коэффициент сжатия 4:1, при этом геометрия вектора остается практически нетронутой.</p></li>
<li><p><strong>PQ (Product Quantization):</strong> Разбивает высокоразмерный вектор на несколько подпространств. Например, 128-мерный вектор может быть разбит на 8 подвекторов по 16 измерений каждый. В каждом подпространстве предварительно обучается небольшая кодовая книга (обычно с 256 записями), и каждый подвектор представлен 8-битным индексом, указывающим на ближайшую запись кодовой книги. Это означает, что исходный 128-D вектор <code translate="no">float32</code> (который требует 512 байт) может быть представлен всего 8 байтами (8 подпространств × 1 байт каждое), что позволяет достичь коэффициента сжатия 64:1.</p></li>
</ul>
<h2 id="How-to-Use-the-IVF-Vector-Index-for-Search" class="common-anchor-header">Как использовать индекс ЭКО-вектора для поиска<button data-href="#How-to-Use-the-IVF-Vector-Index-for-Search" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>После построения таблицы центроидов, инвертированных списков, а также кодировщика и кодовых книг сжатия (опционально) индекс ЭКО можно использовать для ускорения поиска сходства. Обычно этот процесс состоит из трех основных этапов, как показано ниже:</p>
<p>
  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/ivf_search_process_025d3f444f.webp" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<h3 id="Step-1-Calculate-distances-from-the-query-vector-to-all-centroids" class="common-anchor-header">Шаг 1: Вычисление расстояний от вектора запроса до всех центроидов</h3><p>Когда поступает вектор запроса q, система сначала определяет, к каким кластерам он, скорее всего, относится. Затем она вычисляет расстояние между q и каждым центроидом в таблице центроидов C - обычно используя евклидово расстояние или внутреннее произведение в качестве метрики сходства. Затем центроиды сортируются по расстоянию до вектора запроса, в результате чего получается упорядоченный список от ближайшего к наиболее удаленному.</p>
<p>Например, как показано на рисунке, порядок следующий: C4 &lt; C2 &lt; C1 &lt; C3 &lt; C5.</p>
<h3 id="Step-2-Select-the-nearest-nprobe-clusters" class="common-anchor-header">Шаг 2: Выбор ближайших кластеров nprobe</h3><p>Чтобы не сканировать весь набор данных, ЭКО выполняет поиск только в верхних <em>nprobe-кластерах</em>, которые находятся ближе всего к вектору запроса.</p>
<p>Параметр nprobe определяет область поиска и напрямую влияет на баланс между скоростью и запоминанием:</p>
<ul>
<li><p>Меньший nprobe приводит к более быстрым запросам, но может снизить запоминаемость.</p></li>
<li><p>Больший nprobe улучшает отзыв, но увеличивает задержку.</p></li>
</ul>
<p>В реальных системах nprobe может динамически настраиваться в зависимости от бюджета задержки или требований к точности. В приведенном выше примере, если nprobe = 2, система будет искать только в кластере 2 и кластере 4 - двух ближайших кластерах.</p>
<h3 id="Step-3-Search-the-nearest-neighbor-in-the-selected-clusters" class="common-anchor-header">Шаг 3: Поиск ближайшего соседа в выбранных кластерах</h3><p>После того как кластеры-кандидаты выбраны, система сравнивает вектор запроса q с векторами, хранящимися в них. Существует два основных режима сравнения:</p>
<ul>
<li><p><strong>Точное сравнение (IVF_FLAT)</strong>: Система извлекает исходные векторы из выбранных кластеров и вычисляет их расстояния до q напрямую, получая наиболее точные результаты.</p></li>
<li><p><strong>Приближенное сравнение (IVF_PQ / IVF_SQ8)</strong>: Когда используется сжатие PQ или SQ8, система применяет <strong>метод таблиц поиска</strong> для ускорения вычисления расстояний. Перед началом поиска она предварительно вычисляет расстояния между вектором запроса и каждой записью кодовой книги. Затем для каждого вектора можно просто "просмотреть и просуммировать" эти предварительно вычисленные расстояния, чтобы оценить сходство.</p></li>
</ul>
<p>Наконец, результаты-кандидаты из всех искомых кластеров объединяются и ранжируются, в результате чего на выходе получается Top-k наиболее похожих векторов.</p>
<h2 id="IVF-In-Practice" class="common-anchor-header">Практическое применение ЭКО<button data-href="#IVF-In-Practice" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>После того как вы поняли, как <strong>строятся</strong> и <strong>ищутся</strong> векторные индексы IVF, следующим шагом будет их применение в реальных рабочих нагрузках. На практике вам часто придется балансировать между <strong>производительностью</strong>, <strong>точностью</strong> и <strong>использованием памяти</strong>. Ниже приведены некоторые практические рекомендации, взятые из инженерного опыта.</p>
<h3 id="How-to-Choose-the-Right-nlist" class="common-anchor-header">Как выбрать правильный nlist</h3><p>Как уже упоминалось, параметр nlist определяет количество кластеров, на которые делится набор данных при построении индекса ЭКО.</p>
<ul>
<li><p><strong>Больший nlist</strong>: Создает более мелкие кластеры, то есть каждый кластер содержит меньшее количество векторов. Это уменьшает количество векторов, сканируемых во время поиска, и, как правило, приводит к более быстрым запросам. Но построение индекса занимает больше времени, а таблица центроидов потребляет больше памяти.</p></li>
<li><p><strong>Меньший nlist</strong>: Ускоряет построение индекса и уменьшает потребление памяти, но каждый кластер становится более "переполненным". Каждый запрос должен просканировать больше векторов в кластере, что может привести к узким местам в производительности.</p></li>
</ul>
<p>Исходя из этих компромиссов, вот практическое эмпирическое правило:</p>
<p>Для наборов данных <strong>миллионного масштаба</strong> хорошей отправной точкой является <strong>nlist ≈ √n</strong> (n - количество векторов в индексируемом осколке данных).</p>
<p>Например, если у вас 1 миллион векторов, попробуйте nlist = 1 000. Для больших наборов данных - десятков или сотен миллионов - большинство векторных баз данных разбивают данные так, чтобы каждый из них содержал около миллиона векторов, что делает это правило практичным.</p>
<p>Поскольку nlist фиксируется при создании индекса, его изменение в дальнейшем означает перестройку всего индекса. Поэтому лучше всего экспериментировать на ранних этапах. Протестируйте несколько значений - лучше всего в степени двойки (например, 1024, 2048) - чтобы найти оптимальное значение, которое сбалансирует скорость, точность и память для вашей рабочей нагрузки.</p>
<h3 id="How-to-Tune-nprobe" class="common-anchor-header">Как настроить nprobe</h3><p>Параметр nprobe управляет количеством кластеров, в которых выполняется поиск во время запроса. Он напрямую влияет на компромисс между отзывом и задержкой.</p>
<ul>
<li><p><strong>Больший nprobe</strong>: Охватывает больше кластеров, что приводит к увеличению запоминания, но также и к увеличению задержки. Задержка, как правило, линейно увеличивается с ростом числа перебираемых кластеров.</p></li>
<li><p><strong>Меньший nprobe</strong>: Охватывает меньшее количество кластеров, что приводит к меньшей задержке и более быстрым запросам. Однако при этом может быть пропущено несколько истинных ближайших соседей, что несколько снижает запоминаемость и точность результатов.</p></li>
</ul>
<p>Если ваше приложение не очень чувствительно к задержке, стоит поэкспериментировать с nprobe динамически - например, протестировать значения от 1 до 16, чтобы посмотреть, как меняются отзыв и задержка. Цель состоит в том, чтобы найти ту точку, где отзыв будет приемлемым, а задержка останется в пределах целевого диапазона.</p>
<p>Поскольку nprobe является параметром поиска во время выполнения, его можно изменять "на лету", не требуя перестройки индекса. Это обеспечивает быструю, недорогую и очень гибкую настройку для различных рабочих нагрузок и сценариев запросов.</p>
<h3 id="Common-Variants-of-the-IVF-Index" class="common-anchor-header">Общие варианты индекса ЭКО</h3><p>При построении индекса IVF необходимо решить, использовать ли кодировку сжатия для векторов в каждом кластере, и если да, то какой метод использовать.</p>
<p>Это приводит к появлению трех распространенных вариантов индекса ЭКО:</p>
<table>
<thead>
<tr><th><strong>Вариант ЭКО</strong></th><th><strong>Ключевые особенности</strong></th><th><strong>Примеры использования</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>IVF_FLAT</strong></td><td>Хранит необработанные векторы в каждом кластере без сжатия. Обеспечивает наивысшую точность, но при этом потребляет больше всего памяти.</td><td>Идеально подходит для средних наборов данных (до сотен миллионов векторов), где требуется высокая отзывчивость (95 %+).</td></tr>
<tr><td><strong>IVF_PQ</strong></td><td>Применяет Product Quantization (PQ) для сжатия векторов в кластерах. Регулируя степень сжатия, можно значительно сократить потребление памяти.</td><td>Подходит для крупномасштабного поиска векторов (сотни миллионов и более), где допустима некоторая потеря точности. При коэффициенте сжатия 64:1 запоминание обычно составляет около 70 %, но может достигать 90 % и выше при уменьшении коэффициента сжатия.</td></tr>
<tr><td><strong>IVF_SQ8</strong></td><td>Использует скалярное квантование (SQ8) для квантования векторов. Потребление памяти находится между IVF_FLAT и IVF_PQ.</td><td>Идеально подходит для крупномасштабного векторного поиска, где необходимо поддерживать относительно высокий уровень запоминания (90 %+) при одновременном повышении эффективности.</td></tr>
</tbody>
</table>
<h2 id="IVF-vs-HNSW-Pick-What-Fits" class="common-anchor-header">ЭКО против HNSW: выбирайте то, что подходит<button data-href="#IVF-vs-HNSW-Pick-What-Fits" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Помимо IVF, <strong>HNSW (Hierarchical Navigable Small World)</strong> является еще одним широко используемым векторным индексом in-memory. В таблице ниже приведены основные различия между ними.</p>
<table>
<thead>
<tr><th></th><th><strong>ЭКО</strong></th><th><strong>HNSW</strong></th></tr>
</thead>
<tbody>
<tr><td><strong>Концепция алгоритма</strong></td><td>Кластеризация и разбиение на группы</td><td>Многослойная навигация по графу</td></tr>
<tr><td><strong>Использование памяти</strong></td><td>Относительно низкий</td><td>Относительно высокий</td></tr>
<tr><td><strong>Скорость построения индекса</strong></td><td>Быстро (требуется только кластеризация)</td><td>Медленная (требуется построение многослойного графа)</td></tr>
<tr><td><strong>Скорость выполнения запросов (без фильтрации)</strong></td><td>Быстро, зависит от <em>nprobe</em></td><td>Чрезвычайно быстро, но с логарифмической сложностью</td></tr>
<tr><td><strong>Скорость запроса (с фильтрацией)</strong></td><td>Стабильно - выполняется грубая фильтрация на уровне центроидов для сужения круга кандидатов</td><td>Нестабильно - особенно при высоком коэффициенте фильтрации (90 %+) граф становится фрагментированным и может деградировать до почти полного обхода графа, что даже медленнее, чем поиск методом грубой силы</td></tr>
<tr><td><strong>Скорость отзыва</strong></td><td>Зависит от того, используется ли сжатие; без квантования запоминание может достигать <strong>95 %+</strong></td><td>Обычно выше, около <strong>98%+</strong></td></tr>
<tr><td><strong>Ключевые параметры</strong></td><td><em>nlist</em>, <em>nprobe</em></td><td><em>m</em>, <em>ef_construction</em>, <em>ef_search</em></td></tr>
<tr><td><strong>Примеры использования</strong></td><td>Когда память ограничена, но требуется высокая производительность запроса и запоминание; хорошо подходит для поиска с условиями фильтрации</td><td>Когда памяти достаточно, а цель - чрезвычайно высокая запоминаемость и производительность запроса, но фильтрация не нужна, или коэффициент фильтрации низкий.</td></tr>
</tbody>
</table>
<p>В реальных приложениях очень часто встречаются условия фильтрации - например, "искать только векторы от конкретного пользователя" или "ограничить результаты определенным временным диапазоном". Из-за различий в алгоритмах IVF обычно справляется с отфильтрованным поиском эффективнее, чем HNSW.</p>
<p>Сила IVF заключается в его двухуровневом процессе фильтрации. Сначала он выполняет грубый фильтр на уровне центроида (кластера), чтобы быстро сузить набор кандидатов, а затем выполняет тонкие расчеты расстояний в выбранных кластерах. Это обеспечивает стабильную и предсказуемую производительность, даже если отфильтровывается большая часть данных.</p>
<p>В отличие от этого, HNSW основан на обходе графа. Из-за своей структуры он не может напрямую использовать условия фильтрации во время обхода. При низком коэффициенте фильтрации это не вызывает серьезных проблем. Однако когда коэффициент фильтрации высок (например, отфильтровывается более 90 % данных), оставшийся граф часто становится фрагментированным, образуя множество "изолированных узлов". В таких случаях поиск может превратиться в почти полный обход графа - иногда даже хуже, чем поиск "грубой силой".</p>
<p>На практике ЭКО-индексы уже используются во многих важных случаях в различных областях:</p>
<ul>
<li><p><strong>Поиск в электронной коммерции:</strong> Пользователь может загрузить изображение товара и мгновенно найти визуально похожие товары из миллионов объявлений.</p></li>
<li><p><strong>Поиск патентов:</strong> По краткому описанию система может найти наиболее семантически связанные патенты из огромной базы данных - гораздо эффективнее, чем традиционный поиск по ключевым словам.</p></li>
<li><p><strong>Базы знаний RAG:</strong> ЭКО помогает извлекать наиболее релевантный контекст из миллионов документов-арендаторов, благодаря чему модели ИИ генерируют более точные и обоснованные ответы.</p></li>
</ul>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Выбор правильного индекса зависит от конкретного случая использования. Если вы работаете с большими массивами данных или нуждаетесь в поддержке фильтрованного поиска, лучше всего подойдет IVF. По сравнению с индексами на основе графов, такими как HNSW, IVF обеспечивает более быстрое построение индексов, меньшее потребление памяти и высокий баланс между скоростью и точностью.</p>
<p><a href="https://milvus.io/">Milvus</a>, самая популярная векторная база данных с открытым исходным кодом, обеспечивает полную поддержку всего семейства IVF, включая IVF_FLAT, IVF_PQ и IVF_SQ8. Вы можете легко экспериментировать с этими типами индексов и найти тот, который лучше всего соответствует вашим потребностям в производительности и памяти. Полный список индексов, которые поддерживает Milvus, можно найти на этой <a href="https://milvus.io/docs/index-explained.md">странице Milvus Index doc</a>.</p>
<p>Если вы создаете поиск по изображениям, рекомендательные системы или базы знаний RAG, попробуйте IVF-индексацию в Milvus - и вы увидите, как эффективен крупномасштабный векторный поиск в действии.</p>
