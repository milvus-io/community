---
id: why-ai-agents-like-openclaw-burn-through-tokens-and-how-to-cut-costs.md
title: Почему ИИ-агенты вроде OpenClaw сжигают токены и как сократить расходы
author: Min Yin
date: 2026-2-28
cover: assets.zilliz.com/Blog_Open_Claw_Burning_Through_Tokens_1_39b7ee4fdf.jpg
tag: Engineering
recommend: false
publishToMedium: true
tags: 'OpenClaw, Token Optimization, Vector Search, AI Agents, Milvus'
meta_keywords: >-
  OpenClaw token costs, OpenClaw token optimization, reduce OpenClaw API costs,
  hybrid search BM25 vector, AI agent memory, memsearch, Milvus
meta_title: |
  Why AI Agents like OpenClaw Burn Through Tokens and How to Cut Costs
desc: >-
  Почему счета за токены у OpenClaw и других ИИ-агентов растут, и как это
  исправить с помощью BM25 + векторного поиска (index1, QMD, Milvus) и памяти,
  ориентированной на Markdown (memsearch).
origin: >-
  https://milvus.io/blog/why-ai-agents-like-openclaw-burn-through-tokens-and-how-to-cut-costs.md
---
<custom-h1>Почему ИИ-агенты вроде OpenClaw сжигают токены и как сократить расходы</custom-h1><p>Если вы проводили время с <a href="https://milvus.io/blog/openclaw-formerly-clawdbot-moltbot-explained-a-complete-guide-to-the-autonomous-ai-agent.md">OpenClaw</a> (ранее Clawdbot и Moltbot), вы уже знаете, насколько хорош этот ИИ-агент. Он быстрый, локальный, гибкий и способен выполнять удивительно сложные рабочие процессы в Slack, Discord, вашей кодовой базе и практически во всем остальном, к чему вы его подключите. Но как только вы начинаете использовать его всерьез, быстро проявляется одна закономерность: <strong>использование токенов начинает расти.</strong></p>
<p>Это не вина конкретно OpenClaw - так сегодня ведет себя большинство агентов ИИ. Они запускают вызов LLM практически для всего: поиска файла, планирования задачи, написания заметки, выполнения инструмента или последующего вопроса. А поскольку универсальной валютой этих вызовов являются токены, каждое действие имеет свою стоимость.</p>
<p>Чтобы понять, откуда берутся эти затраты, нам нужно заглянуть под капот к двум основным участникам:</p>
<ul>
<li><strong>Поиск:</strong> Плохо построенный поиск втягивает в себя огромные объемы контекста - целые файлы, журналы, сообщения и участки кода, которые на самом деле не нужны модели.</li>
<li><strong>Память:</strong> Хранение неважной информации заставляет агента перечитывать и обрабатывать ее при последующих вызовах, что со временем увеличивает расход маркеров.</li>
</ul>
<p>Обе эти проблемы приводят к увеличению эксплуатационных расходов без улучшения возможностей.</p>
<h2 id="How-AI-Agents-Like-OpenClaw-Actually-Perform-Searches--and-Why-That-Burns-Tokens" class="common-anchor-header">Как агенты ИИ, подобные OpenClaw, на самом деле выполняют поиск - и почему это сжигает токены<button data-href="#How-AI-Agents-Like-OpenClaw-Actually-Perform-Searches--and-Why-That-Burns-Tokens" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Когда агенту нужна информация из вашей кодовой базы или библиотеки документов, он обычно выполняет эквивалент общепроектного <strong>Ctrl+F</strong>. Каждая совпадающая строка возвращается - без ранжирования, без фильтрации и без приоритетов. В Claude Code это реализовано с помощью специального инструмента Grep, построенного на ripgrep. В OpenClaw нет встроенного инструмента поиска по кодовой базе, но его инструмент exec позволяет базовой модели выполнять любые команды, а загруженные навыки могут подсказать агенту, как использовать инструменты вроде rg. В обоих случаях поиск по кодовой базе возвращает совпадения ключевых слов без ранжирования и фильтрации.</p>
<p>Такой грубый подход хорошо работает в небольших проектах. Но по мере роста репозиториев растет и цена. Нерелевантные совпадения нагромождаются в контекстном окне LLM, заставляя модель читать и обрабатывать тысячи лексем, которые ей на самом деле не нужны. Один неохватный поиск может потянуть за собой полные файлы, огромные блоки комментариев или журналы, в которых есть ключевое слово, но нет основного намерения. Повторите эту схему в течение длительного сеанса отладки или исследования, и объем информации быстро увеличится.</p>
<p>И OpenClaw, и Claude Code пытаются управлять этим ростом. OpenClaw обрезает избыточные результаты работы инструментов и уплотняет длинные истории разговоров, а Claude Code ограничивает вывод данных из файлов и поддерживает уплотнение контекста. Эти меры работают, но только после того, как раздутый запрос уже выполнен. Неранжированные результаты поиска все еще потребляют токены, и вы все еще платите за них. Управление контекстом помогает будущим обращениям, а не первоначальному вызову, который вызвал потери.</p>
<h2 id="How-AI-Agent-Memory-Works-and-Why-It-Also-Costs-Tokens" class="common-anchor-header">Как работает память агентов ИИ и почему она тоже стоит жетонов<button data-href="#How-AI-Agent-Memory-Works-and-Why-It-Also-Costs-Tokens" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Поиск - не единственный источник лишних жетонов. Каждый фрагмент контекста, который агент извлекает из памяти, должен быть также загружен в контекстное окно LLM, и это тоже стоит токенов.</p>
<p>API LLM, на которые сегодня опирается большинство агентов, не имеют состояния: API сообщений Anthropic требует полной истории разговора при каждом запросе, API завершения чата OpenAI работает аналогичным образом. Даже более новый государственный API Responses от OpenAI, который управляет состоянием разговора на стороне сервера, по-прежнему требует полного контекстного окна при каждом вызове. Память, загруженная в контекст, стоит токенов независимо от того, как она туда попала.</p>
<p>Чтобы обойти эту проблему, агентские фреймворки записывают заметки в файлы на диске и загружают соответствующие заметки обратно в контекстное окно, когда они нужны агенту. Например, OpenClaw хранит курируемые заметки в файле MEMORY.md и добавляет ежедневные журналы в файлы Markdown с временными метками, а затем индексирует их с помощью гибридного BM25 и векторного поиска, чтобы агент мог вызвать соответствующий контекст по требованию.</p>
<p>Дизайн памяти OpenClaw работает хорошо, но он требует полной экосистемы OpenClaw: процесса шлюза, соединений с платформой обмена сообщениями и остального стека. То же самое можно сказать и о памяти Claude Code, которая привязана к его CLI. Если вы создаете собственного агента вне этих платформ, вам понадобится отдельное решение. В следующем разделе мы рассмотрим доступные инструменты для решения обеих проблем.</p>
<h2 id="How-to-Stop-OpenClaw-From-Burning-Through-Tokens" class="common-anchor-header">Как остановить OpenClaw от сжигания токенов<button data-href="#How-to-Stop-OpenClaw-From-Burning-Through-Tokens" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Если вы хотите сократить количество потребляемых OpenClaw токенов, есть два рычага, за которые вы можете потянуть.</p>
<ul>
<li>Первый - <strong>улучшение поиска</strong> - замена дампов ключевых слов в стиле grep на ранжированные, ориентированные на релевантность инструменты поиска, чтобы модель видела только ту информацию, которая действительно важна.</li>
<li>Второй - <strong>улучшение памяти</strong> - переход от непрозрачного, зависящего от фреймворка хранилища к тому, что можно понять, проверить и контролировать.</li>
</ul>
<h3 id="Replacing-grep-with-Better-Retrieval-index1-QMD-and-Milvus" class="common-anchor-header">Замена grep на лучший поиск: index1, QMD и Milvus</h3><p>Многие агенты кодирования ИИ осуществляют поиск в кодовых базах с помощью grep или ripgrep. В Claude Code есть специальный инструмент Grep, построенный на ripgrep. В OpenClaw нет встроенного инструмента для поиска по кодовой базе, но его инструмент exec позволяет базовой модели выполнять любые команды, а такие навыки, как ripgrep или QMD, могут быть загружены, чтобы управлять поиском агента. Без навыка, ориентированного на поиск, агент будет использовать любой подход, который выберет базовая модель. Суть проблемы одинакова для всех агентов: без ранжированного поиска совпадения ключевых слов попадают в контекстное окно без фильтрации.</p>
<p>Это работает, когда проект достаточно мал, чтобы каждое совпадение удобно помещалось в контекстном окне. Проблема начинается, когда кодовая база или библиотека документов разрастается до такой степени, что ключевое слово дает десятки или сотни совпадений и агенту приходится загружать их все в подсказку. При таких масштабах вам нужны результаты, ранжированные по релевантности, а не просто отфильтрованные по совпадению.</p>
<p>Стандартное решение - гибридный поиск, сочетающий два взаимодополняющих метода ранжирования:</p>
<ul>
<li>BM25 оценивает каждый результат по тому, как часто и насколько уникально термин встречается в данном документе. Сфокусированный файл, в котором слово "аутентификация" упоминается 15 раз, ранжируется выше, чем обширный файл, в котором оно упоминается один раз.</li>
<li>Векторный поиск преобразует текст в числовые представления смысла, поэтому "аутентификация" может соответствовать "потоку входа" или "управлению сеансами", даже если у них нет общих ключевых слов.</li>
</ul>
<p>Ни один из методов сам по себе не является достаточным: BM25 пропускает перефразированные термины, а векторный поиск - точные термины, например коды ошибок. Комбинирование обоих методов и объединение ранжированных списков с помощью алгоритма слияния позволяет устранить оба пробела.</p>
<p>Приведенные ниже инструменты реализуют эту схему в разных масштабах. Grep - базовый инструмент, с которого все начинают. index1, QMD и Milvus добавляют гибридный поиск с возрастающей производительностью.</p>
<h4 id="index1-fast-hybrid-search-on-a-single-machine" class="common-anchor-header">index1: быстрый гибридный поиск на одной машине</h4><p><a href="https://github.com/gladego/index1">index1</a> - это инструмент CLI, который упаковывает гибридный поиск в один файл базы данных SQLite. FTS5 обрабатывает BM25, sqlite-vec - векторное сходство, а RRF объединяет ранжированные списки. Вкрапления генерируются Ollama локально, поэтому ничего не покидает вашу машину.</p>
<p>index1 разбивает код по структуре, а не по количеству строк: Markdown-файлы разбиваются по заголовкам, Python-файлы - по AST, JavaScript и TypeScript - по шаблонам regex. Это означает, что результаты поиска возвращают целостные блоки, такие как полная функция или полный раздел документации, а не произвольные диапазоны строк, которые обрываются на середине блока. Время отклика составляет от 40 до 180 мс для гибридных запросов. Без Ollama он возвращается к BM25-only, который по-прежнему ранжирует результаты, а не выводит каждое совпадение в контекстное окно.</p>
<p>index1 также включает модуль эпизодической памяти для хранения извлеченных уроков, первопричин ошибок и архитектурных решений. Эти воспоминания хранятся в той же базе данных SQLite, что и индекс кода, а не в виде отдельных файлов.</p>
<p>Примечание: index1 - это проект на ранней стадии (0 звезд, 4 фиксации на февраль 2026 года). Оцените его в сравнении с вашей собственной кодовой базой, прежде чем делать коммит.</p>
<ul>
<li><strong>Лучше всего подходит для</strong>: одиночных разработчиков или небольших команд с кодовой базой, умещающейся на одной машине, которые ищут быстрое улучшение по сравнению с grep.</li>
<li><strong>Перерастет его, если</strong>: вам нужен многопользовательский доступ к одному и тому же индексу, или ваши данные превышают тот объем, который удобно обрабатывать в одном файле SQLite.</li>
</ul>
<h4 id="QMD-higher-accuracy-through-local-LLM-re-ranking" class="common-anchor-header">QMD: более высокая точность за счет локального переранжирования LLM</h4><p><a href="https://github.com/tobi/qmd">QMD</a> (Query Markup Documents), созданный основателем Shopify Тоби Лютке, добавляет третий этап: LLM-реранжирование. После того как BM25 и векторный поиск возвращают кандидатов, локальная языковая модель перечитывает лучшие результаты и упорядочивает их по фактической релевантности вашему запросу. Это позволяет выявить случаи, когда и ключевые слова, и семантические совпадения дают правдоподобные, но неверные результаты.</p>
<p>QMD работает полностью на вашей машине, используя три модели GGUF общим объемом около 2 ГБ: модель встраивания (embeddinggemma-300M), кросс-кодерный реранкер (Qwen3-Reranker-0.6B) и модель расширения запроса (qmd-query-expansion-1.7B). Все три загружаются автоматически при первом запуске. Никаких вызовов облачного API, никаких ключей API.</p>
<p>Компромиссом является время холодного старта: загрузка трех моделей с диска занимает примерно 15-16 секунд. QMD поддерживает режим постоянного сервера (qmd mcp), который сохраняет модели в памяти между запросами, устраняя штраф за холодный старт при повторных запросах.</p>
<ul>
<li><strong>Лучше всего подходит для:</strong> сред, критичных к конфиденциальности, где никакие данные не могут покинуть вашу машину, и где точность поиска важнее времени отклика.</li>
<li><strong>Перерастайте его, если:</strong> вам нужны ответы за доли секунды, общий командный доступ, или ваш набор данных превышает возможности одной машины.</li>
</ul>
<h4 id="Milvus-hybrid-search-at-team-and-enterprise-scale" class="common-anchor-header">Milvus: гибридный поиск в масштабах команды и предприятия</h4><p>Вышеописанные инструменты для одной машины хорошо подходят для индивидуальных разработчиков, но их возможности ограничены, когда нескольким людям или агентам требуется доступ к одной и той же базе знаний. <a href="https://github.com/milvus-io/milvus"></a></p>
<p><a href="https://github.com/milvus-io/milvus">Milvus</a> - это векторная база данных с открытым исходным кодом, созданная для следующего этапа: распределенная, многопользовательская и способная работать с миллиардами векторов.</p>
<p>Ключевая особенность Milvus для этого случая - встроенный Sparse-BM25, доступный с Milvus 2.5 и значительно ускоренный в 2.6. Вы предоставляете необработанный текст, и Milvus выполняет его внутреннюю токенизацию с помощью анализатора, построенного на tantivy, а затем преобразует результат в разреженные векторы, которые предварительно вычисляются и сохраняются во время индексации.</p>
<p>Поскольку представление BM25 уже сохранено, при поиске не нужно пересчитывать оценки на лету. Эти разреженные векторы хранятся вместе с плотными векторами (семантическими вкраплениями) в той же Коллекции. Во время запроса вы объединяете оба сигнала с помощью ранжировщика, такого как RRFRanker, который Milvus предоставляет из коробки. Та же гибридная схема поиска, что и в Index1 и QMD, но работающая на горизонтально масштабируемой инфраструктуре.</p>
<p>Milvus также предоставляет возможности, которые не под силу одномашинным инструментам: многопользовательская изоляция (отдельные базы данных или коллекции для каждой команды), репликация данных с автоматическим обходом отказа, а также горячее/холодное хранение данных для экономичного хранения. Для агентов это означает, что несколько разработчиков или несколько экземпляров агентов могут одновременно запрашивать одну и ту же базу знаний, не наступая на данные друг друга.</p>
<ul>
<li><strong>Лучшее решение для</strong>: нескольких разработчиков или агентов, совместно использующих базу знаний, больших или быстрорастущих наборов документов, а также для производственных сред, нуждающихся в репликации, отказоустойчивости и контроле доступа.</li>
</ul>
<p>Подведем итоги:</p>
<table>
<thead>
<tr><th>Инструмент</th><th>Этап</th><th>Развертывание</th><th>Миграционный сигнал</th></tr>
</thead>
<tbody>
<tr><td>Claude Native Grep</td><td>Прототипирование</td><td>Встроенный, без необходимости настройки</td><td>Счета растут или запросы замедляются</td></tr>
<tr><td>индекс1</td><td>Одномашинный (скорость)</td><td>Локальный SQLite + Ollama</td><td>Нужен многопользовательский доступ или данные перерастают рамки одной машины</td></tr>
<tr><td>QMD</td><td>Одномашинный (точность)</td><td>Три локальные модели GGUF</td><td>Нужны общие индексы для команды</td></tr>
<tr><td>Milvus</td><td>Команда или производство</td><td>Распределенный кластер</td><td>Большие наборы документов или многопользовательские требования</td></tr>
</tbody>
</table>
<h3 id="Reducing-AI-Agent-Token-Costs-by-Giving-Them-Persistent-Editable-Memory-with-memsearch" class="common-anchor-header">Сокращение затрат на токены агентов ИИ за счет предоставления им постоянной, редактируемой памяти с помощью memsearch</h3><p>Оптимизация поиска позволяет сократить трату токенов на один запрос, но это не помогает справиться с тем, что агент сохраняет между сессиями.</p>
<p>Каждый фрагмент контекста, который агент вспоминает из памяти, должен быть загружен в подсказку, и на это тоже тратятся жетоны. Вопрос не в том, хранить ли память, а в том, как. От способа хранения зависит, сможете ли вы увидеть, что запомнил агент, исправить это, если что-то не так, и взять с собой при смене инструмента.</p>
<p>Большинство фреймворков терпят неудачу по всем трем пунктам. Mem0 и Zep хранят все в векторной базе данных, что работает для поиска, но делает память..:</p>
<ul>
<li><strong>Непрозрачной.</strong> Вы не можете увидеть, что помнит агент, не обратившись к API.</li>
<li><strong>Трудно редактируемой.</strong> Исправление или удаление памяти означает вызов API, а не открытие файла.</li>
<li><strong>Заблокирована.</strong> Переход на другой фреймворк означает экспорт, конвертацию и повторный импорт данных.</li>
</ul>
<p>OpenClaw использует другой подход. Вся память хранится в обычных файлах Markdown на диске. Агент автоматически пишет ежедневные журналы, а человек может открывать и редактировать любой файл памяти напрямую. Это решает все три проблемы: память читаема, редактируема и переносима.</p>
<p>Компромисс заключается в накладных расходах на развертывание. Запуск памяти OpenClaw означает запуск всей экосистемы OpenClaw: процесса шлюза, соединений платформы обмена сообщениями и всего остального стека. Для команд, уже использующих OpenClaw, это нормально. Для всех остальных барьер слишком высок. <strong>memsearch</strong> был создан, чтобы устранить этот пробел: он извлекает шаблон памяти OpenClaw, основанный на Markdown, в отдельную библиотеку, которая работает с любым агентом.</p>
<p><strong><a href="https://github.com/zilliztech/memsearch">memsearch</a></strong>, созданный Zilliz (командой, стоящей за Milvus), рассматривает файлы в формате Markdown как единый источник истины. В файле MEMORY.md хранятся долгосрочные факты и решения, которые вы пишете вручную. Ежедневные журналы (2026-02-26.md) генерируются автоматически из сводок сессий. Векторный индекс, хранящийся в Milvus, - это производный слой, который можно перестроить из Markdown в любое время.</p>
<p>На практике это означает, что вы можете открыть любой файл memsearch в текстовом редакторе, прочитать, что именно знает агент, и изменить это. Сохраните файл, и наблюдатель за файлами memsearch обнаружит изменение и автоматически переиндексирует его. Вы можете управлять воспоминаниями с помощью Git, просматривать созданные ИИ воспоминания с помощью запросов на перенос или перемещаться на новую машину, копируя папку. Если индекс Milvus потерян, вы восстанавливаете его из файлов. Файлы никогда не подвергаются риску.</p>
<p>Под капотом memsearch использует ту же гибридную схему поиска, что описана выше: разделение фрагментов по структуре заголовков и границам абзацев, BM25 + векторный поиск, а также компактная команда на базе LLM, которая суммирует старые воспоминания, когда журналы становятся большими.  <span class="img-wrapper">
    <img translate="no" src="https://assets.zilliz.com/Blog_Open_Claw_Burning_Through_Tokens_3_d9df026b47.png" alt="" class="doc-image" id="" />
    <span></span>
  </span>
</p>
<p>Лучше всего подходит для: команд, которым нужна полная видимость того, что запоминает агент, необходим контроль версий над памятью или нужна система памяти, которая не привязана к какому-то одному фреймворку агента.</p>
<p>Подведем итоги:</p>
<table>
<thead>
<tr><th>Возможности</th><th>Mem0 / Zep</th><th>memsearch</th></tr>
</thead>
<tbody>
<tr><td>Источник истины</td><td>Векторная база данных (единственный источник данных)</td><td>Файлы Markdown (первичные) + Milvus (индекс)</td></tr>
<tr><td>Прозрачность</td><td>Черный ящик, для проверки требуется API</td><td>Откройте любой файл .md для чтения</td></tr>
<tr><td>Возможность редактирования</td><td>Изменение через вызовы API</td><td>Редактирование непосредственно в любом текстовом редакторе, автоматическая переиндексация</td></tr>
<tr><td>Контроль версий</td><td>Требуется отдельное ведение журнала аудита</td><td>Git работает как родной</td></tr>
<tr><td>Стоимость миграции</td><td>Экспорт → преобразование формата → повторный импорт</td><td>Копирование папки Markdown</td></tr>
<tr><td>Сотрудничество человека и ИИ</td><td>ИИ пишет, человек наблюдает</td><td>Люди могут редактировать, дополнять и рецензировать</td></tr>
</tbody>
</table>
<h2 id="Which-setup-fits-your-scale" class="common-anchor-header">Какая настройка подходит для вашего масштаба<button data-href="#Which-setup-fits-your-scale" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><table>
<thead>
<tr><th>Сценарий</th><th>Поиск</th><th>Память</th><th>Когда двигаться дальше</th></tr>
</thead>
<tbody>
<tr><td>Ранний прототип</td><td>Grep (встроенный)</td><td>-</td><td>Счета растут или запросы замедляются</td></tr>
<tr><td>Одиночный разработчик, только поиск</td><td><a href="https://github.com/gladego/index1">index1</a> (скорость) или <a href="https://github.com/tobi/qmd">QMD</a> (точность)</td><td>-</td><td>Нужен многопользовательский доступ или данные перерастают рамки одной машины</td></tr>
<tr><td>Один разработчик, оба</td><td><a href="https://github.com/gladego/index1">индекс1</a></td><td><a href="https://github.com/zilliztech/memsearch">memsearch</a></td><td>Нужен многопользовательский доступ или данные перерастают одну машину</td></tr>
<tr><td>Команда или производство, оба</td><td><a href="https://github.com/milvus-io/milvus">Milvus</a></td><td><a href="https://github.com/zilliztech/memsearch">memsearch</a></td><td>-</td></tr>
<tr><td>Быстрая интеграция, только память</td><td>-</td><td>Mem0 или Zep</td><td>Необходимость проверки, редактирования или миграции памяти</td></tr>
</tbody>
</table>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Расходы на токены, связанные с постоянно работающими агентами ИИ, не являются неизбежными. В этом руководстве мы рассмотрели две области, в которых улучшение инструментария может сократить потери: поиск и память.</p>
<p>Grep работает в небольших масштабах, но по мере роста кодовых баз нерейтинговые совпадения ключевых слов переполняют контекстное окно контентом, который никогда не был нужен модели. <a href="https://github.com/gladego/index1"></a><a href="https://github.com/gladego/index1">index1</a> и <a href="https://github.com/tobi/qmd"></a> QMD решают эту проблему на одной машине, сочетая скоринг ключевых слов BM25 с векторным поиском и возвращая только самые релевантные результаты. Для команд, многоагентных систем или производственных рабочих нагрузок <a href="https://milvus.io"></a><a href="https://milvus.io">Milvus</a> обеспечивает ту же гибридную схему поиска на горизонтально масштабируемой инфраструктуре.</p>
<p>Что касается памяти, то большинство фреймворков хранят все в векторной базе данных: непрозрачной, трудно редактируемой вручную и привязанной к фреймворку, который ее создал. <a href="https://github.com/zilliztech/memsearch">memsearch</a> использует другой подход. Память живет в обычных файлах Markdown, которые можно читать, редактировать и контролировать с помощью Git. Milvus служит производным индексом, который может быть перестроен из этих файлов в любое время. Вы остаетесь в курсе того, что известно агенту.</p>
<p>И <a href="https://github.com/zilliztech/memsearch"></a><a href="https://github.com/zilliztech/memsearch">memsearch</a>, и <a href="https://github.com/milvus-io/milvus"></a><a href="https://github.com/milvus-io/milvus">Milvus</a> имеют открытый исходный код. Мы активно развиваем memsearch и будем рады обратной связи от тех, кто использует его в производстве. Откройте проблему, отправьте PR или просто расскажите нам, что работает, а что нет.</p>
<p>Проекты, упомянутые в этом руководстве:</p>
<ul>
<li><a href="https://github.com/zilliztech/memsearch">memsearch</a>: Память для ИИ-агентов, основанная на Markdown, при поддержке Milvus.</li>
<li><a href="https://github.com/milvus-io/milvus">Milvus</a>: Векторная база данных с открытым исходным кодом для масштабируемого гибридного поиска.</li>
<li><a href="https://github.com/gladego/index1">index1</a>: BM25 + векторный гибридный поиск для агентов кодирования ИИ.</li>
<li><a href="https://github.com/tobi/qmd">QMD</a>: локальный гибридный поиск с повторным ранжированием LLM.</li>
</ul>
<h2 id="Keep-Reading" class="common-anchor-header">Продолжить чтение<button data-href="#Keep-Reading" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><a href="https://milvus.io/blog/we-extracted-openclaws-memory-system-and-opensourced-it-memsearch.md">Мы извлекли систему памяти OpenClaw и выложили ее в открытый доступ (memsearch)</a></li>
<li><a href="https://milvus.io/blog/adding-persistent-memory-to-claude-code-with-the-lightweight-memsearch-plugin.md">Постоянная память для кода Клода: memsearch ccplugin</a></li>
<li><a href="https://milvus.io/blog/openclaw-formerly-clawdbot-moltbot-explained-a-complete-guide-to-the-autonomous-ai-agent.md">Что такое OpenClaw? Полное руководство по агенту искусственного интеллекта с открытым исходным кодом</a></li>
<li><a href="https://milvus.io/blog/stepbystep-guide-to-setting-up-openclaw-previously-clawdbotmoltbot-with-slack.md">Самоучитель по OpenClaw: Подключение к Slack для локального ИИ-ассистента</a></li>
</ul>
