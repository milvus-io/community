---
id: >-
  ai-code-review-gets-better-when-models-debate-claude-vs-gemini-vs-codex-vs-qwen-vs-minimax.md
title: >-
  Анализ кода ИИ становится лучше, когда модели спорят: Claude vs Gemini vs
  Codex vs Qwen vs MiniMax
author: Li Liu
date: 2026-02-26T00:00:00.000Z
cover: >-
  assets.zilliz.com/Code_Review_Benchmark_Cover_Fixed_Icons_2048x1143_11zon_1_04796f1364.jpg
tag: Engineering
recommend: false
publishToMedium: true
tags: 'AI Code Review, Qwen, Claude, Gemini, Codex'
meta_keywords: >-
  AI code review, LLM code review benchmark, Claude vs Gemini vs Codex, AI code
  review benchmark, multi-model AI debate
meta_title: |
  Claude vs Gemini vs Codex vs Qwen vs MiniMax Code Review
desc: >-
  Мы протестировали Claude, Gemini, Codex, Qwen и MiniMax на выявление реальных
  ошибок. Лучшая модель показала результат 53 %. После состязаний обнаружение
  подскочило до 80 %.
origin: 'https://milvus.io/blog/ai-code-review-benchmark-multi-model-debate.md'
---
<p>Недавно я использовал модели искусственного интеллекта для проверки запроса на исправление, и результаты оказались противоречивыми: Claude отметила гонку данных, а Gemini сказала, что код чист. Мне стало любопытно, как поведут себя другие модели ИИ, поэтому я прогнал последние флагманские модели Claude, Gemini, Codex, Qwen и MiniMax через структурированный бенчмарк для проверки кода. Результаты? Самая эффективная модель отловила только 53 % известных ошибок.</p>
<p>Однако на этом мое любопытство не закончилось: что, если бы эти модели ИИ работали вместе? Я попробовал заставить их поспорить друг с другом, и после пяти раундов состязаний обнаружение ошибок подскочило до 80 %. Самые сложные ошибки, требующие понимания на уровне системы, в режиме дебатов обнаруживались на 100 %.</p>
<p>В этом посте мы рассмотрим дизайн эксперимента, результаты по каждой модели, а также то, что механизм дебатов показывает, как на самом деле использовать ИИ для ревью кода.</p>
<h2 id="Benchmarking-Claude-Gemini-Codex-Qwen-and-MiniMax-for-code-review" class="common-anchor-header">Бенчмаркинг Claude, Gemini, Codex, Qwen и MiniMax для ревью кода<button data-href="#Benchmarking-Claude-Gemini-Codex-Qwen-and-MiniMax-for-code-review" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Если вы использовали модели для ревью кода, то наверняка заметили, что они отличаются не только по точности, но и по тому, как они читают код. Например:</p>
<p>Claude обычно проходит цепочку вызовов сверху донизу и тратит время на "скучные" пути (обработка ошибок, повторные попытки, очистка). Часто именно там скрываются настоящие баги, поэтому я не ненавижу тщательность.</p>
<p>Близнецы склонны начинать с сильного вердикта ("это плохо" / "выглядит хорошо"), а затем работать в обратном направлении, чтобы обосновать его с точки зрения дизайна/структуры. Иногда это полезно. Иногда это выглядит так, как будто вы сначала пролистали, а потом приняли решение.</p>
<p>Codex более тихий. Но когда он что-то отмечает, это часто бывает конкретным и действенным - меньше комментариев, больше "эта строчка неправильная, потому что X".</p>
<p>Однако это впечатления, а не измерения. Чтобы получить реальные цифры, я установил контрольную точку.</p>
<h3 id="Setup" class="common-anchor-header">Настройка</h3><p><strong>В тестировании участвовали пять флагманских моделей:</strong></p>
<ul>
<li><p>Claude Opus 4.6</p></li>
<li><p>Gemini 3 Pro</p></li>
<li><p>GPT-5.2-Codex</p></li>
<li><p>Qwen-3.5-Plus</p></li>
<li><p>MiniMax-M2.5</p></li>
</ul>
<p><strong>Инструментарий (Magpie)</strong></p>
<p>Я использовал <a href="https://github.com/liliu-z/magpie">Magpie</a>, инструмент бенчмаркинга с открытым исходным кодом, который я создал. Его работа заключается в том, чтобы делать "подготовку к обзору кода", которую вы обычно делаете вручную: собирать окружающий контекст (цепочки вызовов, связанные модули и релевантный смежный код) и передавать его в модель <em>до того, как</em> она рассмотрит PR.</p>
<p><strong>Тестовые примеры (PR Milvus с известными ошибками)</strong></p>
<p>Набор данных состоит из 15 запросов на исправление ошибок из <a href="https://github.com/milvus-io/milvus">Milvus</a> (векторная база данных с открытым исходным кодом, созданная и поддерживаемая <a href="https://zilliz.com/">Zilliz</a>). Эти PR полезны в качестве эталона, потому что каждый из них был слит, а затем потребовал реверта или исправления после появления ошибки в производстве. Таким образом, в каждом случае есть известная ошибка, которую мы можем оценить.</p>
<p><strong>Уровни сложности ошибок</strong></p>
<p>Не все эти ошибки одинаково сложно найти, поэтому я разделил их на три уровня сложности:</p>
<ul>
<li><p><strong>L1:</strong> видны только по диффу (use-after-free, off-by-one).</p></li>
<li><p><strong>L2 (10 случаев):</strong> Требует понимания окружающего кода, чтобы обнаружить такие вещи, как семантические изменения интерфейса или гонки параллелизма. Это наиболее распространенные ошибки при ежедневном просмотре кода.</p></li>
<li><p><strong>L3 (5 случаев):</strong> Требует понимания системного уровня, чтобы выявить такие проблемы, как несоответствия между состояниями модулей или проблемы совместимости при обновлении. Это самые сложные тесты на то, насколько глубоко модель может рассуждать о кодовой базе.</p></li>
</ul>
<p><em>Примечание: Каждая модель отловила все ошибки L1, поэтому я исключил их из оценки.</em></p>
<p><strong>Два режима оценки</strong></p>
<p>Каждая модель запускалась в двух режимах:</p>
<ul>
<li><p><strong>Raw:</strong> модель видит только PR (diff + все, что находится в содержимом PR).</p></li>
<li><p><strong>R1:</strong> Magpie извлекает окружающий контекст (релевантные файлы / сайты вызовов / связанный код) <em>до того, как</em> модель проанализирует его. Это имитирует рабочий процесс, в котором вы готовите контекст заранее, а не просите модель угадать, что ей нужно.</p></li>
</ul>
<h3 id="Results-L2-+-L3-only" class="common-anchor-header">Результаты (только L2 + L3)</h3><table>
<thead>
<tr><th>Режим</th><th>Клод</th><th>Близнецы</th><th>Codex</th><th>MiniMax</th><th>Qwen</th></tr>
</thead>
<tbody>
<tr><td>Raw</td><td>53% (1-й)</td><td>13% (последний)</td><td>33%</td><td>27%</td><td>33%</td></tr>
<tr><td>R1 (с контекстом от Magpie)</td><td>47% ⬇️</td><td>33%⬆️</td><td>27%</td><td>33%</td><td>40%⬆️</td></tr>
</tbody>
</table>
<p>Четыре вывода:</p>
<p><strong>1. Claude доминирует в необработанном обзоре.</strong> Он набрал 53 % общего обнаружения и 5/5 баллов по ошибкам L3 без какой-либо помощи контекста. Если вы используете одну модель и не хотите тратить время на подготовку контекста, Claude - лучший выбор.</p>
<p><strong>2. Gemini нуждается в помощи контекста.</strong> Его сырая оценка в 13 % была самой низкой в группе, но после того, как Magpie предоставил окружающий код, она подскочила до 33 %. Gemini не очень хорошо собирает свой собственный контекст, но он показывает достойные результаты, когда вы делаете эту работу заранее.</p>
<p><strong>3. Qwen - самый сильный исполнитель с поддержкой контекста.</strong> В режиме R1 он набрал 40 %, а в режиме L2 - 5/10, что является самым высоким результатом на этом уровне сложности. Для ежедневных рутинных проверок, когда вы готовы подготовить контекст, Qwen - практичный выбор.</p>
<p><strong>4. Больше контекста не всегда помогает.</strong> Он поднял Gemini (13 % → 33 %) и MiniMax (27 % → 33 %), но в действительности навредил Claude (53 % → 47 %). Claude и так отлично справляется с организацией контекста, поэтому дополнительная информация, скорее всего, внесла шум, а не ясность. Урок: подбирайте рабочий процесс в соответствии с моделью, а не считайте, что больший объем контекста универсально лучше.</p>
<p>Эти результаты соответствуют моему повседневному опыту. Клод на первом месте - это не удивительно. Оглядываясь назад, я понимаю, что Gemini занял более низкое место, чем я ожидал: я обычно использую Gemini в многооборотных беседах, когда я итерирую дизайн или вместе решаю проблему, и он хорошо работает в такой интерактивной обстановке. Этот бенчмарк представляет собой фиксированный однопроходный конвейер, а это именно тот формат, в котором Gemini слабее всего. Далее в разделе "Дебаты" будет показано, что если дать Gemini многораундовый, состязательный формат, его производительность заметно улучшится.</p>
<h2 id="Let-AI-Models-Debate-with-Each-Other" class="common-anchor-header">Пусть модели ИИ спорят друг с другом<button data-href="#Let-AI-Models-Debate-with-Each-Other" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Каждая модель показала свои сильные стороны и "слепые пятна" в отдельных бенчмарках. Поэтому я хотел проверить: что произойдет, если модели будут анализировать работу друг друга, а не только код?</p>
<p>Поэтому я добавил слой дебатов поверх одного бенчмарка. Все пять моделей участвуют в пяти раундах:</p>
<ul>
<li><p>В первом раунде каждая модель независимо рецензирует один и тот же PR.</p></li>
<li><p>После этого я передаю все пять обзоров всем участникам.</p></li>
<li><p>Во втором раунде каждая модель обновляет свою позицию, основываясь на мнениях остальных четырех.</p></li>
<li><p>Повторяйте до пятого раунда.</p></li>
</ul>
<p>К концу каждая модель не просто реагирует на код - она реагирует на аргументы, которые уже были раскритикованы и пересмотрены несколько раз.</p>
<p>Чтобы это не превратилось в "LLM громко соглашаются", я ввел одно жесткое правило: <strong>каждое утверждение должно указывать на конкретный код в качестве доказательства</strong>, и модель не может просто сказать "хорошо" - она должна объяснить, почему она изменила свое мнение.</p>
<h3 id="Results-Best-Solo-vs-Debate-Mode" class="common-anchor-header">Результаты: Лучшее соло против режима дебатов</h3><table>
<thead>
<tr><th>Режим</th><th>L2 (10 случаев)</th><th>L3 (5 случаев)</th><th>Общее обнаружение</th></tr>
</thead>
<tbody>
<tr><td>Лучший одиночка (сырой Клод)</td><td>3/10</td><td>5/5</td><td>53%</td></tr>
<tr><td>Дебаты (все пять моделей)</td><td>7/10 (удвоенный)</td><td>5/5 (все пойманные)</td><td>80%</td></tr>
</tbody>
</table>
<h3 id="What-stands-out" class="common-anchor-header">Что выделяется</h3><p><strong>1. Обнаружение L2 увеличилось вдвое.</strong> Количество рутинных ошибок средней сложности выросло с 3/10 до 7/10. Это те ошибки, которые чаще всего встречаются в реальных кодовых базах, и именно в них отдельные модели допускают непоследовательные промахи. Наибольший вклад механизма обсуждения заключается в устранении этих повседневных пробелов.</p>
<p><strong>2. Ошибки L3: ноль промахов.</strong> В прогонах с одной моделью только Claude поймал все пять ошибок L3 системного уровня. В режиме дебатов группа сравнялась с ним, что означает, что вам больше не нужно делать ставку на правильную модель, чтобы получить полный охват L3.</p>
<p><strong>3. Дебаты заполняют слепые зоны, а не поднимают потолок.</strong> Ошибки системного уровня не были сложной частью для самого сильного человека. У Клода они уже были. Основной вклад механизма дебатов - исправление слабости Клода в рутинных ошибках L2, где индивидуальный Клод поймал только 3 из 10, а группа дебатеров - 7. Отсюда и происходит скачок 53 % → 80 %.</p>
<h3 id="What-debate-actually-looks-like-in-practice" class="common-anchor-header">Как дебаты выглядят на практике</h3><p>Приведенные выше цифры показывают, что дебаты работают, но конкретный пример показывает <em>, почему</em> они работают. Вот краткое описание того, как пять моделей справились с <strong>PR #44474</strong>, который ввел ленивую загрузку первичных ключей при поиске.</p>
<p><a href="https://github.com/milvus-io/milvus/pull/44474"><strong>PR #44474:</strong></a> Оптимизация ленивой загрузки, которая изменила поиск, чтобы получать первичные ключи по требованию вместо того, чтобы загружать их все заранее.</p>
<p><strong>Раунд 1</strong></p>
<p>Gemini начал раунд агрессивно:</p>
<p>"Этот PR - минное поле с плохими практиками, потенциальным неопределенным поведением и регрессом производительности, замаскированным под "оптимизацию"".</p>
<p>Клод и Квен сосредоточились на неопределенном поведении и производительности, сойдясь на одной и той же проблеме:</p>
<p>"Модель памяти C++ не гарантирует безопасности при одновременной записи в разные индексы одного и того же вектора без надлежащей синхронизации".</p>
<p>Все трое нашли реальные проблемы, но ни один из них не уловил более глубокую ошибку. Тогда Codex, едва заговорив, отметил то, что пропустили остальные: размер вектора <code translate="no">primary_keys_</code> изменяется, но только те позиции, которые проходят через фазу уменьшения, действительно заполняются. Все остальные позиции остаются с нулевым значением.</p>
<p><strong>Раунд 2</strong></p>
<p>Клод подхватил находку Кодекса и проследил дальнейшее развитие событий:</p>
<p>"Я нашел это: <code translate="no">SortEqualScoresByPks</code> запускается раньше <code translate="no">ReduceResultData</code>. Поэтому, когда выполняется <code translate="no">SortEqualScoresByPks</code>, ноль PK еще не был лениво извлечен. Весь вектор <code translate="no">primary_keys_</code> полон значений PkType, сконструированных по умолчанию".</p>
<p>Проще говоря, функция сортировки запускается до загрузки ключей, поэтому она сортирует нули. Клод открыто признал свою ошибку:</p>
<p>"Codex-cli выявил настоящую критическую ошибку корректности. Я пропустил это в своем первом раунде".</p>
<h2 id="Which-Combination-of-Models-Can-Find-the-Most-Bugs" class="common-anchor-header">Какая комбинация моделей может найти больше всего ошибок?<button data-href="#Which-Combination-of-Models-Can-Find-the-Most-Bugs" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Скачок с 53 % до 80 % произошел потому, что пять моделей перекрыли "слепые зоны" друг друга. Но не все могут позволить себе настраивать и запускать пять моделей на пять раундов обсуждения для каждого обзора кода.</p>
<p><strong>Поэтому я протестировал более простую версию: если вы можете запустить только две модели, какая из них позволит вам приблизиться к мультимодельному потолку?</strong></p>
<p>Я использовал прогоны <strong>с поддержкой контекста (R1)</strong> и подсчитал, сколько из 15 известных ошибок нашла каждая модель:</p>
<ul>
<li><p><strong>Claude:</strong> 7/15 (47%)</p></li>
<li><p><strong>Qwen:</strong> 6/15 (40 %)</p></li>
<li><p><strong>Gemini:</strong> 5/15 (33%)</p></li>
<li><p><strong>MiniMax:</strong> 5/15 (33%)</p></li>
<li><p><strong>Кодекс:</strong> 4/15 (27%)</p></li>
</ul>
<p>Важно не то, сколько ошибок находит каждая модель, а то <em>, какие</em> ошибки она пропускает. Из 8 ошибок, пропущенных Claude, Gemini обнаружила три: состояние гонки параллелизма, проблему совместимости с API облачного хранилища и отсутствие проверки разрешений. С другой стороны, Gemini пропустила большинство ошибок в структурах данных и глубокой логике, а Claude - почти все. Их слабые места почти не пересекаются, что и делает их сильной парой.</p>
<table>
<thead>
<tr><th>Пара двух моделей</th><th>Комбинированное покрытие</th></tr>
</thead>
<tbody>
<tr><td>Клод + Близнецы</td><td>10/15</td></tr>
<tr><td>Клод + Квен</td><td>9/15</td></tr>
<tr><td>Клод + Кодекс</td><td>8/15</td></tr>
<tr><td>Клод + МиниМакс</td><td>8/15</td></tr>
</tbody>
</table>
<p>Все пять моделей вместе покрыли 11 из 15, оставив 4 ошибки, которые пропустила каждая модель.</p>
<p><strong>Клод + Близнецы,</strong> как пара из двух моделей, уже достигает 91% от этого потолка из пяти моделей. Для данного бенчмарка это самая эффективная комбинация.</p>
<p>Тем не менее, Claude + Gemini не является лучшей парой для всех типов ошибок. Когда я разложил результаты по категориям ошибок, получилась более нюансированная картина:</p>
<table>
<thead>
<tr><th>Тип ошибки</th><th>Всего</th><th>Клод</th><th>Близнецы</th><th>Кодекс</th><th>MiniMax</th><th>Qwen</th></tr>
</thead>
<tbody>
<tr><td>Пробелы в проверке</td><td>4</td><td>3</td><td>2</td><td>1</td><td>1</td><td>3</td></tr>
<tr><td>Жизненный цикл структуры данных</td><td>4</td><td>3</td><td>1</td><td>1</td><td>3</td><td>1</td></tr>
<tr><td>Параллельные гонки</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>Совместимость</td><td>2</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>Глубокая логика</td><td>3</td><td>1</td><td>0</td><td>1</td><td>1</td><td>1</td></tr>
<tr><td>Всего</td><td>15</td><td>7</td><td>5</td><td>4</td><td>5</td><td>6</td></tr>
</tbody>
</table>
<p>Разбивка по типам ошибок показывает, почему ни одна пара не является универсально лучшей.</p>
<ul>
<li><p>Для ошибок жизненного цикла структуры данных Клод и MiniMax сравнялись на 3/4.</p></li>
<li><p>По ошибкам валидации Клод и Qwen сравнялись на 3/4.</p></li>
<li><p>В вопросах параллелизма и совместимости Клод набрал ноль баллов, и Gemini восполняет эти пробелы.</p></li>
<li><p>Ни одна модель не охватывает всего, но Claude охватывает самый широкий диапазон и ближе всего к универсальности.</p></li>
</ul>
<p>Все модели пропустили четыре ошибки. Одна связана с приоритетом правил грамматики ANTLR. Одна была связана с семантическим несоответствием блокировки чтения/записи в разных функциях. Одна требовала понимания различий в бизнес-логике между типами уплотнения. А одна была ошибкой молчаливого сравнения, когда одна переменная использовала мегабайты, а другая - байты.</p>
<p>Все эти четыре ошибки объединяет то, что код синтаксически корректен. Ошибки кроются в предположениях, которые разработчик вынашивал в своей голове, а не в diff и даже не в окружающем коде. Примерно в этом месте AI code review сегодня достигает своего потолка.</p>
<h2 id="After-Finding-Bugs-Which-Model-is-the-Best-at-Fixing-Them" class="common-anchor-header">Какая модель лучше всего справляется с исправлением ошибок после их обнаружения?<button data-href="#After-Finding-Bugs-Which-Model-is-the-Best-at-Fixing-Them" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>В ревью кода найти ошибки - это половина работы. Вторая половина - их исправление. Поэтому после дебатов я добавил экспертную оценку, чтобы определить, насколько полезными на самом деле являются предложения каждой модели по исправлению ошибок.</p>
<p>Чтобы измерить это, я добавил раунд взаимной оценки после дебатов. Каждая модель открывала новую сессию и выступала в роли анонимного судьи, оценивая отзывы других моделей. Пять моделей были случайным образом сопоставлены с рецензентом A/B/C/D/E, поэтому никто из судей не знал, какая модель создала тот или иной обзор. Каждый судья выставлял оценки по четырем параметрам, оцениваемым от 1 до 10: точность, практичность, глубина и ясность.</p>
<table>
<thead>
<tr><th>Модель</th><th>Точность</th><th>Действенность</th><th>Глубина</th><th>Ясность</th><th>В целом</th></tr>
</thead>
<tbody>
<tr><td>Qwen</td><td>8.6</td><td>8.6</td><td>8.5</td><td>8.7</td><td>8,6 (1-е место)</td></tr>
<tr><td>Клод</td><td>8.4</td><td>8.2</td><td>8.8</td><td>8.8</td><td>8.6 (1 место)</td></tr>
<tr><td>Кодекс</td><td>7.7</td><td>7.6</td><td>7.1</td><td>7.8</td><td>7.5</td></tr>
<tr><td>Близнецы</td><td>7.4</td><td>7.2</td><td>6.7</td><td>7.6</td><td>7.2</td></tr>
<tr><td>MiniMax</td><td>7.1</td><td>6.7</td><td>6.9</td><td>7.4</td><td>7.0</td></tr>
</tbody>
</table>
<p>Qwen и Claude с большим отрывом заняли первое место. Оба получили стабильно высокие оценки по всем четырем параметрам, в то время как Codex, Gemini и MiniMax оказались ниже на целый балл или даже больше. Примечательно, что Gemini, который оказался ценным партнером Клода по поиску ошибок в парном анализе, занял самое низкое место по качеству рецензирования. Очевидно, что умение находить проблемы и умение объяснить, как их исправить, - это разные навыки.</p>
<h2 id="Conclusion" class="common-anchor-header">Заключение<button data-href="#Conclusion" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p><strong>Claude</strong> - это тот, кому можно доверить самые сложные обзоры. Он прорабатывает целые цепочки вызовов, следует глубоким логическим путям и вникает в собственный контекст, не требуя, чтобы вы кормили его с ложечки. По ошибкам системного уровня L3 ничто другое и близко не подошло. Иногда он бывает слишком самоуверен в математике, но когда другая модель доказывает, что она не права, он признает это и объясняет, где его рассуждения не работают. Используйте его для основного кода и ошибок, которые вы не можете позволить себе пропустить.</p>
<p><strong>Близнецы</strong> вступают в игру. У них сильное мнение о стиле кода и инженерных стандартах, и они быстро структурируют проблемы. Недостатком является то, что он часто остается на поверхности и не копает достаточно глубоко, поэтому он получил низкий балл при оценке коллег. Близнецы действительно заслужили свое место в качестве претендента: их отталкивание заставляет другие модели перепроверять свою работу. Работайте в паре с Claude, чтобы получить структурную перспективу, которую Claude иногда пропускает.</p>
<p><strong>Codex</strong> почти не говорит ни слова. Но когда он говорит, это имеет значение. Его процент попадания в реальные ошибки высок, и он умеет уловить то, мимо чего прошли все остальные. В примере с PR #44474 Codex был той моделью, которая заметила проблему первичных ключей с нулевым значением, запустившую всю цепочку. Думайте о нем как о дополнительном рецензенте, который улавливает то, что пропустила ваша основная модель.</p>
<p><strong>Qwen</strong> - самый хорошо подготовленный из пяти. Качество его обзоров соответствует качеству обзоров Клода, и он особенно хорош в объединении различных точек зрения в предложения по исправлению, которые вы можете реально использовать. Кроме того, у него самый высокий показатель обнаружения L2 в режиме контекстной помощи, что делает его надежным помощником по умолчанию для ежедневных PR-обзоров. Единственная слабость: в длинных многораундовых дебатах он иногда теряет связь с предыдущим контекстом и начинает давать непоследовательные ответы в последующих раундах.</p>
<p><strong>MiniMax</strong> оказался самым слабым в самостоятельном поиске ошибок. Лучше всего использовать его для пополнения группы из нескольких моделей, а не в качестве самостоятельного рецензента.</p>
<h2 id="Limitations-of-This-Experiment" class="common-anchor-header">Ограничения данного эксперимента<button data-href="#Limitations-of-This-Experiment" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Несколько предостережений, чтобы не упустить из виду этот эксперимент:</p>
<p><strong>Размер выборки невелик.</strong> Всего 15 PR, все из одного проекта на Go/C++ (Milvus). Эти результаты не распространяются на все языки или кодовые базы. Рассматривайте их как ориентировочные, а не окончательные.</p>
<p><strong>Модели по своей природе случайны.</strong> Запуск одного и того же запроса дважды может дать разные результаты. Цифры в этом посте - это единичный снимок, а не стабильное ожидаемое значение. К рейтингу отдельных моделей следует относиться с осторожностью, хотя общие тенденции (дебаты превосходят отдельные модели, разные модели лучше справляются с разными типами ошибок) совпадают.</p>
<p><strong>Порядок выступлений был исправлен.</strong> В дебатах использовался один и тот же порядок во всех раундах, что могло повлиять на то, как реагировали модели, выступавшие позже. В будущем эксперименте можно было бы рандомизировать порядок выступлений в каждом раунде, чтобы проконтролировать это.</p>
<h2 id="Try-it-yourself" class="common-anchor-header">Попробуйте сами<button data-href="#Try-it-yourself" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><p>Все инструменты и данные этого эксперимента находятся в открытом доступе:</p>
<ul>
<li><p><a href="https://github.com/liliu-z/magpie"><strong>Magpie</strong></a>: Инструмент с открытым исходным кодом, который собирает контекст кода (цепочки вызовов, связанные PR, затронутые модули) и организует многомодельные состязательные дебаты для рецензирования кода.</p></li>
<li><p><a href="https://github.com/liliu-z/ai-code-review-arena"><strong>AI-CodeReview-Arena</strong></a>: Полный конвейер оценки, конфигурации и скрипты.</p></li>
<li><p><a href="https://github.com/liliu-z/ai-code-review-arena/blob/main/prs/manifest.yaml"><strong>Тестовые примеры</strong></a>: Все 15 PR с аннотированными известными ошибками.</p></li>
</ul>
<p>Все ошибки в этом эксперименте были получены из реальных запросов на исправление в <a href="https://github.com/milvus-io/milvus">Milvus</a>, векторной базе данных с открытым исходным кодом, созданной для приложений ИИ. У нас довольно активное сообщество в <a href="https://discord.com/invite/8uyFbECzPX">Discord</a> и <a href="https://milvusio.slack.com/join/shared_invite/zt-3nntzngkz-gYwhrdSE4~76k0VMyBfD1Q#/shared-invite/email">Slack</a>, и мы были бы рады, если бы в код заглянуло больше людей. И если вы проведете этот бенчмарк на своей собственной кодовой базе, пожалуйста, поделитесь результатами! Мне очень интересно, сохранятся ли тенденции в разных языках и проектах.</p>
<h2 id="Keep-Reading" class="common-anchor-header">Продолжить чтение<button data-href="#Keep-Reading" class="anchor-icon" translate="no">
      <svg translate="no"
        aria-hidden="true"
        focusable="false"
        height="20"
        version="1.1"
        viewBox="0 0 16 16"
        width="16"
      >
        <path
          fill="#0092E4"
          fill-rule="evenodd"
          d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"
        ></path>
      </svg>
    </button></h2><ul>
<li><p><a href="https://milvus.io/blog/glm5-vs-minimax-m25-vs-gemini-3-deep-think.md">GLM-5 vs. MiniMax M2.5 vs. Gemini 3 Глубокое размышление: какая модель подходит для вашего стека агентов ИИ?</a></p></li>
<li><p><a href="https://milvus.io/blog/adding-persistent-memory-to-claude-code-with-the-lightweight-memsearch-plugin.md">Добавление постоянной памяти в код Claude с помощью легкого плагина memsearch</a></p></li>
<li><p><a href="https://milvus.io/blog/we-extracted-openclaws-memory-system-and-opensourced-it-memsearch.md">Мы извлекли систему памяти OpenClaw и выложили ее в открытый доступ (memsearch)</a></p></li>
</ul>
